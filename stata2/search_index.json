[
["index.html", "Intermediate Stata (Statistical Modeling) Chapter 1 Preface 1.1 How to use this document 1.2 Contact information 1.3 Acknowledgments", " Intermediate Stata (Statistical Modeling) Josh Errickson 2017-10-03 Chapter 1 Preface 1.1 How to use this document These notes are published in bookdown format which enables easy creation of longform documents (using a mixture of markdown, R, and for these notes specifically, Stata’s dyndoc). The table of contents is found on the left hand side, with subsections expanding below the current section. At the top of the page are three icons, from left to right they 1) Hide/show the table of contents, 2) Search this document, and 3) Change the font the book is displayed in. All images should link to full-size versions to see detail if needed. 1.2 Contact information 1.2.1 CSCAR Office Hours: Monday-Friday 9am to 5pm (Closed Tuesday 12-1pm) Phone: 734.764.7828 Statistical Assistance: stats-consulting@umich.edu Data Science Assistance: ds-consulting@umich.edu High Performance Computing: hpc-consulting@umich.edu http://cscar.research.umich.edu/ 1.2.2 Author Josh Errickson 3550 Rackham Building 915 E. Washington Street Ann Arbor, MI 48109-1070 Email: jerrick@umich.edu 1.3 Acknowledgments These notes have evolved over the years thanks to many CSCAR statisticians, including Giselle Kolenic, Brady West, Heidi Reichert, and Lingling Zhang. This material was created for use in workshops and short courses presented by faculty and staff from the Consulting for Statistics, Computing &amp; Analytics Research (CSCAR) at the University of Michigan. No part of this material may be used for other purposes, copied, changed, or sold. "],
["summarizing-data.html", "Chapter 2 Summarizing Data 2.1 describe, summarize, codebook 2.2 mean 2.3 Estimation Commands 2.4 tab 2.5 correlate 2.6 Exercise 1", " Chapter 2 Summarizing Data Before we consider modeling the data in any meaningful way, it is important to explore the data to get some sense of what the data looks like, as certain modeling decisions will depend on the structure of the data. This chapter and the next will cover how to examine and visualize the data. We will be using the built-in data set “auto” for a lot of the examples. If you’re not familiar with the sysuse command, it functions similarly to use, except only loads one of the several built-in Stata data sets. . sysuse auto, clear (1978 Automobile Data) The “auto” data set contains characteristics on a number of cars from 1978. 2.1 describe, summarize, codebook The majority of the Stata modeling commands require all variables to be numeric. String variables can be used in some places, but there are plenty of times when you might expect them to work, but they don’t. As a result, I recommend converting all categorical variables into numeric. To help with this, the describe command can tell us what variables are string and which aren’t. . describe Contains data from /Applications/Stata/ado/base/a/auto.dta obs: 74 1978 Automobile Data vars: 12 13 Apr 2016 17:45 size: 3,182 (_dta has notes) ------------------------------------------------------------------------------- storage display value variable name type format label variable label ------------------------------------------------------------------------------- make str18 %-18s Make and Model price int %8.0gc Price mpg int %8.0g Mileage (mpg) rep78 int %8.0g Repair Record 1978 headroom float %6.1f Headroom (in.) trunk int %8.0g Trunk space (cu. ft.) weight int %8.0gc Weight (lbs.) length int %8.0g Length (in.) turn int %8.0g Turn Circle (ft.) displacement int %8.0g Displacement (cu. in.) gear_ratio float %6.2f Gear Ratio foreign byte %8.0g origin Car type ------------------------------------------------------------------------------- Sorted by: foreign Here we can see that “make” is a string; but make is unique per row (it identifies the make and model of each car) so it’s not something we’re going to use in the model. If you wanted to use string functions (see help string functions for details) to extract out the manufacturer of each car (e.g. there are 7 Buicks in the data), that resultant “manufacturer” variable would be something we’d need to convert to a numeric. The main tools you’d need would be destring (which converts numeric values saved as strings into numbers) and encode (which converts strings to numerics with appropriate value labels). describe is also useful to get a sense of the size of your data. Once we’ve taken a look at the structure of the data, we can start exploring each variable. The summarize and codebook commands contains almost the same information, presented in slightly different ways. It can be useful to look at both. For example, . summ price, detail Price ------------------------------------------------------------- Percentiles Smallest 1% 3291 3291 5% 3748 3299 10% 3895 3667 Obs 74 25% 4195 3748 Sum of Wgt. 74 50% 5006.5 Mean 6165.257 Largest Std. Dev. 2949.496 75% 6342 13466 90% 11385 13594 Variance 8699526 95% 13466 14500 Skewness 1.653434 99% 15906 15906 Kurtosis 4.819188 . codebook price ------------------------------------------------------------------------------- price Price ------------------------------------------------------------------------------- type: numeric (int) range: [3291,15906] units: 1 unique values: 74 missing .: 0/74 mean: 6165.26 std. dev: 2949.5 percentiles: 10% 25% 50% 75% 90% 3895 4195 5006.5 6342 11385 Things to look for here include Values which are outside of expected values. The summarize commands gives the 1st and 99th percentiles (1% and 99% of values are below those thresholds, respectively) and codebook gives the range. If, for example, we saw a minimum value of -203 or a maximum value of 145200 (keep in mind these are 1978 dollars!), that’s an indication that there is an issue with the data, likely a mistake. The mean is as expected. If this is higher or lower than expected, it might be an indication of skew or the existence of outliers. If it is very close to the minimum or maximum value, perhaps you have a point mass (e.g. if you polled 18-21 year old’s on their number of children, there would be a lot of 0’s but a few non-zeros). If the standard deviation is very small (relative to the mean), then the variable has very consistent values. A standard deviation of 0 indicates a constant. The codebook reports the number of missing; if you have missing data, double check that it is not an error in the data. Perhaps multiple imputation is needed. If the variable is categorical (e.g. race), is the number of unique entries reported in the codebook as expected? 2.2 mean The mean command gives summary statistics on the mean of a variable. . mean price Mean estimation Number of obs = 74 -------------------------------------------------------------- | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ price | 6165.257 342.8719 5481.914 6848.6 -------------------------------------------------------------- These are characteristics of the estimated mean of the “price” variable. The standard deviation reported from the summarize command above represents the variability among individual cars; the standard error reported by mean the variability of means: if we were to repeatedly draw samples of size 74, the standard error is a measure of the variability of the means from all those samples. The confidence interval is interpreted as if we were to continue drawing those samples of size 74, we would expect 95% of those samples to have an estimated mean within those bounds. It is not that we’re 95% confident that the true population mean falls in that range - either it does or it doesn’t! 2.3 Estimation Commands The introduction of mean allows us to discuss estimation commands. An estimation command is any command that fits a statistical model - some of these are obvious such as regress for linear regression, but others such as mean which we just ran are also estimation commands because it is estimating a confidence interval. summarize is not because it only provides statistics about the current sample instead of making inference into the population. Almost all estimation commands have the same general syntax: command varlist [if] [in] [weight] [,options] The sections inside [ and ] are optional. The command can sometimes consist of a main command and one or more subcommands. The varlist can be empty, have a single entry, or have multiple entries (the order of which is sometimes of importance - generally the first is some outcome or dependent variable and the rest are predictors or independent variables).1 Estimation commands are stored after they are run, and exist regardless of how many other non-estimation commands are run in between them. These non-estimation commands include data manipulation and postestimation commands. As soon as another estimation command is run, the first is dropped and the new one is saved. This allows interesting things such as replaying a command (calling the estimation command again without any varlist to re-display it’s results) even if the data is gone! . clear . list . mean Mean estimation Number of obs = 74 -------------------------------------------------------------- | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ price | 6165.257 342.8719 5481.914 6848.6 -------------------------------------------------------------- A larger benefit of this is that if you are fitting a model on one data set and want to get predicted values on another, you could do something like this (this is pseudo-code, not real Stata!): use fitting_data model y x1 x2 use newdata, clear predict fitted 2.3.1 Postestimation commands Since the last estimation command is saved, any commands which need to reference it (called postestimation commands) do so inherently, no need to specify. For example, let’s reload the data and run mean on a few variables. . sysuse auto, clear (1978 Automobile Data) . mean mpg headroom length Mean estimation Number of obs = 74 -------------------------------------------------------------- | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ mpg | 21.2973 .6725511 19.9569 22.63769 headroom | 2.993243 .0983449 2.797242 3.189244 length | 187.9324 2.588409 182.7737 193.0911 -------------------------------------------------------------- Let’s say we want to obtain the correlation matrix2 . estat vce, corr Correlation matrix of coefficients of mean model e(V) | mpg headroom length -------------+------------------------------ mpg | 1.0000 headroom | -0.4138 1.0000 length | -0.7958 0.5163 1.0000 Here we see that both length and headroom are negatively correlated with mpg; as the car gets larger, its mileage decreases. Headroom and length are positively correlated, so cars aren’t just growing in one direction! The estat command is somewhat generic, we will see other uses of it later. Similar to how you can get help with any command with help, e.g. help mean, you can get a list of all postestimation commands that a given estimation command supports: help mean postestimation There is also a link to the postestimation page in the help for the estimation command. 2.3.2 Storing and restoring estimation commands The obvious downside to Stata’s approach to saving the most recent estimation command is that you lose all earlier commands. If you have only a limited number of commands and each is fast, this isn’t a big deal. However, with some more advanced approaches, modeling can become very slow, so you may not want to lose the results. Stata has a solution for this, allowing us to store and recall estimation commands without having to re-run them. This has an obvious parallel to the preserve/restore commands that affect the data. You have the choice of saving the results temporarily (in memory) or permanently (to a file). There are the obvious pro’s and con’s to each approach. For these notes I will focus primarily on storing the results in memory, but I will point out where the commands differ if saving to a file. Let’s run a fresh mean call to work with. The estimates command will be used. . mean price mpg Mean estimation Number of obs = 74 -------------------------------------------------------------- | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ price | 6165.257 342.8719 5481.914 6848.6 mpg | 21.2973 .6725511 19.9569 22.63769 -------------------------------------------------------------- . estimates query (active results produced by mean; not yet stored) The query subcommand tells us what estimation command was last run, and whether it has already been saved. Here it has not. Let’s save these results. . estimates store mean1 To save to a file, use estimates save instead. Now let’s run a second mean commands. . mean mpg headroom length Mean estimation Number of obs = 74 -------------------------------------------------------------- | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ mpg | 21.2973 .6725511 19.9569 22.63769 headroom | 2.993243 .0983449 2.797242 3.189244 length | 187.9324 2.588409 182.7737 193.0911 -------------------------------------------------------------- . est store mean2 . est query (active results produced by mean; also stored as mean2) Now query is telling us that the current estimation commands are (obviously) stored as “mean2”. Let’s use estimates restore to jump between the two. (If saving to a file, use estimates use instead.) . est restore mean1 (results mean1 are active now) . estat vce, corr Correlation matrix of coefficients of mean model e(V) | price mpg -------------+-------------------- price | 1.0000 mpg | -0.4686 1.0000 . est query (active results produced by mean; also stored as mean1) To “replay” an estimation command (re-display the results without re-running the model), you can either restore it and call the blank command again: . est restore mean2 (results mean2 are active now) . mean Mean estimation Number of obs = 74 -------------------------------------------------------------- | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ mpg | 21.2973 .6725511 19.9569 22.63769 headroom | 2.993243 .0983449 2.797242 3.189244 length | 187.9324 2.588409 182.7737 193.0911 -------------------------------------------------------------- or use estimates replay directly: . est query (active results produced by mean; also stored as mean2) . est replay mean1 ------------------------------------------------------------------------------- Model mean1 ------------------------------------------------------------------------------- Mean estimation Number of obs = 74 -------------------------------------------------------------- | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ price | 6165.257 342.8719 5481.914 6848.6 mpg | 21.2973 .6725511 19.9569 22.63769 -------------------------------------------------------------- One use of stored estimates that can be useful is creating a table to include all the results. . est table mean1 mean2 ---------------------------------------- Variable | mean1 mean2 -------------+-------------------------- price | 6165.2568 mpg | 21.297297 21.297297 headroom | 2.9932432 length | 187.93243 ---------------------------------------- If you are familiar with regression, you should be able to see how useful this might be! Finally, we can see all saved commands with dir, drop a specific estimation command with drop, or remove all with clear: . est dir ------------------------------------------------------- name | command depvar npar title -------------+----------------------------------------- mean1 | mean Mean 2 mean2 | mean Mean 3 ------------------------------------------------------- . est drop mean1 . est dir ------------------------------------------------------- name | command depvar npar title -------------+----------------------------------------- mean2 | mean Mean 3 ------------------------------------------------------- . est clear . est dir 2.4 tab Continuing on with exploring the data, categorical variables are not summarized well by the mean. Instead, we’ll look at a tabulation. . tabulate rep78 Repair | Record 1978 | Freq. Percent Cum. ------------+----------------------------------- 1 | 2 2.90 2.90 2 | 8 11.59 14.49 3 | 30 43.48 57.97 4 | 18 26.09 84.06 5 | 11 15.94 100.00 ------------+----------------------------------- Total | 69 100.00 This gives us the count at each level, the percent at each level, as well as the cumulative percent (e.g. 57.97% of observations have a value of 3 or below). The cumulative percentage is only informative for an ordinal variable (a categorical variable that has an ordering too it), and not an unordered categorical variable such as race. Note that it is counting a total of 69 observations to total 100% of the data. However, you may have noticed earlier that we have 74 rows of data. By default, tabulate does not include any information about missing values. The missing option corrects that. . tab rep78, missing Repair | Record 1978 | Freq. Percent Cum. ------------+----------------------------------- 1 | 2 2.70 2.70 2 | 8 10.81 13.51 3 | 30 40.54 54.05 4 | 18 24.32 78.38 5 | 11 14.86 93.24 . | 5 6.76 100.00 ------------+----------------------------------- Total | 74 100.00 It’s important to keep in mind the difference between the percentages of the two outputs. For example, 11.59% of non-missing values of rep78 are 2, whereas only 10.81% of all values are 2. There are a few other options related to how the results are visualized which we will not cover. 2.4.1 Two-way tables We will cover two-way tables (also known as “crosstabs”) later in univariate analysis, but there is a peculiarity to tab related to it. If you pass two variables to tab, it creates the crosstab: . tab rep78 foreign, missing Repair | Record | Car type 1978 | Domestic Foreign | Total -----------+----------------------+---------- 1 | 2 0 | 2 2 | 8 0 | 8 3 | 27 3 | 30 4 | 9 9 | 18 5 | 2 9 | 11 . | 4 1 | 5 -----------+----------------------+---------- Total | 52 22 | 74 What if instead you wanted each individual table? You could run multiple tab statements, or use the tab1 command instead. . tab1 rep78 foreign, missing -&gt; tabulation of rep78 Repair | Record 1978 | Freq. Percent Cum. ------------+----------------------------------- 1 | 2 2.70 2.70 2 | 8 10.81 13.51 3 | 30 40.54 54.05 4 | 18 24.32 78.38 5 | 11 14.86 93.24 . | 5 6.76 100.00 ------------+----------------------------------- Total | 74 100.00 -&gt; tabulation of foreign Car type | Freq. Percent Cum. ------------+----------------------------------- Domestic | 52 70.27 70.27 Foreign | 22 29.73 100.00 ------------+----------------------------------- Total | 74 100.00 If you give more than two arguments to tab, it will not run. If you wanted all pairwise tables, you can use tab2: . tab2 rep78 foreign headroom, missing -&gt; tabulation of rep78 by foreign Repair | Record | Car type 1978 | Domestic Foreign | Total -----------+----------------------+---------- 1 | 2 0 | 2 2 | 8 0 | 8 3 | 27 3 | 30 4 | 9 9 | 18 5 | 2 9 | 11 . | 4 1 | 5 -----------+----------------------+---------- Total | 52 22 | 74 -&gt; tabulation of rep78 by headroom Repair | Record | Headroom (in.) 1978 | 1.5 2.0 2.5 3.0 3.5 | Total -----------+-------------------------------------------------------+---------- 1 | 1 1 0 0 0 | 2 2 | 0 3 0 0 1 | 8 3 | 0 5 5 4 10 | 30 4 | 2 1 5 3 2 | 18 5 | 0 3 4 4 0 | 11 . | 1 0 0 2 2 | 5 -----------+-------------------------------------------------------+---------- Total | 4 13 14 13 15 | 74 Repair | Record | Headroom (in.) 1978 | 4.0 4.5 5.0 | Total -----------+---------------------------------+---------- 1 | 0 0 0 | 2 2 | 2 1 1 | 8 3 | 3 3 0 | 30 4 | 5 0 0 | 18 5 | 0 0 0 | 11 . | 0 0 0 | 5 -----------+---------------------------------+---------- Total | 10 4 1 | 74 -&gt; tabulation of foreign by headroom | Headroom (in.) Car type | 1.5 2.0 2.5 3.0 3.5 | Total -----------+-------------------------------------------------------+---------- Domestic | 3 10 4 7 13 | 52 Foreign | 1 3 10 6 2 | 22 -----------+-------------------------------------------------------+---------- Total | 4 13 14 13 15 | 74 | Headroom (in.) Car type | 4.0 4.5 5.0 | Total -----------+---------------------------------+---------- Domestic | 10 4 1 | 52 Foreign | 0 0 0 | 22 -----------+---------------------------------+---------- Total | 10 4 1 | 74 2.4.2 Generating dummy variables Although Stata has excellent categorical variable handling capabilities, you may occasionally have the situation where you want the dummy variables instead of a category. For an example of the difference, consider a “campus” variable with three options, “central”, “north” and “medical”. Imagine our data looks like: id campus campuscentral campusnorth campusmedical 1 north 0 1 0 2 central 1 0 0 3 north 0 1 0 4 north 0 1 0 5 medical 0 0 1 Notice that the information in campus and the information encoded in campuscentral, campusnorth, and campusmedical are identical. A 1 in the campus____ variables represents “True” and 0 represents “False”, and only a single 1 is allowed per row. As mentioned, we will most of the time use categorical variables such as campus over dummy variables like campus_____ (these are used in the actual model, but Stata creates them for you behind the scenes so you don’t need to worry about them), but if necessary, you can create the dummy variables using tab: . list rep* in 1/5 +-------+ | rep78 | |-------| 1. | 3 | 2. | 3 | 3. | . | 4. | 3 | 5. | 4 | +-------+ . tab rep78, gen(reps) Repair | Record 1978 | Freq. Percent Cum. ------------+----------------------------------- 1 | 2 2.90 2.90 2 | 8 11.59 14.49 3 | 30 43.48 57.97 4 | 18 26.09 84.06 5 | 11 15.94 100.00 ------------+----------------------------------- Total | 69 100.00 . list rep* in 1/5 +-----------------------------------------------+ | rep78 reps1 reps2 reps3 reps4 reps5 | |-----------------------------------------------| 1. | 3 0 0 1 0 0 | 2. | 3 0 0 1 0 0 | 3. | . . . . . . | 4. | 3 0 0 1 0 0 | 5. | 4 0 0 0 1 0 | +-----------------------------------------------+ If you are not familiar with the list command, it prints out data. Giving it a variable (or multiple) restricts it to those (here we restricted it to rep*, which is any variable that starts with “rep” - the * is a wildcard), and the in statement restricts to the first 5 observations (we just want a quick visualization, not to print everything). Take note of how the missing value is treated when creating the dummies. 2.5 correlate With the use of tab and tab2 for crosstabs, we’ve left univariate summaries and moved to joint summaries. For continuous variables, we can use the correlation to examine how similar two continuous variables are. The most common version of correlation is Pearson’s correlation, which ranges from -1 to 1. A value of 0 represents no correlation, a value of 1 represents perfect correlation, a value of -1 represents perfect negative correlation. We can calculate the Pearson’s correlation with correlate. . correlate weight length (obs=74) | weight length -------------+------------------ weight | 1.0000 length | 0.9460 1.0000 This produces whats known as the correlation matrix. The diagonal entries are both 1, because clearly each variable is perfectly correlated with itself! The off-diagonal entries are identical since correlation is a symmetric operation. The value of .95 is extremely close to one, as we would expect - longer cars are heavier and perhaps vice-versa. Another way to think of it is that once we know weight, learning length does not add much information. On the other hand, . corr price turn (obs=74) | price turn -------------+------------------ price | 1.0000 turn | 0.3096 1.0000 with a correlation of .31, learning turn when you already know price does add a lot of information. We can look at multiple correlations at once as well. . corr mpg weight length (obs=74) | mpg weight length -------------+--------------------------- mpg | 1.0000 weight | -0.8072 1.0000 length | -0.7958 0.9460 1.0000 We see the .9460 we saw earlier, but notice also that mpg is negatively correlated with both weight and length - a larger car gets worse mileage and low mileage cars tend to be large. A few notes: The amount of information contained is irrespective of the sign; knowing the mpg of a car, adding information about its weight doesn’t add much information. The two correlations with mpg are extremely similar. We might generally expect that, given that weight and length are so strongly correlated. Note that despite that we expect that, it is not a rule - it is entirely possible (though unlikely) that the correlations with mpg could be very dissimilar. 2.5.1 varlists in Stata Consider if we wanted to look at all the continuous variables in the data. We could write corr price mpg … and make a very long command. The collection of all variables would be a “varlist”. Stata has several ways of short cutting this. The first we’ve already seen when we used the wildcard “*” above. We can use * anywhere in the variable name to denote any number of additional characters. E.g. “this*var” matches “thisvar”, “thisnewvar”, “this-var”, “thisHFJHDJSHFKDHFKSHvar”, etc. A second wildcard, “?”, represents just a single variable, so “this*var” would match only “this-var” from that list, as well as “thisAvar”, “thisJvar”, etc. Secondly, we can match a subset of variables that are next to each other using “-”. All variable, starting with the one to the left of the - and ending with the one to the right of the - are included. For example, . desc, simple make headroom turn reps1 reps5 price trunk displacement reps2 mpg weight gear_ratio reps3 rep78 length foreign reps4 . desc trunk-turn storage display value variable name type format label variable label ------------------------------------------------------------------------------- trunk int %8.0g Trunk space (cu. ft.) weight int %8.0gc Weight (lbs.) length int %8.0g Length (in.) turn int %8.0g Turn Circle (ft.) We can combine those two, as well as specifying individual variables. . corr price-rep78 t* displacement (obs=69) | price mpg rep78 trunk turn displa~t -------------+------------------------------------------------------ price | 1.0000 mpg | -0.4559 1.0000 rep78 | 0.0066 0.4023 1.0000 trunk | 0.3232 -0.5798 -0.1572 1.0000 turn | 0.3302 -0.7355 -0.4961 0.6008 1.0000 displacement | 0.5479 -0.7434 -0.4119 0.6287 0.8124 1.0000 price, mpg and rep78 are included as part of price-rep78, t* matches trunk and turn, and displacement is included by itself. Finally, there is the special variable list _all, which is shorthand for all variables (e.g. firstvar-lastvar). It is accepted in most but not all places that take in variables. . corr _all (make ignored because string variable) (obs=69) | price mpg rep78 headroom trunk weight length -------------+--------------------------------------------------------------- price | 1.0000 mpg | -0.4559 1.0000 rep78 | 0.0066 0.4023 1.0000 headroom | 0.1112 -0.3996 -0.1480 1.0000 trunk | 0.3232 -0.5798 -0.1572 0.6608 1.0000 weight | 0.5478 -0.8055 -0.4003 0.4795 0.6691 1.0000 length | 0.4425 -0.8037 -0.3606 0.5240 0.7326 0.9478 1.0000 turn | 0.3302 -0.7355 -0.4961 0.4347 0.6008 0.8610 0.8631 displacement | 0.5479 -0.7434 -0.4119 0.4763 0.6287 0.9316 0.8621 gear_ratio | -0.3802 0.6565 0.4103 -0.3790 -0.5107 -0.7906 -0.7232 foreign | -0.0174 0.4538 0.5922 -0.3347 -0.4053 -0.6460 -0.6110 reps1 | -0.0945 -0.0086 -0.4230 -0.2550 -0.2175 0.0149 0.0054 reps2 | -0.0223 -0.1346 -0.5180 0.1603 0.0586 0.1480 0.1778 reps3 | 0.0859 -0.2796 -0.3622 0.1726 0.2724 0.2975 0.2218 reps4 | -0.0153 0.0384 0.3592 -0.0195 -0.0589 -0.1223 -0.0909 reps5 | -0.0351 0.4542 0.7065 -0.2337 -0.2498 -0.3925 -0.3492 | turn displa~t gear_r~o foreign reps1 reps2 reps3 -------------+--------------------------------------------------------------- turn | 1.0000 displacement | 0.8124 1.0000 gear_ratio | -0.7005 -0.8381 1.0000 foreign | -0.6768 -0.6383 0.7266 1.0000 reps1 | 0.0471 -0.0131 -0.0355 -0.1143 1.0000 reps2 | 0.2939 0.1733 -0.2468 -0.2395 -0.0626 1.0000 reps3 | 0.2526 0.3038 -0.2449 -0.3895 -0.1515 -0.3176 1.0000 reps4 | -0.1748 -0.1231 0.2287 0.2526 -0.1026 -0.2151 -0.5211 reps5 | -0.4110 -0.4093 0.2894 0.4863 -0.0752 -0.1577 -0.3820 | reps4 reps5 -------------+------------------ reps4 | 1.0000 reps5 | -0.2587 1.0000 Notice that it automatically ignored the string variable make. Not all commands will work this well, so _all may occasionally fail. 2.5.2 Pairwise completion vs complete case You may have noticed that the cor command reports the number of observations it used, for example, the first few correlations all used 74 observations, but the _all version used on 69. correlate uses what’s known as complete cases analysis - any observation missing any value used in the command is excluded. rep78 is missing 5 observations (run the misstable summarize command to see this). On the other hand, pairwise completion only excluded missing values from the relevant comparisons. If a given correlation doesn’t involve rep78, it will use all the data. We can obtain this with pwcorr. . corr rep78 price trunk (obs=69) | rep78 price trunk -------------+--------------------------- rep78 | 1.0000 price | 0.0066 1.0000 trunk | -0.1572 0.3232 1.0000 . pwcorr rep78 price trunk | rep78 price trunk -------------+--------------------------- rep78 | 1.0000 price | 0.0066 1.0000 trunk | -0.1572 0.3143 1.0000 Notice the two correlations involving rep78 are identical - the same set of observations are dropped in both. However, the correlation between price and trunk differs - incorrelate, it is only using 69 observations, whereas inpwcorr` it uses all 74. It may seem that pwcorr is always superior (and, in isolation it is). However, most models such as regression only support complete cases analysis, so in those cases, if you are exploring your data, it does not make sense to do pairwise comparison. Ultimately, the choice remains up to you. If the results from correlate and pwcorr do differ drastically, that is a sign of something else going on! 2.5.3 Spearman correlation One limitation of Pearson’s correlation is that it is detecting linear relationships only. A famous example of this is Anscombe’s quartet: In each pair, the Pearson correlation is an identical .8162! In the first, that’s what we want. In the second, the relationship is strong but non-linear. In the third, only one value is not perfectly correlated, so the Pearsons correlation is diminished. In the fourth, only the existence of the single outlier is driving the relationship. Spearman correlation is an alternative to Pearson correlation. It works by ranking each variable and then performing Pearson’s correlation. The command in Stata is spearman. . corr price trunk (obs=74) | price trunk -------------+------------------ price | 1.0000 trunk | 0.3143 1.0000 . spearman price trunk, matrix (obs=74) | price trunk -------------+------------------ price | 1.0000 trunk | 0.3996 1.0000 The matrix option forces output to mirror correlate, otherwise it produces a slightly different output when given only two variables. spearman uses complete cases; to use pairwise complete instead, pass the option pw: . spearman mpg-headroom, pw (obs=varies) | mpg rep78 headroom -------------+--------------------------- mpg | 1.0000 rep78 | 0.3098 1.0000 headroom | -0.4866 -0.1583 1.0000 How does Spearman’s correlation compare to Pearson’s for Anscombe’s quartet? Comparison Pearson Spearman \\(y_1, x_1\\) .8162 .8182 \\(y_2, x_2\\) .8162 .6909 \\(y_3, x_3\\) .8162 .9909 \\(y_4, x_4\\) .8162 .5000 The second correlation diminishes, the third drastically increases, and the fourth decreases as well. 2.6 Exercise 1 For these exercises, we’ll be using data from NHANES, the National Health And Nutrition Examination Survey. The data is on Stata’s website, and you can load it via webuse nhanes2, clear Use describe to get a sense of the data. How many observations? How many variables? Use tab, summarize, mean, and/or codebook to get an understanding of the some of variables that we’ll be using a lot going forward: region houssiz sex diabetes iron Does race have any missing data? Does diabetes? Does lead? What is more highly correlated? A person’s height and weight, or their diastolic and systolic blood pressure? We won’t cover in this class, but there are multiple-equation estimating commands which have syntax command (varlist) (varlist) … (varlist) [if] [in] [weight] [,options].↩ We could just run correlate, but the postestimation commands following mean are fairly limited, so bare with me here. Postestimation commands following models are much more interesting!↩ "],
["visualization.html", "Chapter 3 Visualization 3.1 The graph command 3.2 Other graphs 3.3 Plotting by group 3.4 Getting help on Graphs 3.5 Displaying multiple graphs simultaneously 3.6 Exercise 2", " Chapter 3 Visualization Stata has robust graphing capabilities that can both generate numerous types of plots, as well as modify them as needed. We’ll only cover the basics here, for a reference we would recommend A Visual Guide to Stata Graphics by Michael Mitchell, which lays out step-by-step syntax for the countless graphs that can be generated in Stata. Let’s reload the auto dataset to make sure we’re starting on the same page. . sysuse auto, clear (1978 Automobile Data) 3.1 The graph command Most (though not all, see some other graphs below) graphs in Stata are created by the graph command. Generally the syntax is graph &lt;type&gt; &lt;variable(s)&gt;, &lt;options&gt; The “type” is the subcommand. For example, to create a bar chart of price by rep78, we could run . graph bar price, over(rep78) For further information, we could instead construct a boxplot. . graph box price, over(rep78) There are a few other infrequently used graphs, see help graph for details. There is a plot subcommand, twoway, which takes additional sub-subcommands, and supports a wide range of types. graph twoway &lt;type&gt; &lt;variable(s)&gt;, &lt;options&gt; twoway creates most of the scatterplot-esque plots. The “types” in twoway are subcommands different from the subcommands in non-twoway graph, it takes options such as scatter to create a scatterplot: . graph twoway scatter mpg weight Note: For graph twoway commands, the graph is optional. E.g., these commands are equivalent: graph twoway scatter mpg weight twoway scatter mpg weight This is not true of commands like graph box. The options in the graphing commands are quite extensive and enable tweaking of many different settings. Rather than a full catalog of the options, here’s an example: . twoway scatter mpg weight, msymbol(s) /// &gt; mcolor(blue) /// &gt; mfcolor(yellow) /// &gt; msize(3) /// &gt; title(&quot;Mileage vs Weight&quot;) /// &gt; xtitle(&quot;Weight (in lbs)&quot;) /// &gt; ytitle(&quot;Mileage&quot;) /// &gt; ylabel(15 &quot;15&quot; 25 &quot;25&quot; 35 &quot;35&quot;) . Graphs made using twoway have an additional benefit - it is easy to stack them. For example, twoway lfit creates a best-fit line between the points: . twoway lfit mpg weight This isn’t really that useful. It would be much better to overlap those two - generate the scatter plot, then add the best fit line. We can easily do that by passing multiple plots to twoway: . twoway (scatter mpg weight) (lfit mpg weight) Note that the order of the plots matters - if you can tell, the best-fit line was drawn on top of the scatter plot points. If you reversed the order in the command (twoway (lfit mpg weight) (scatter mpg weight)), the line would be drawn first and the points on top of it. Finally, note that options can be passed to each individual plot: . twoway (scatter mpg weight, msymbol(t)) /// &gt; (lfit mpg weight, lcolor(green)) Putting these options “globally”, as twoway (…) (…), msymbol(to) would NOT work, as msymbol is an option specifically for twoway scatter (and a few others), not for the more general twoway. 3.2 Other graphs There are a very large number of graphs which do not exist under the graph command. Most are very niche, but the most important general example is histogram, which has its own command. . histogram mpg (bin=8, start=12, width=3.625) You can see a full list of the non-graph plots by looking at help graph other 3.3 Plotting by group All graph commands accept a by(&lt;grouping var&gt;) option which will repeat the graphing command for each level of the grouping variable, and display all graphs on the same output. For example, . hist mpg, by(foreign) Alternatively, you may way to represent another variable on a single plot. For example, let’s say we want to create the scatter plot and best-fit from above, but differentiate the genders on one graph (rather than two separate windows via by). To do this, we’d overlap two scatter and lfit plots in a single twoway, each with a conditional if. . twoway (scatter mpg weight if foreign == 0) /// &gt; (scatter mpg weight if foreign == 1) /// &gt; (lfit mpg weight if foreign == 0) /// &gt; (lfit mpg weight if foreign == 1) Notice that Stata automatically made each plot a separate color, but not in a logical fashion. Here’s a cleaned up version: . twoway (scatter mpg weight if foreign == 0, mcolor(orange)) /// &gt; (scatter mpg weight if foreign == 1, mcolor(green)) /// &gt; (lfit mpg weight if foreign == 0, lcolor(orange) lwidth(1.4)) /// &gt; (lfit mpg weight if foreign == 1, lcolor(green) lwidth(1.4)), /// &gt; legend(label(1 &quot;Domestic&quot;) label(2 &quot;Foreign&quot;) order(1 2)) /// &gt; title(&quot;Mileage vs Weight&quot;) xtitle(&quot;Weight (lbs)&quot;) /// &gt; ytitle(&quot;Mileage&quot;) (Since its not entirely clear from the code, the order(1 2) argument inside legend serves two purposes - first, it “orders” the entries in the legend box, but secondly and more importantly, it does not contain 3 or 4. If you look at the previous plot, it had four entries in the legend for the two scatters plus two lfits. By excluding 3 and 4 from order [3 and 4 corresponding to the two lfits], their legend entries are ignored.) 3.4 Getting help on Graphs There are a ton of options in all these graphs. Rather than list them all, we instead direct you to some various help pages. For general assistance, start with help graph Each individual type of graph has its own help page: help graph box help graph twoway help twoway scatter help histogram There are various generalized options which are the same over the variety of plots. These can be found in the documentation of each individual graph, or you can access them directly: Topics Help command Help with titles, subtitles, notes, captions. help title_options Axis labels, tick marks, scaling, etc. help axis_options Manipulating the legend help legend_options Modifying points (e.g. scatter) help marker_options Adding labels to markers help marker_label_options Options for any lines (e.g. lfit) help cline_options 3.5 Displaying multiple graphs simultaneously You may have noticed that opening a new plot closes the old one. What if you wanted to compare the plots? The behind-the-scenes reason that the old plots are closed is that Stata names each plot and each plot can only be open once. The default name is “Graph”, so with each new plot, the “Graph” plot is overridden. If you closed a plot and wanted to re-open it, you can run the following at any point until you run another graph. graph display Graph When we create a new plot with the default name, we lose the last one. If we give a plot a non-default name, it will be saved (so that it can be re-displayed later) and more importantly, will open a new window without closing the last. Running two plots with custom names opens two separate windows. (These are not run in the notes because obviously this won’t demonstrate well, but try them on your own. hist price, name(g1) hist mpg, name(g2) Names can be re-used (and plots re-generated) easily: hist price, title(&quot;Histogram of Price&quot;) name(g1, replace) We can also list (using dir), re-display (using display), or drop graphs (using drop): graph dir graph display g1 graph drop g1 graph drop _all Finally, if you’d rather have all the graphs in one window with tabs instead of separate windows, use set autotabgraphs on You still need to name graphs separately. 3.6 Exercise 2 Reload the NHANES data if you haven’t: webuse nhanes2, clear Using twoway scatter and twoway lfit, create a scatter plot of diastolic and systolic blood pressure, by gender. Be sure to color the lines and points consistenly and to clean up the legend. "],
["univariate-and-some-bivariate-analysis.html", "Chapter 4 Univariate (and some Bivariate) Analysis 4.1 One-sample t-test 4.2 Two-sample t-test 4.3 Chi-square test 4.4 Exercise 3 4.5 Citations", " Chapter 4 Univariate (and some Bivariate) Analysis We start with analyzing single variables at a time, and then quickly discuss a chi-squared test which is a bivariate analysis. While these tests form the basis of many other methods, by themselves they are of limited us. All the tests we discuss here come with two very strong assumptions: No other measured or unmeasured variables play a role in any relationship. The relationship is the same for any subpopulation of your data. To see why these assumptions are so strong, lets consider the two-sample t-test. We’ll discuss them in depth below, but the short version is that a two-sample t-test compares whether two groups have the same average value. If you were comparing average height between two groups, perhaps you’d find that one group was much taller than the other. But what if the shorter group was almost entirely made of children? We have no way of knowing whether the difference between groups is due to this or due to real differences. For the second assumption, imagine some scenario where you have two groups. Among men, there is a large difference between groups, whereas among women, there is no difference between groups. On average (assuming a roughly 50/50 gender split), you’d see a moderate difference. But no one actually has a moderate difference! The two subgroups have different effects, so the two-sample t-test captures neither! That said, there are a few situations where these tests are useful as stand-alone tests. Exploratory/descriptive/pilot studies: Situations when are you not making strong claims, merely describing something you see in the data. Very small sample sizes: While not ideal, small sample sizes can’t handle more complicated analysis, so these simple ones are all you have. In randomized controlled experiments (such as in a lab): In these situations, you truly can ensure that both those assumptions are met. Just randomization is not sufficient, as the benefit of randomization is only guaranteed theoretically for infinitely large samples. Randomization helps a lot, but it’s not perfect! Reload the auto data set . sysuse auto, clear (1978 Automobile Data) 4.1 One-sample t-test A one-sample t-test tests whether the mean of a variable is equal to some constant. It is not needed a lot of the time (if we hypothesize that the average test score is 70, and every students get above an 80, why would we need to test this?), but we introduce it here just as a basis of further methods. There are several assumptions necessary for a one-sample t-test, most of which are trivial/not that important. The two relatively important ones are Independence. Each value must come from an independent source. If you have repeated measures (e.g. two different measures for the same person), this is violated. See the section of mixed models for dealing with this sort of data. The distribution of the mean is normal. Note that this assumption is not about the data itself. This assumption is valid if either the sample suggests that the data is normal (a bell-curve) or the sample size is large (above ~30)3. If this assumption does not hold, we generally still use the t-test, although there are tests called “non-parametric” tests which do not require this assumption. Not everyone is convinced they are necessary. There is no way to test the independence assumption, you must determine this based upon your knowledge of the data. We can try and determine whether the normal distribution is valid for the data, but note that failing to find this does not imply the normality of the mean assumption is violated. This can be checked with a histogram or a qq-plot. Let’s test whether the average mileage in the data is different from 20. . hist mpg, normal (bin=8, start=12, width=3.625) In a histogram, we are looking for violations from the bell-shape of a normal curve. We added the normal curve to the plot by the normal option. We can see that while the data is not perfectly normal, it also is not too far off. We can look at a qq-plot for further details. . qnorm mpg With a qq-plot, we are looking for the points to roughly fall in a straight line, alone the 45-degree line. Here we see some mild violations of those, especially at either end. Put together, this indicates that normality is not perfect, but its also not a terrible violation. However, we don’t really care! The sample size is large enough that that assumption is valid. We can therefore perform our test using the ttest command. . ttest mpg == 20 One-sample t test ------------------------------------------------------------------------------ Variable | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- mpg | 74 21.2973 .6725511 5.785503 19.9569 22.63769 ------------------------------------------------------------------------------ mean = mean(mpg) t = 1.9289 Ho: mean = 20 degrees of freedom = 73 Ha: mean &lt; 20 Ha: mean != 20 Ha: mean &gt; 20 Pr(T &lt; t) = 0.9712 Pr(|T| &gt; |t|) = 0.0576 Pr(T &gt; t) = 0.0288 Our null hypothesis, the claim we are trying to see whether we have the evidence to reject, is that the average mpg in the population represented by this data is 20. We sometimes also just call 20, the value the null hypothesis is testing, the null hypothesis (e.g. “We are testing against the null hypothesis of 20.”). Looking at the analysis, we see that we get some summary statistics. The 95% confidence interval represents the range which, if we were to draw repeated samples of the same size (74) from the population of all cars, we would expect 95% of the estimated means to fall in that range. Below that, we see some details of the test being run. The important detail is that the null Hypothesis (Ho) is what you meant for it to be. Finally, we get our p-values. There are three separate ones given for three different alternative hypotheses - testing whether the mean is less than 20, greater than 20, or simply not equal to 20. Each p-value represents the probability of observing a mean as extreme as the one we saw if 20 was the true population mean. In other words, we get the following three interpretations: There is 97.12% chance that if the true mean were 20, we would observe a sample mean of 21.2973 or lower. There is 2.88% chance that if the true mean were 20, we would observe a sample mean of 21.2973 or higher. There is 5.76% chance that if the true mean were 20, we would observe a sample mean of 21.2973 or higher or a sample mean of 18.7027. The last interpretation (corresponding to Ha: mean != 20 from the output) is known as the two-sided p-value, and should be your default. The other two, known as one-sided p-values, can be used if a priori you decide that you’re only interested in one direction. The typical threshold used is p = .05, so we would fail to reject the null hypothesis (since .0576 &gt; .05), although this is extremely close to significant! (Of course that first p-value &amp; interpretation is unnecessary - the observed test statistic is greater than 20, so there’s no chance we’d be able to make the argument that the true mean is less than 20!) 4.2 Two-sample t-test While the one-sided t-test isn’t used very often, the two-sample version is. There are two different variations of it. 4.2.1 Independent The independent two-sample t-test arises when you have two independent groups which have the same measurement, and you wish to test whether the mean of the two groups is equivalent. The same basic assumptions, independence (although here it’s both within group [each observation is independent from each other] and between groups [the two groups are independent, e.g. these are not repeated measures on the same person]) and normality of the mean. For example in our data, we can test whether foreign and American cars have the same average mileage. We will again use the ttest command, though slightly differently. . ttest mpg, by(foreign) Two-sample t test with equal variances ------------------------------------------------------------------------------ Group | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- Domestic | 52 19.82692 .657777 4.743297 18.50638 21.14747 Foreign | 22 24.77273 1.40951 6.611187 21.84149 27.70396 ---------+-------------------------------------------------------------------- combined | 74 21.2973 .6725511 5.785503 19.9569 22.63769 ---------+-------------------------------------------------------------------- diff | -4.945804 1.362162 -7.661225 -2.230384 ------------------------------------------------------------------------------ diff = mean(Domestic) - mean(Foreign) t = -3.6308 Ho: diff = 0 degrees of freedom = 72 Ha: diff &lt; 0 Ha: diff != 0 Ha: diff &gt; 0 Pr(T &lt; t) = 0.0003 Pr(|T| &gt; |t|) = 0.0005 Pr(T &gt; t) = 0.9997 The output is very similar to above. We get some descriptives by group, and combined, as well as a confidence interval for the difference. Notice that the null hypothesis (Ho) is simply 0 because we are testing whether the difference between the groups is statistically significant, regardless of the actual values of the means. The p-values have the same interpretation, though in terms of the difference of means rather than just the means. More-so than even in the one-sided test, you should use the two-sided p-value, so our p-value is 0.0005. Just as a note, do not report this p-value as 0 if you round! Instead, report that the p-value is less than .01 or less than .001. However, there is an additional assumption here that we neglected; namely that the variance of the two groups is equivalent. If you make this assumption, then the above analysis is sufficient. However, if you don’t make this assumption, you can instead run the two-sample t-test with unequal variances by adding the unequal option. . ttest mpg, by(foreign) unequal Two-sample t test with unequal variances ------------------------------------------------------------------------------ Group | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- Domestic | 52 19.82692 .657777 4.743297 18.50638 21.14747 Foreign | 22 24.77273 1.40951 6.611187 21.84149 27.70396 ---------+-------------------------------------------------------------------- combined | 74 21.2973 .6725511 5.785503 19.9569 22.63769 ---------+-------------------------------------------------------------------- diff | -4.945804 1.555438 -8.120053 -1.771556 ------------------------------------------------------------------------------ diff = mean(Domestic) - mean(Foreign) t = -3.1797 Ho: diff = 0 Satterthwaite's degrees of freedom = 30.5463 Ha: diff &lt; 0 Ha: diff != 0 Ha: diff &gt; 0 Pr(T &lt; t) = 0.0017 Pr(|T| &gt; |t|) = 0.0034 Pr(T &gt; t) = 0.9983 The unequal test is slightly more conservative, but there has been some work (e.g. Delacre et al 2017, Ruxton 2006) showing that you should always use the unequal version. The equal variance version (the first one we ran) is known as Student’s4 t-test, and the unequal variance version (with the unequal option) is known as Welch’s t-test. If you want to test whether the variance between the two groups is equal, you can use sdtest in a similar fashion to ttest (sdtest mpg, by(foreign)). 4.2.2 Paired We noted in the assumptions above that we need the two groups to be independent. What if they aren’t? An example of paired data would include before-and-after measures or measures from two family members. In both cases, it is reasonable to assume the two measures from the same person or family are more similar than a measure from one person against a measure from another person. The auto data set does not have a good example of paired data, so lets use the “bpwide” data instead. . sysuse bpwide, clear (fictional blood-pressure data) . list in 1/5 +-----------------------------------------------+ | patient sex agegrp bp_bef~e bp_after | |-----------------------------------------------| 1. | 1 Male 30-45 143 153 | 2. | 2 Male 30-45 163 170 | 3. | 3 Male 30-45 153 168 | 4. | 4 Male 30-45 153 142 | 5. | 5 Male 30-45 146 141 | +-----------------------------------------------+ We have measures from different patients, including a before and after measure of the blood pressure. We again use ttest to perform the test: . ttest bp_after == bp_before Paired t test ------------------------------------------------------------------------------ Variable | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- bp_after | 120 151.3583 1.294234 14.17762 148.7956 153.921 bp_bef~e | 120 156.45 1.039746 11.38985 154.3912 158.5088 ---------+-------------------------------------------------------------------- diff | 120 -5.091667 1.525736 16.7136 -8.112776 -2.070557 ------------------------------------------------------------------------------ mean(diff) = mean(bp_after - bp_before) t = -3.3372 Ho: mean(diff) = 0 degrees of freedom = 119 Ha: mean(diff) &lt; 0 Ha: mean(diff) != 0 Ha: mean(diff) &gt; 0 Pr(T &lt; t) = 0.0006 Pr(|T| &gt; |t|) = 0.0011 Pr(T &gt; t) = 0.9994 We see that the blood pressure after is slightly lower (151 vs 156), and the two-sided p-value is statistically significant! While we technically used the paired t-test for this, behind the scenes, this is identical to generating a difference variable (e.g. post score - pre score) and performing the one-sample t-test: . gen bp_change = bp_after - bp_before . ttest bp_change == 0 One-sample t test ------------------------------------------------------------------------------ Variable | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- bp_cha~e | 120 -5.091667 1.525736 16.7136 -8.112776 -2.070557 ------------------------------------------------------------------------------ mean = mean(bp_change) t = -3.3372 Ho: mean = 0 degrees of freedom = 119 Ha: mean &lt; 0 Ha: mean != 0 Ha: mean &gt; 0 Pr(T &lt; t) = 0.0006 Pr(|T| &gt; |t|) = 0.0011 Pr(T &gt; t) = 0.9994 4.3 Chi-square test While t-tests can technically be used with binary data or ordinal variables5, they cannot with non-ordinal. If we want to compare two categorical or binary variables, we can instead use a \\(\\chi^2\\) test (\\(\\chi\\) is a Greek letter which is spelled “chi” in English, and rhymes with “why”). There are a few variations on the \\(\\chi^2\\) test, the version we talk of here is a test of association, where we are testing the null hypothesis that the distribution of one variable is the same at every level of another variable. Let’s return to the auto data set, and compare rep78 (Repair Record 1978) and foreign. We’ll start by looking at a crosstab. . sysuse auto, clear (1978 Automobile Data) . tab foreign rep78 | Repair Record 1978 Car type | 1 2 3 4 5 | Total -----------+-------------------------------------------------------+---------- Domestic | 2 8 27 9 2 | 48 Foreign | 0 0 3 9 9 | 21 -----------+-------------------------------------------------------+---------- Total | 2 8 30 18 11 | 69 If you’re not familiar with crosstabs, each cell represents the number of observations which fall into the category - for example, there are 8 cars which are both Domestic and have a Repair record of 2. We can think of testing whether the association between these two variables exists from either direction - we can either ask “Is the probability of a car having a specific repair record the same among domestic cars as among foreign cars”, or “Is the probability of a car being foreign or domestic the same across all levels of repair record”. We test it by adding the chi2 option to tab: . tab foreign rep78, chi2 | Repair Record 1978 Car type | 1 2 3 4 5 | Total -----------+-------------------------------------------------------+---------- Domestic | 2 8 27 9 2 | 48 Foreign | 0 0 3 9 9 | 21 -----------+-------------------------------------------------------+---------- Total | 2 8 30 18 11 | 69 Pearson chi2(4) = 27.2640 Pr = 0.000 Here we can see that the p-value (pr) associated with the \\(\\chi^2\\) test is very low (less than .001, p-values are never 06!), so we can reject the null hypothesis and claim there is evidence of a statistically significant association between foreign and rep78. Note that the \\(\\chi^2\\) test ignores scale - if we had 4,800 Domestic cars instead of 48, and the percentage at each Repair Record were the same (e.g. instead of 8 Domestic cars with Repair Record of 2, we had 800), we would get the same test result. Finally, the critiques above about the lack of usefulness of t-tests extends to \\(\\chi^2\\) tests as well. 4.4 Exercise 3 Load up the NHANES data. sysuse nhanes2, clear We don’t care about normality assumptions here. Why? The average height of all men in America is 5’9&quot; (176 cm). Does this sample appear representative of this fact? (E.g. is there evidence that the average in the sample differs from this?) We would expect height and weight to differ by gender. Is there any evidence that age does? Take a look at a crosstab of race and diabetes. Does it look like the percentage of diabetics is the same across race? The col or row option might help here. Test this with a \\(\\chi^2\\) test. 4.5 Citations Delacre, Marie, Daniël Lakens, and Christophe Leys. “Why Psychologists Should by Default Use Welch’s t-test Instead of Student’s t-test.” International Review of Social Psychology 30.1 (2017). Ruxton, Graeme D. “The unequal variance t-test is an underused alternative to Student’s t-test and the Mann–Whitney U test.” Behavioral Ecology 17.4 (2006): 688-690. This is by the central limit theorem.↩ Named not for students in a class, but the pseudonym for statistician William Sealy Gosset. Gosset worked at the Guinness brewery when he was performing some of his seminal work and wasn’t allowed to publish under his own name, so he used the pseudonym Student.↩ Categorical variables that are ordered in some sense - e.g. clothing sizes of small, medium and large.↩ Unless you’re in a pathological setting, i.e. you’re testing whether the average height of a population is less than 6 feet, and somehow you get an average height of -20 feet….↩ "],
["regression.html", "Chapter 5 Regression 5.1 Terminology 5.2 Linear Regression 5.3 Exercise 4 5.4 Logistic Regression 5.5 Other regression models 5.6 Exercise 5", " Chapter 5 Regression One notable exclusion from the previous chapter was comparing the mean of a continuous variables across three or more groups. Two-sample t-tests compare the means across two groups, and \\(\\chi^2\\) tests can compare two categorical variables with arbitrary number of levels, but the traditional test for comparing means across multiple groups is ANOVA (ANalysis Of VAriance). While historically this has been a very useful procedure due to the ease with which it can be performed manually, its modern use has been supplanted by regression, which is mathematically equivalent and easier to extend (the downside of regression is that it is more difficult to calculate, but given that we are no longer doing statistical analyses by hand…). This relationship extends to other variations of ANOVA such as ANCOVA or MANOVA. If you still want to fit an ANOVA, it can be done with the oneway command. Otherwise we turn now to regression. Regression is a set of techniques where we attempt to fit a model to a data set estimating the relationships between a set of predictor variables (either continuous or categorical in nature) and a response variable of interest. There are many versions of regression which are appropriate for different types of response variables, or address different concerns when fitting the model. In this chapter and the next, we will discuss a few variations. 5.1 Terminology When discussing any form of regression, we think of predicting the value of one variable7 based upon several other variables. The variable we are predicting can be called the “outcome”, the “response” or the “dependent variable”. The variables upon which we are predicting can be called “predictors”, “covariates”, or “independent variables”. 5.2 Linear Regression Linear regression (also known as Ordinary Least Squares (OLS) regression) is the most basic form of regression, where the response variable is continuous. Technically the response variable can also be binary or categorical but there are better regression models for those types of outcomes. Linear regression fits this model: \\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p + \\epsilon \\] \\(Y\\) represents the outcome variable. \\(X_1, X_2, \\cdots, X_p\\) represent the predictors, of which there are \\(p\\) total. \\(\\beta_0\\) represents the intercept. If you have a subject for which every predictor is equal to zero, \\(\\beta_0\\) represents their predicted outcome. The other \\(\\beta\\)’s are called the coefficients, and represent the relationship between each predictor and the response. We will cover their interpretation in detail later. \\(\\epsilon\\) represents the error. Regression is a game of averages, but for any individual observation, the model will contain some error. Linear regression models can be used to predict expected values on the response variable given values on the predictors, and \\(\\epsilon\\) represents the difference between a prediction based on the model and what the actual value of the response variable is. Stata can be used to estimate the regression coefficients in a model like the one above, and perform statistical tests of the null hypothesis that the coefficients are equal to zero (and thus that predictor variables are not important in explaining the response). Note that the response \\(Y\\) is modeled as a linear combination of the predictors and their coefficients. Some introductory statistical classes distinguish between simple regression (with only a single predictor) and multiple regression (with more than one predictor). While this is useful for developing the theory of regression, simple regression is not commonly used for real analysis, as it ignores one of the main benefits of regression, controlling for other predictors (to be discussed later). We will now fit a model, discussing assumptions afterwards, because almost all assumption checks can only occur once the model is fit! 5.2.1 Fitting the model Stata’s regress command fit the linear regression model. It is followed by the outcome variable followed by all predictors. For this example, let’s use the auto data and fit a relatively simple model, predicting mpg based on gear_ratio and headroom. . regress mpg gear_ratio headroom Source | SS df MS Number of obs = 74 -------------+---------------------------------- F(2, 71) = 25.48 Model | 1021.0804 2 510.540202 Prob &gt; F = 0.0000 Residual | 1422.37906 71 20.0335078 R-squared = 0.4179 -------------+---------------------------------- Adj R-squared = 0.4015 Total | 2443.45946 73 33.4720474 Root MSE = 4.4759 ------------------------------------------------------------------------------ mpg | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- gear_ratio | 6.801356 1.240026 5.48 0.000 4.328815 9.273898 headroom | -1.443796 .6688077 -2.16 0.034 -2.77736 -.1102312 _cons | 5.113759 4.889846 1.05 0.299 -4.636317 14.86384 ------------------------------------------------------------------------------ There is a lot of important output here, so we will step through each piece. First, the top left table is the ANOVA table. If you were to fit a regression model with a single categorical predictor, this would be identical to running ANOVA via oneway. In general we don’t need to interpret anything here, as there are further measures of model fit in the regression frameworks. Next, the top right part has a series of measures. Regression performs complete case analysis - any observations missing any variable involved in this model is ignored in the model. (See multiple imputation for details on getting around this.) Check “Number of obs” to ensure the number of observations is what you expect. Here, the data has 74 rows, so the regression model is using all the data (there is no missingness in mpg, weight or displacement). The F-test which follows (“F(2, 71)”8 and “Prob &gt; F”) is testing the null hypothesis that all coefficients are 0. In other words, if this test fails to reject, the conclusion is the model captures no relationships. In this case, do not continue interpreting the results; either your conclusion is that there is no relationship, or you need to return to the model design phase. If this test does reject, you can continue interpreting. The \\(R^2\\) (“R-squared”) is a measure of model fit. It ranges from 0 to 1 and is a percentage, explaining what percent in the variation in the response is explained by the linear relationship with the predictors. What’s considered a “large” \\(R^2\\) depends greatly on your field and the situation, in very general terms, .6 is good and above .8 is great. However, if you know that there are a lot of unmeasured variables, a much smaller \\(R^2\\) can be considered good as well. Mathematically, adding a new predictor to the model will increase the \\(R^2\\), regardless of how useless the variable is.9 This makes \\(R^2\\) poor for model comparison, as it would always select the model with the most predictors. Instead, the adjusted \\(R^2\\) (“Adj R-Squared”) accounts for this; it penalizes the \\(R^2\\) by the number of predictors in the model. Use the \\(R^2\\) to measure model fit, use the adjusted \\(R^2\\) for model comparison. The root mean squared error (“Root MSE”, as known as RMSE) is a measure of the average difference between the observed outcome and the predicted outcome. It can be used as another measure of model fit, as it is on the scale of the outcome variable. So for this example, the RMSE is 4.4759 so the average error in the model is about 4.5 mpg. Finally, we get to the coefficient table. Each row represents a single predictor. The “_cons” row is the intercept; it’s Coef. of 5.1138 represents the average response when all other predictors are 0. This is usually not interesting; how many cars weighing 0 lbs do you know of? So we’ll ignore this and instead go over the other rows. “Coef.”: These are the \\(\\beta\\) from the above model. We interpret each as “For a 1 increase in the value of the covariate with all other predictors held constant, we would predict this change in the response, on average.” For example, for every additional inch10 of headroom in a car (while its gear ratio is constant), it is predicted to have an average of 1.4438 lower mpg. “Std. Err.”: This represents the error attached to the coefficient. This is rarely interpreted; but if it gets extremely large or extremely small (and the Coef. doesn’t likewise go to extremes), its an indication there may be something wrong. “t”: This is the standardized coefficient, calculated as Coef./Std. Err. We can’t directly compare the Coef.’s because of the different scales, but we can examine the standardized coefficients to get a sense of which predictor has a larger impact. In this model, we see that the impact of weight is much more than the impact of displacement. “P&gt;|t|”: The p-value testing whether the coefficient is significantly different than 0. In this model, we see that both gear_ratio and headroom have significant p-values. “[95% Conf. interval]”: A range of possible values. Whenever we look at any model, a distinction needs to be drawn between statistical significance and practical significance. While these two interpretations of significance often align, they are not guaranteed to. We often have statistical significance (a p-value less than .05) when there is no practical significance (aka clinical significance, a difference that isn’t scientifically interesting). This is mostly a function of sample size; with a large sample even very small effects can have small p-values. Alternatively, a large practical significance with a low statistical significance can occur with very noisy data or a small sample size, which might indicate further study with a larger sample is needed. 5.2.2 Including categorical predictors Let’s say we want to add rep78, a categorical variable with 5 levels, to the model. Naively, we simply add it: . regress mpg gear_ratio headroom rep78 Source | SS df MS Number of obs = 69 -------------+---------------------------------- F(3, 65) = 19.94 Model | 1121.49787 3 373.832624 Prob &gt; F = 0.0000 Residual | 1218.70503 65 18.7493081 R-squared = 0.4792 -------------+---------------------------------- Adj R-squared = 0.4552 Total | 2340.2029 68 34.4147485 Root MSE = 4.33 ------------------------------------------------------------------------------ mpg | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- gear_ratio | 6.631208 1.330095 4.99 0.000 3.974825 9.287591 headroom | -1.22008 .6651028 -1.83 0.071 -2.548382 .1082225 rep78 | .9568854 .5816842 1.65 0.105 -.2048182 2.118589 _cons | 1.802318 4.849991 0.37 0.711 -7.883782 11.48842 ------------------------------------------------------------------------------ We only get a single coefficient. Stata is treating rep78 as continuous. When including a categorical predictor, Stata will create dummy variables (variables which take on value 1 if the observation is in that category and 0 otherwise) and include all but one, which is the reference (or baseline). Since we only see a single coefficient here, we know Stata did it incorrectly. The issue is that Stata doesn’t know we want to treat rep78 as categorical. If we prefix the variable name with i., Stata will know it is categorical. . regress mpg gear_ratio headroom i.rep78 Source | SS df MS Number of obs = 69 -------------+---------------------------------- F(6, 62) = 11.50 Model | 1232.82371 6 205.470619 Prob &gt; F = 0.0000 Residual | 1107.37918 62 17.8609546 R-squared = 0.5268 -------------+---------------------------------- Adj R-squared = 0.4810 Total | 2340.2029 68 34.4147485 Root MSE = 4.2262 ------------------------------------------------------------------------------ mpg | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- gear_ratio | 6.847879 1.308106 5.23 0.000 4.233012 9.462745 headroom | -.9726907 .688578 -1.41 0.163 -2.349138 .4037571 | rep78 | 2 | 1.203595 3.505518 0.34 0.733 -5.803835 8.211025 3 | .0441395 3.232826 0.01 0.989 -6.418187 6.506466 4 | -.0010255 3.309233 -0.00 1.000 -6.616088 6.614037 5 | 4.401329 3.363577 1.31 0.196 -2.322367 11.12502 | _cons | 2.809121 5.272816 0.53 0.596 -7.731087 13.34933 ------------------------------------------------------------------------------ First, note that headroom no longer has a significant coefficient! This implies that rep78 and headroom are correlated, and in the first model where we did not include rep78, all of rep78’s effect was coming through headroom. Once we control for rep78, headroom is no longer significant. We will discuss multicollinearity later, as well as why this is why model selection is bad. Now we see 4 rows for rep78, each corresponding to a comparison between response 1 and the row. For example, the first row, 2, is saying that when rep78 is 2 compared to when it is 1 (with gear_ratio and headroom held at some fixed level), the average predicted response drops by 1.204 (though it is not statistical significant). The last row, 5, is saying that when rep78 is 5 compare to when it is 1 (with gear_ratio and headroom held at some fixed level, the average predicted response increases by 4.401 (again, not statistically significant). To see the other comparisons (does 2 differ from 4?), we can use the margins command. . margins rep78 Predictive margins Number of obs = 69 Model VCE : OLS Expression : Linear prediction, predict() ------------------------------------------------------------------------------ | Delta-method | Margin Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- rep78 | 1 | 20.42972 3.123396 6.54 0.000 14.18614 26.6733 2 | 21.63332 1.548599 13.97 0.000 18.53771 24.72892 3 | 20.47386 .7900386 25.92 0.000 18.8946 22.05313 4 | 20.4287 1.021406 20.00 0.000 18.38694 22.47046 5 | 24.83105 1.341573 18.51 0.000 22.14929 27.51282 ------------------------------------------------------------------------------ . margins rep78, pwcompare(pv) Pairwise comparisons of predictive margins Model VCE : OLS Expression : Linear prediction, predict() ----------------------------------------------------- | Delta-method Unadjusted | Contrast Std. Err. t P&gt;|t| -------------+--------------------------------------- rep78 | 2 vs 1 | 1.203595 3.505518 0.34 0.733 3 vs 1 | .0441395 3.232826 0.01 0.989 4 vs 1 | -.0010255 3.309233 -0.00 1.000 5 vs 1 | 4.401329 3.363577 1.31 0.196 3 vs 2 | -1.159456 1.698355 -0.68 0.497 4 vs 2 | -1.204621 1.896518 -0.64 0.528 5 vs 2 | 3.197733 2.129823 1.50 0.138 4 vs 3 | -.0451649 1.315328 -0.03 0.973 5 vs 3 | 4.357189 1.601822 2.72 0.008 5 vs 4 | 4.402354 1.642697 2.68 0.009 ----------------------------------------------------- The first margins call, without any options, displays the marginal means for each category - if every car had rep78 at those levels, it’s the average predicted mileage of all cars. The t-test here is useless - it’s only testing that the average mileage of the cars in each group is not 0! The second margins call adds the pwcompare(pv) option, which performs pairwise test between each pair of rep78 levels. This is similar to a post-hoc test from ANOVA if you are familiar with it. The only statistical significance we find is 5 vs 3 and 5 vs 4, suggesting that 5 is dissimilar from 3 and 4. (Confusingly, 3 and 4 are not dissimilar from 1 or 2, but 5 is similar to 1 and 2! These sort of things can happen; its best to focus only on the comparisons that are of theoretical interest.) By default, using i. makes the first level (lowest numerical value) as the reference category. You can adjust this by using ib#. instead, such as: . regress mpg headroom gear_ratio ib3.rep78 Source | SS df MS Number of obs = 69 -------------+---------------------------------- F(6, 62) = 11.50 Model | 1232.82371 6 205.470619 Prob &gt; F = 0.0000 Residual | 1107.37918 62 17.8609546 R-squared = 0.5268 -------------+---------------------------------- Adj R-squared = 0.4810 Total | 2340.2029 68 34.4147485 Root MSE = 4.2262 ------------------------------------------------------------------------------ mpg | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- headroom | -.9726907 .688578 -1.41 0.163 -2.349138 .4037571 gear_ratio | 6.847879 1.308106 5.23 0.000 4.233012 9.462745 | rep78 | 1 | -.0441395 3.232826 -0.01 0.989 -6.506466 6.418187 2 | 1.159456 1.698355 0.68 0.497 -2.235507 4.554419 4 | -.0451649 1.315328 -0.03 0.973 -2.674469 2.584139 5 | 4.357189 1.601822 2.72 0.008 1.155193 7.559185 | _cons | 2.853261 4.978251 0.57 0.569 -7.098121 12.80464 ------------------------------------------------------------------------------ This does not fit a different model. Both models (with i.rep78 and ib3.rep78) are identical, we’re just seeing slight variations. If the models do change (especially the model fit numbers in the top right), something has gone wrong. 5.2.3 Interactions Each coefficient we’ve look at so far is only testing whether there is a relationship between the predictor and response when the other predictors are held constant. What if we think the relationship changes based on the value of other predictors? For example, we might be interested in whether the relationship between a car’s headroom and its mileage depends on it’s gear ratio. Perhaps we think that cars with higher gear ratio (a high gear ratio is indicative of a sportier car) won’t be as affected by headroom as a stand-in for size, because sportier cars generally are better made. Mathematically an interaction is nothing more than a literal multiplication. For example, if our model has only two predictors, \\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon \\] then to add an interaction between \\(X_1\\) and \\(X_2\\), we simply add a new multiplicative term. \\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1\\times X_2) + \\epsilon \\] \\(\\beta_1\\) represents the relationship between \\(X_1\\) and \\(Y\\) when \\(X_2\\) is identically equal to 0. \\(\\beta_2\\) represents the relationship between \\(X_2\\) and \\(Y\\) when \\(X_1\\) is identically equal to 0. \\(\\beta_3\\) represents both: the change in the relationship between \\(X_1\\) and \\(Y\\) as \\(X_2\\) changes. the change in the relationship between \\(X_2\\) and \\(Y\\) as \\(X_1\\) changes. Adding these to the regress call is almost as easy. We’ll use # or ## instead. # includes only the interaction, whereas ## includes both the interaction and the main effects. a#b: Only the interaction a##b: Main effect for a, main effect for b, and the interaction. a b a#b: Same as a##b a b a##b: Same as a##b, except it’ll be uglier because you’re including main effects twice and one will be ignored. . regress mpg c.headroom##c.gear_ratio i.rep78 Source | SS df MS Number of obs = 69 -------------+---------------------------------- F(7, 61) = 9.97 Model | 1248.72541 7 178.389344 Prob &gt; F = 0.0000 Residual | 1091.47749 61 17.8930736 R-squared = 0.5336 -------------+---------------------------------- Adj R-squared = 0.4801 Total | 2340.2029 68 34.4147485 Root MSE = 4.23 ------------------------------------------------------------------------------ mpg | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- headroom | -6.118452 5.501801 -1.11 0.270 -17.11998 4.88308 gear_ratio | 1.663074 5.653574 0.29 0.770 -9.641945 12.96809 | c.headroom#| c.gear_ratio | 1.706674 1.810387 0.94 0.350 -1.913418 5.326766 | rep78 | 2 | 1.420052 3.516173 0.40 0.688 -5.610971 8.451075 3 | .3597184 3.253001 0.11 0.912 -6.14506 6.864496 4 | .7112303 3.397286 0.21 0.835 -6.082064 7.504524 5 | 4.839447 3.398527 1.42 0.160 -1.956328 11.63522 | _cons | 18.27442 17.23312 1.06 0.293 -16.18532 52.73417 ------------------------------------------------------------------------------ Note that we used c., similar to i.. c. forces Stata to treat it as continuous. Stata assumes anything in an interaction is categorical, so we need c. here! This can get pretty confusing, but it’s never wrong to include i. or c. when specifying a regression. Once we include an interaction, the relationship between the variables included in the interaction and the response are not constant - the relationship depends on the value of the other interacted variables. This can be hard to visualize with the basic regression output, so we’ll look at margins again instead. We’ll want to look at the relationship between mpg and headroom at a few different values of gear_ratio to get a sense of the pattern. gear_ratio ranges from 2.19 to 3.89 (this can be obtained with summarize or codebook, just don’t forget to save the results or re-run the regress command to gain access to the postestimation commands again), so let’s look at the relationship at those extremes and at 3: . margins, dydx(headroom) at(gear_ratio = (2.19 3 3.89)) Average marginal effects Number of obs = 69 Model VCE : OLS Expression : Linear prediction, predict() dy/dx w.r.t. : headroom 1._at : gear_ratio = 2.19 2._at : gear_ratio = 3 3._at : gear_ratio = 3.89 ------------------------------------------------------------------------------ | Delta-method | dy/dx Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- headroom | _at | 1 | -2.380836 1.645048 -1.45 0.153 -5.670313 .9086406 2 | -.9984305 .6897375 -1.45 0.153 -2.377646 .3807848 3 | .5205093 1.727385 0.30 0.764 -2.933611 3.974629 ------------------------------------------------------------------------------ While none of the p-values are significant, let’s pretend they were for the sake of discussion. Notice the pattern in the “dy/dx” column. With a low gear ratio, the relationship between headroom and mpg is negative - a larger headroom car is predicted to have lower mileage. At the other end, with a high gear ratio, the relationship is much closer to 0, and perhaps even slightly positive. Follow this with a call to marginsplot for a great visualization: . marginsplot Variables that uniquely identify margins: gear_ratio With low gear_ratio, there is a negative relationship between headroom and mileage - adding headroom to a low gear ratio car is predicted to decrease mileage, on average. However, the effect decreases as gear ratio increases, and at high levels of gear ratio, there is no longer any relationship. You can detect this by looking at the means (the points) and the confidence bands; here there is no relationship at all, but there is some suggestion that the relationship we describe may be occurring. Note that the choice of looking at the effect of headroom for different levels of gear ratio was arbitrary; we could have easily looked at the effect of gear ratio for different levels of headroom (just swap what’s the in the dydx( ) and at( ) options). The choice in a real modeling situation should depend on which is more interesting. 5.2.3.1 Centering Some sources suggest centering continuous predictors before including them in an interaction. This can help slightly with interpretation (the main effects are the relationship when the other variable involved in the interaction are at their mean, rather than at zero) but doesn’t actually affect model fit. To center, use the following: . summ gear_ratio Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- gear_ratio | 74 3.014865 .4562871 2.19 3.89 . gen gear_ratioc = gear_ratio - `r(mean)' . summ headroom Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- headroom | 74 2.993243 .8459948 1.5 5 . gen headroomc = headroom - `r(mean)' . summ gear_ratioc headroomc Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- gear_ratioc | 74 2.92e-09 .4562871 -.8248648 .8751352 headroomc | 74 1.56e-08 .8459948 -1.493243 2.006757 . regress mpg c.headroomc##c.gear_ratioc i.rep78 Source | SS df MS Number of obs = 69 -------------+---------------------------------- F(7, 61) = 9.97 Model | 1248.72541 7 178.389344 Prob &gt; F = 0.0000 Residual | 1091.47749 61 17.8930736 R-squared = 0.5336 -------------+---------------------------------- Adj R-squared = 0.4801 Total | 2340.2029 68 34.4147485 Root MSE = 4.23 ------------------------------------------------------------------------------ mpg | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- headroomc | -.973061 .689197 -1.41 0.163 -2.351196 .4050735 gear_ratioc | 6.771564 1.311782 5.16 0.000 4.148494 9.394633 | c.headroomc#| c. | gear_ratioc | 1.706674 1.810387 0.94 0.350 -1.913418 5.326765 | rep78 | 2 | 1.420052 3.516173 0.40 0.688 -5.610971 8.451075 3 | .3597184 3.253001 0.11 0.912 -6.14506 6.864496 4 | .7112302 3.397286 0.21 0.835 -6.082064 7.504524 5 | 4.839447 3.398527 1.42 0.160 -1.956328 11.63522 | _cons | 20.37576 3.132586 6.50 0.000 14.11177 26.63975 ------------------------------------------------------------------------------ If you compare fit characteristics and the interaction coefficient (and other coefficients), you’ll notice nothing has changed save the coefficient for headroomc and gear_ratioc. If we were to re-run the margins commands from before, we’d see the same results. 5.2.4 Robust standard errors The standard error associated with each coefficient are determined with the assumption that the model is “true” and that, were we given an infinite sample size, the estimates \\(\\hat{\\beta}\\) would converge to the true \\(\\beta\\). In many situations, this is clearly untrue. If you believe this is untrue, the estimates will be unaffected, but their standard errors will be incorrect. We can adjust for this by using “robust” standard errors, also known as Sandwich estimators or Huber-White estimators, with the vce(robust) option to regress. . regress mpg c.headroom##c.gear_ratio i.rep78, vce(robust) Linear regression Number of obs = 69 F(7, 61) = 13.06 Prob &gt; F = 0.0000 R-squared = 0.5336 Root MSE = 4.23 ------------------------------------------------------------------------------ | Robust mpg | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- headroom | -6.118452 4.773167 -1.28 0.205 -15.66299 3.426085 gear_ratio | 1.663074 5.162335 0.32 0.748 -8.659652 11.9858 | c.headroom#| c.gear_ratio | 1.706674 1.570136 1.09 0.281 -1.433007 4.846355 | rep78 | 2 | 1.420052 3.305466 0.43 0.669 -5.189636 8.02974 3 | .3597184 3.328762 0.11 0.914 -6.296553 7.01599 4 | .7112303 3.462482 0.21 0.838 -6.21243 7.634891 5 | 4.839447 4.021867 1.20 0.234 -3.202773 12.88167 | _cons | 18.27442 15.83866 1.15 0.253 -13.39693 49.94578 ------------------------------------------------------------------------------ Notice that compared to the previous model, the Coef estimates but the standard errors (and corresponding t-statistic, p-value and confidence interval) are slightly different. Typically, the robust standard errors should be slightly larger than the non-robust standard errors, but not always (as in this case). Generally, the only situation where the robust standard errors will decrease is when the error variance is highest for observations near the average value of the predictors. This does not often happen (generally the higher residuals occur in observations that could be considered outliers). There has been some argument that robust standard errors should always be used, because if the model is correctly specified, the robust standard errors and regular standard errors should be almost identical, so there is no harm in using them. 5.2.5 Assumptions There are three main assumptions when running a linear regression. Some we can test, some we cannot (and need to rely on our knowledge of the data). 5.2.5.1 Relationship is linear and additive Recall the linear regression model: \\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p + \\epsilon \\] This very explicitly assumes that the relationship is linear (as opposed to something non-linear, such as quadratic or exponential) and additive (as opposed to multiplicative). We can examine this assumption by looking at plots of the residuals (estimated errors): . rvfplot What we’re seeing here is a scatterplot between the fitted values (the predicted values for each individual) and their errors (the difference between the predicted values and observed values). If you can see a pattern in the scatterplot, that is evidence that this assumption is violated. Importantly, not seeing any pattern is not evidence that the assumption is valid! You’ll still need to cover this assumption with theory and knowledge of the data. This image, from Julian Faraway’s Linear Models with R book, demonstrates a lack of pattern (the first) and a pattern (the third). (We will discuss the second plot below). If this assumption is violated, you will need to reconsider the structure in your model, perhaps by adding a squared term (e.g. reg y c.x c.x#c.x). 5.2.5.1.1 Obtaining predicted values and residuals In the rvfplot, we plotted residuals versus predicted values - neither of which we have in the data. If there is some analysis beyond what rvfplot produces that you’re interested in, the predict command can obtain these. The general syntax for predict is: predict &lt;new var name&gt;, &lt;statistic&gt; There are quite a few options for the “statistic”, but the two most commonly used ones are: xb: The linear prediction (also the default). This is the predicted value for each individual based on the model. residuals: The residuals. The difference between the predicted value and observed value. In other words, we can replicate the above rvfplot via: . predict linearpredictor, xb (5 missing values generated) . predict resids, residuals (5 missing values generated) . twoway scatter resids linearpredictor (The two warnings about missing values are due to 4 cars not having a value of rep78. See multiple imputation for a strategy for dealing with missing data.) 5.2.5.2 Errors are homogeneous “Homogeneity” is a fancy term for “uniform in distribution”, whereas “heterogeneity” represents “not uniform in distribution”. If we were to take a truly random sample of all individuals in Michigan, the distribution of their heights would be homogeneous - it is reasonable to assume there is only a single distribution at work there. If on the other hand, we took a random sample of basketball players and school children, this would definitely be heterogeneous. The basketball players have a markedly difference distribution of heights that school children! In linear regression, the homogeneity assumption is that the distribution of the errors is uniform. Violations would include errors changing as the predictor increased, or several groups having very different noise in their measurements. This is an assumption we can examine, again with the residuals vs fitted plot. We’re looking for either a blatant deviation from a mean of 0, or an increasing/decreasing variability on the y-axis over time. Refer back to the image above, looking at the middle plot. As the fitted values increase, the error spreads out. If this assumption is violated, you may consider restructuring your model as above, or transforming either your response or predictors using log transforms. 5.2.5.3 Independence This last assumption is that each row of your data is independent. If you have repeated measures, this is violated. If you have subjects drawn from groups (i.e. students in classrooms), this is violated. There is no way to test for this, it requires knowing the data set. If this assumption is violated, consider fitting a mixed model instead. 5.2.6 Miscellaneous concerns 5.2.6.1 Multicollinearity Multicollinearity is an issue when 2 or more predictors are correlated. If only two are correlated, looking at their correlation (with pwcorr or correlate) may provide some indication, but you can have many-way multicollinearity where each pairwise correlation is low. You can use the variance inflation factor to try and identify if this is an issue. . estat vif Variable | VIF 1/VIF -------------+---------------------- headroom | 83.74 0.011942 gear_ratio | 26.00 0.038456 c.headroom#| c.gear_ratio | 72.97 0.013705 rep78 | 2 | 4.89 0.204632 3 | 10.03 0.099719 4 | 8.58 0.116527 5 | 5.97 0.167545 -------------+---------------------- Mean VIF | 30.31 The rule of thumb is VIF &gt; 10 or 1/VIF (called the tolerance) &lt; .1 suggests that the variable is involved in multicollinearity and more exploration may be needed. We’ve got a ton of multicollinearity here, so we’d need to explore more and perhaps exclude one of them. Multicollinearity can be an issue because the more correlated predictors are, the more likely that their combined effect will be inappropriately spread among them. For a very simple example, imagine that we have the model \\[ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon \\] If \\(X_1\\) and \\(X_2\\) are uncorrelated, then we can estimate \\(\\beta_1\\) and \\(\\beta_2\\) without a problem. Consider the extreme situations where \\(X_1\\) and \\(X_2\\) are perfectly correlated.11 We can therefore rewrite the equation as \\[ Y = \\beta_0 + (\\beta_1 + \\beta_2)X_1 + \\epsilon \\] since with perfect correlation, \\(X_1\\) and \\(X_2\\) are identical.12 Now, when we fit the model, we would have estimates of \\(\\beta_1\\) and \\(\\beta_2\\) which sum to the “truth”, but the individual level of each of \\(\\beta_1\\) and \\(\\beta_2\\) could be anything. For example, if the “true” \\(\\beta_1\\) and \\(\\beta_2\\) are 1 and 3, they sum to 4. We could get estimated coefficients of 1 and 3, or 3 and 1, or -20 and 24! This is an extreme example, but in practice we can be close to this situation. 5.2.6.2 Overfitting Overfitting occurs when a model includes so many predictors that you can no longer generalize to the population. The rule of thumb is that you should have no more than one predictor for every 10-20 observations. The smaller your sample size, the more conservative you should be. For example, a sample size of 100 should use no more than 10-20 predictors. Recall that a categorical predictor with \\(k\\) different levels adds \\(k-1\\) predictors! 5.2.6.3 Model Selection is bad There is a literature on the idea of model selection, that is, an automated (or sometimes manual) way of testing many versions of a model with a different subset of the predictors in an attempt to find the model that fits best. These are sometimes called “stepwise” procedures. This method has a number of flaws, including Doing this is basically “p-value mining”, that is, running a lot of tests till you find a p-value you like. Your likelihood of making a false positive is very high. As we saw earlier, adding a new variable can have an effect on existing predictors. Instead of doing model selection, you should use your knowledge of the data to select a subset of the variables which are either a) of importance to you, b) theoretically influential on the outcome (e.g. demographic variables) or c) what others (reviewers) would think are influential on the outcome. Then you can fit a single model including all of this. The “subset” can be all predictors if the sample size is sufficient. Note that adjustments to fix assumptions (e.g. transformations) or multicollinearity would not fall into the category of model selection and are fine to use. 5.3 Exercise 4 Reload the NHANES data. webuse nhanes2, clear Fit a linear regression model predicting lead based upon sex, race, age, weight, height, and region. Make sure to handle categorical variables appropriately! Answer the following questions which may or may not require running additional command. How well does the model fit? Does one gender tend to have higher levels of lead? Is the coefficient on age statistically significant? Do you think it is clinically interesting? Looking at all the differences between regions, what conclusion can you draw? Add an interaction between gender and age. What is the interpretation here? Do any assumptions appear violated? Does multicollinearity appear to be a concern? 5.4 Logistic Regression Let’s violate one of the assumptions. Instead of the relationship being linear, we can generalize to allow the relationship to be any functional form. These types of models are called “Generalized Linear Models” or “GLMs”, mainly because we can transform the model to be linear in some sense. Logistic regression is one specific form of a GLM, others in Poisson and Negative Binomial regression. Logistic regression is used when the outcome is dichotomous - either a positive outcome (1) or a negative outcome (0). For example, presence or absence of some disease. The equation for this model is \\[ P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p)}} + \\epsilon. \\] The right hand side is what’s known as the “logit” function, that is, logit(\\(z\\)) = \\(\\frac{1}{1 + e^{-z}}\\). Understanding this form isn’t crucial to understanding the model, but there are two quirks that need to be examined. The relationship between the \\(X\\)’s and the outcome is nonlinear. Note that the left-hand side of this model is not just \\(Y\\), but rather, the probability of \\(Y\\) being 1 (a positive result) given the predictors. Unlike linear regression where we are explicitly predicting the outcome, a logistic model is instead trying to predict everyone’s probability of a positive outcome. 5.4.1 Fitting the logistic model We can fit this using the logit command in State. It works very similarly to regress. Let’s predict whether a car is foreign based on headroom and gear ratio again. . logit foreign gear_ratio headroom Iteration 0: log likelihood = -45.03321 Iteration 1: log likelihood = -24.070574 Iteration 2: log likelihood = -21.955086 Iteration 3: log likelihood = -21.905247 Iteration 4: log likelihood = -21.905069 Iteration 5: log likelihood = -21.905069 Logistic regression Number of obs = 74 LR chi2(2) = 46.26 Prob &gt; chi2 = 0.0000 Log likelihood = -21.905069 Pseudo R2 = 0.5136 ------------------------------------------------------------------------------ foreign | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- gear_ratio | 5.731199 1.32087 4.34 0.000 3.142342 8.320056 headroom | -.2450151 .453866 -0.54 0.589 -1.134576 .6445459 _cons | -18.2719 4.565065 -4.00 0.000 -27.21926 -9.324539 ------------------------------------------------------------------------------ When you try this yourself, you may notice that its not quite as fast as regress. That is because for linear regression we have a “closed form solution” - we just do some quick math and reach an answer. However, almost every other type of regression lacks a closed form solution, so instead we solve it iteratively - Stata guesses at the best coefficients that minimize error13, and keeps guessing (using the knowledge of the previous guesses) until it stops getting significantly different results. From this output, we get the “Number of obs” again. Instead of an ANOVA table with a F-statistic to test model significance, there is instead a “chi2” (\\(\\chi^2\\), pronounced “ky-squared” as in “Kyle”). In this model, we reject the null that all coefficients are identically 0. When we move away from linear regression, we no longer get an \\(R^2\\) measure. There have been various pseudo-\\(R^2\\)’s suggested, and Stata reports one here, but be careful assigning too much meaning to it. It is not uncommon to get pseudo-\\(R^2\\) values that are negative or above 1. We’ll discuss measuring goodness of fit below. The coefficients table is interpreted in almost the same way as with regression. We see that gear_ratio has a significant positive coefficient, and headroom’s coefficient is indistinguishable from 0. We cannot nicely interpret the coefficients. All we can say is that “As gear ratio increases, the probability of a car being foreign increases.” To add any interpretability to these coefficients, we should instead look at the odds ratios (these coefficients are known as the log-odds). We can obtain this with the or options. . logit, or Logistic regression Number of obs = 74 LR chi2(2) = 46.26 Prob &gt; chi2 = 0.0000 Log likelihood = -21.905069 Pseudo R2 = 0.5136 ------------------------------------------------------------------------------ foreign | Odds Ratio Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- gear_ratio | 308.3386 407.2751 4.34 0.000 23.15803 4105.388 headroom | .7826927 .3552376 -0.54 0.589 .3215584 1.905122 _cons | 1.16e-08 5.30e-08 -4.00 0.000 1.51e-12 .0000892 ------------------------------------------------------------------------------ Note: _cons estimates baseline odds. Notice that the “chi2”, “PseudoR2”, “z” and “P&gt;|z|” do not change - we’re fitting the same model! We’re just changing how the coefficients are represented. Odds ratios null hypothesis is at 1, not at 0. Odds ratios are always positive. So a significant odds ratio will be away from 1, rather than away from 0 as in linear regression or the log odds. We can interpret odds ratios as percentages. The odds ratio for gear_ratio is 308.3386, suggesting that a 1 increase in the odds ratio leads to a 30,833% increase in the odds that the car is foreign! This is massive! But keep in mind, gear_ratio had a very narrow range - an increase of 1 is very large. Let’s rescale gear_ratio and try again. . gen gear_ratio2 = gear_ratio*10 . logit foreign gear_ratio2 headroom, or Iteration 0: log likelihood = -45.03321 Iteration 1: log likelihood = -24.070576 Iteration 2: log likelihood = -21.955089 Iteration 3: log likelihood = -21.905249 Iteration 4: log likelihood = -21.905071 Iteration 5: log likelihood = -21.905071 Logistic regression Number of obs = 74 LR chi2(2) = 46.26 Prob &gt; chi2 = 0.0000 Log likelihood = -21.905071 Pseudo R2 = 0.5136 ------------------------------------------------------------------------------ foreign | Odds Ratio Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- gear_ratio2 | 1.773792 .2342948 4.34 0.000 1.36921 2.297922 headroom | .7826928 .3552376 -0.54 0.589 .3215585 1.905122 _cons | 1.16e-08 5.30e-08 -4.00 0.000 1.51e-12 .0000892 ------------------------------------------------------------------------------ Note: _cons estimates baseline odds. Note once again that the model fit characteristics haven’t changed; we’ve fit the same model, just with different units. Now the interpretation is more reasonable, for every .1 increase in gear_ratio (which corresponds to a 1 increase in gear_ratio2), we predict an average mileage increase of 77%. The odds ratio on headroom is not distinguishable from 1, however, if it were, the interpretation is that increasing the headroom by 1 inch is predicted to decrease the mileage by about 21.7% on average (1 - 0.783). 5.4.2 Categorical Variables and Interactions Both categorical variables and interactions can be included as they were in linear regression, with the appropriate interpretation of coefficients/odds ratios. The margins command also works the same. The predict command adds a new statistic. xb now is the linear predictor, which is often not useful. Instead, the pr statistic returns the estimated probability of a positive outcome. 5.4.3 Logistic regression assumptions The assumptions for logistic regression are simpler than linear. The outcome measure must be binary with a 0 for a negative response and 1 for a positive. (Technically they don’t have to be positive/negative. We could think of predicting male/female and code male = 0 and female = 1. However, the convention would be to consider the outcome as “The person is female” so a 1 represents a “success” and a 0 a “failure” of that statement.) The errors in a logistic regression model are fairly contained (you can’t be wrong than more than 1!) so there are no real assumptions about them. The independence assumption is still here and still important, again, a mixed logistic model may be appropriate if the data is not independent. 5.4.4 Logistic goodness of fit As we mentioned earlier, there are various issues with the Pseudo \\(R^2\\) reported by logit, so use it carefully. There are two alternate approaches. The first is to look at a classification table: . estat classification Logistic model for foreign -------- True -------- Classified | D ~D | Total -----------+--------------------------+----------- + | 16 4 | 20 - | 6 48 | 54 -----------+--------------------------+----------- Total | 22 52 | 74 Classified + if predicted Pr(D) &gt;= .5 True D defined as foreign != 0 -------------------------------------------------- Sensitivity Pr( +| D) 72.73% Specificity Pr( -|~D) 92.31% Positive predictive value Pr( D| +) 80.00% Negative predictive value Pr(~D| -) 88.89% -------------------------------------------------- False + rate for true ~D Pr( +|~D) 7.69% False - rate for true D Pr( -| D) 27.27% False + rate for classified + Pr(~D| +) 20.00% False - rate for classified - Pr( D| -) 11.11% -------------------------------------------------- Correctly classified 86.49% -------------------------------------------------- Classification is based upon the predicted probability, a predicted probability above .5 is classified as “1”, below as “0”. The output here is rather long, but the general idea is to capture how well we’re predicting without having too many false results. (If your outcome variable was 80% 1’s and 20% 0’s, if I predicted all 1’s, I’d be right 80% of the time! So it’s important to also see that I’m wrong in 100% of the true 0’s). Sensitivity is how likely you are to correctly predicted a positive outcome. Specificity is how likely you are to correctly predicted a negative outcome. The positive/negative predictive values are how likely a positive/negative prediction is to be correct. We want those results to be high. The various false rates are for misclassification, and we want those low. In this model, we’ve done pretty good! The second is a more formal test. There are two variants, a Pearson \\(\\chi^2\\) test and the Hosmer-Lemeshow test. Both are fit with the estat gof command. Both are testing the hypothesis that the observed positive responses match predicted positive responses in subgroups of the population. Therefore we do not want to reject these tests, and a large p-value is desired. . estat gof Logistic model for foreign, goodness-of-fit test ------------------------------------------------ number of observations = 74 number of covariate patterns = 58 Pearson chi2(55) = 42.92 Prob &gt; chi2 = 0.8817 We see here a p-value of 0.882, failing to reject the null, so there is no evidence that the model fits well. There is some concern that when the “number of covariate patterns” is close to the number of observations , the Pearson test is invalid. In this data, we see that 58 is indeed “close” to 74. Instead, we can use the Hosmer-Lemeshow by passing the group(#) option: . estat gof, group(10) Logistic model for foreign, goodness-of-fit test ------------------------------------------------ (Table collapsed on quantiles of estimated probabilities) number of observations = 74 number of groups = 10 Hosmer-Lemeshow chi2(8) = 9.89 Prob &gt; chi2 = 0.2727 The p-value remains insignificant at 0.273, still no evidence of poor model fit. Why did we choose 10 groups? It’s just the standard. The only thing to keep in mind is that the Hosmer-Lemeshow test is only appropriate if the number of groups is greater than the number of predictors (including the intercept). In this model, we had two predictors, so that’s 3 total (including the intercept), so 10 is OK. There are some limitations to Hosmer-Lemeshow, and there are more modern alternatives. However, Stata has not implemented any yet that I’m aware of. 5.4.5 Separation Since the logistic regression model is solved iteratively, this can fail for a number of reasons. Before you begin interpreting the model, you’ll want to glance at the iteration steps and make sure that no errors were printed. The most common failure is due to separation. With a binary outcome instead of a continuous outcome, it is much easier to have a predictor (or set of predictors) that perfectly predict the outcome. Consider trying to predict gender based on height. With a smaller sample, it’s not hard to imagine a scenario where every male is taller than every female. This is called “perfect separation”; using this sample, knowing height gives perfect information about gender “Partial separation” can also occur; this is when prediction is perfect only for one limit. Take the height scenario again; say everyone above 5’8&quot; is male, and there are two men but the rest women below 5’8“. Here, we will always predict Male for heights above 5’8”. Separation (of either type) often produces coefficients to be extreme with large standard errors. Stata will sometimes warn about this, but not always. If you notice these exceptional coefficients or if Stata does warn about separation, you’ll need to investigate and consider excluding certain predictors. It may seem counter-intuitive to exclude extremely highly predictive variables, but if a variable produces perfect separation, you don’t need a model to inform you of that. You can examine separation by looking at a table. Imagine we wanted to restructure rep78 as a binary variable, with low repairs (repair record below 3) and high repairs (repair records 3 and above). . gen repair_binary = rep78 &gt;= 3 . replace repair_binary = . if rep78 &gt;= . (5 real changes made, 5 to missing) . label define repbinlabel 0 &quot;Low repairs&quot; 1 &quot;High repairs&quot; . label value repair_binary repbinlabel . tab repair_binary repair_binar | y | Freq. Percent Cum. -------------+----------------------------------- Low repairs | 10 14.49 14.49 High repairs | 59 85.51 100.00 -------------+----------------------------------- Total | 69 100.00 Let’s try fitting the model: . logit foreign repair_binary note: repair_binary != 1 predicts failure perfectly repair_binary dropped and 10 obs not used Iteration 0: log likelihood = -38.411464 Iteration 1: log likelihood = -38.411464 Logistic regression Number of obs = 59 LR chi2(0) = 0.00 Prob &gt; chi2 = . Log likelihood = -38.411464 Pseudo R2 = 0.0000 ------------------------------------------------------------------------------ foreign | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- repair_bin~y | 0 (omitted) _cons | -.5930637 .2719096 -2.18 0.029 -1.125997 -.0601307 ------------------------------------------------------------------------------ Notice the note at the top of the model, and notice that nothing is estimated for repair_binary. We have partial separation: . tab foreign repair_binary | repair_binary Car type | Low repai High repa | Total -----------+----------------------+---------- Domestic | 10 38 | 48 Foreign | 0 21 | 21 -----------+----------------------+---------- Total | 10 59 | 69 Since we have a zero in one of the cells, that’s partial separation. Complete separation would be zero in both off-diagonal cells. 5.4.6 logit Miscellaneous. The logit model supports the margins command just like regress does. It does not support estat vif because variance inflation factors are not defined for logistic models. Collinearity, overfitting, and model selection remain concerns in the logistic model. Robust standard errors via vce(robust) are supported. 5.4.7 logit vs logistic Instead of logit, we could run the logistic command. The only difference is that logistic reports the odds ratio by default whereas logit reports the log odds. My personal preference is logit, but there’s no need to use one over the other. 5.5 Other regression models There are several other models which we will not cover, but function similarly to the above. Poisson regression is useful when you have count data; i.e. number of visitors or number of thunderstorms in a month. It can be fit with the poisson command, and results are interpreted similar to logistic regression (coefficients vs odds ratios); but instead of predicting a positive outcome, its predicting a larger count. If you have very large counts (such that a histogram appears to show a bell curve), linear regression can be used instead. Poisson has the strong assumption that the mean of the outcome is equal to its variance (small average counts have little noise; large average counts have a lot of noise). If this assumption is violated (or you want to check it), negative binomial regression also handles count data, without that assumptions, using nbreg. The output will include a test of “alpha=0”, if this fails to reject, then Poisson regression is sufficient. There are two extensions to logistic regression, ordinal logistic and multinomial. Ordinal logistic is used when there are more than 2 outcome categories, and they are ordered (e.g. not sick (0), mildly sick (1), very sick (2)). Using ologit, Stata estimates an underlying continuous distribution and returns the “cut points”, allowing categorization. If there are multiple groups but not ordered, e.g. race, use mlogit for multinomial logistic regression. It essentially fits a model predicting membership in each group versus all other, with some restrictions across the models. 5.6 Exercise 5 Reload the NHANES data. webuse nhanes2, clear This time we’ll fit a logistic regression model, prediciting diabetes status on sex, race, age, weight, height, and region. As before, be sure to handle categorical variables appropriately. Does the model fit well? Does the model classify well? Ignoring any issues with model fit, what predicts higher odds of having diabetes? There are variations of regression with multiple outcomes, but they are for very specialized circumstances and can generally be fit as several basic regression models instead.↩ The 2 and 71 are degrees of freedom. They don’t typically add any interpretation.↩ The only exception is if the predictor being added is either constant or identical to another variable.↩ This is why it’s important to familiarize yourself with the units in your data!↩ Note that if you provide data with perfect correlation, Stata will drop one of them for you. This in only a thought exercise. If it helps, imagine their correlation is 99% instead of perfect, and add “almost” as a qualifier to most claims.↩ Technically there could be a scaling factors such that \\(X_1 = aX_2 + b\\), but let’s assume without loss of generality that \\(a=1\\) and \\(b=0\\).↩ Technically that maximizes likelihood, but that distinction is not important for understanding.↩ "],
["mixed-models.html", "Chapter 6 Mixed models 6.1 Terminology 6.2 Wide vs Long data, Time-varying vs Time-invariant 6.3 Linear Mixed Model 6.4 Assumptions 6.5 Miscellaneous 6.6 Convergence issues 6.7 Logistic Mixed Model 6.8 Exercise 6", " Chapter 6 Mixed models Let’s violate another regression assumption, independence. While this is usually thought of in the repeated measurements setting, it is not exclusive to that. For example, Repeated measures: You’re conducting a trial on individuals who undergo an intervention. You generate a survey, and have the participants fill out a copy when the intervention begins, 30 days into the intervention, and 1 year after the intervention. If we have a single outcome measure of interest from this survey, we have three measurements for person. These values are not independent; it’s reasonable to think that your measurements are more related to each other than any of mine. Non-repeated measures: You conduct door-to-door sampling of individuals in households, asking about food habits. You collect information on each individual in the house, and want to use individuals as the unit of analysis. It’s likely that two individuals in the same house will have similar food patterns, as opposed to two individuals from different houses. To address the lack of dependence, we will move from normal regression (linear or otherwise) into a mixed models framework, which accounts for this dependence structure. It does this (at the most basic level) by allowing each [individual from the intervention example, household from the door-to-door example] to have its own intercept which we do not estimate. For this chapter, we’ll turn away from the “auto” data set and instead use a sample of the “National Longitudinal Survey” contained in Stata: . webuse nlswork, clear (National Longitudinal Survey. Young Women 14-26 years of age in 1968) This data is a survey taken from 1968-1988, and this specific sample of the data is salary information for women. We have repeated measures in the sense that we have yearly data for women, so each woman can have up to 20 data points. 6.1 Terminology There are several different names for mixed models which you might encounter, that all fit essentially the same model: Mixed model Mixed Effects regression Multilevel regression Hierarchical regression (specifically HLM, hierarchical linear model) The hierarchical/multilevel variations require thinking about the levels of the data and involves “nesting”, where one variable only occurs within another, e.g. family members nested in a household. The most canonical example of this is students in classrooms, we could have Level 1: The lowest level, the students. Level 2: Classroom or teacher (this could also be two separate levels of classrooms inside teacher) Level 3: District Level 4: State Level 5: Country This is taking it a bit far; it’s rare to see more than 3 levels, but in theory, any number can exist. For this workshop, we will only briefly discuss this from hierarchical point of view, preferring the mixed models view (with the reminder again that they are the same!). 6.2 Wide vs Long data, Time-varying vs Time-invariant Before you begin your analysis, you need to ensure that the data is in the proper format. Let’s consider the NLS data, where we have measures of women’s salary over 20 years. Wide format of the data would have row represent a woman, and she would have 20 columns worth of salary information (plus additional demographics). Long format of the data would have each row represent a woman and a year, so that each woman can have up to 20 rows (if a woman wasn’t measured in a given year, that row &amp; year is blank). To fit a mixed model, we need the data in long format. We can use the reshape command to transform wide data to long. This is covered in the Stata I set of notes. Additionally, there is the concept of time-varying vs time-invariant variables. Time-varying variables are those which can be different for each entry within the same individual. Examples include weight or salary. Time-invariant are those which are the same across all entries. Examples include race or baseline characteristics. When data is long, time-invariant variables need to be constant per person. 6.3 Linear Mixed Model The most basic mixed model is the linear mixed model, which extends the linear regression model. A model is called “mixed” because it contains a mixture of fixed effects and random effects. Fixed effects: These are the predictors that are present in regular linear regression. We will obtain coefficients for these predictors and be able to test and interpret them. Technically, an OLS linear model is a mixed model with only fixed effects.14 Random effects: These are the “grouping” variables, and must be categorical (Stata will force every variable to be prefaced by i.). These are essentially just predictors as well, however, we do not obtain coefficients to test or interpret. We do get a measure of the variability across groups, and a test of whether the random effect is benefiting the model. Let’s fit a model using the mixed command. It works similar to regress with a slight tweak. We’ll try and predict log of wages15 using work experience and race. This data . mixed ln_w ttl_exp i.race age || idcode: Performing EM optimization: Performing gradient-based optimization: Iteration 0: log likelihood = -10471.727 Iteration 1: log likelihood = -10471.727 Computing standard errors: Mixed-effects ML regression Number of obs = 28,510 Group variable: idcode Number of groups = 4,710 Obs per group: min = 1 avg = 6.1 max = 15 Wald chi2(4) = 5246.25 Log likelihood = -10471.727 Prob &gt; chi2 = 0.0000 ------------------------------------------------------------------------------ ln_wage | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- ttl_exp | .042996 .0010126 42.46 0.000 .0410113 .0449807 | race | black | -.1178248 .0115814 -10.17 0.000 -.1405238 -.0951257 other | .0995432 .0484246 2.06 0.040 .0046328 .1944536 | age | -.0069699 .0006836 -10.20 0.000 -.0083097 -.0056302 _cons | 1.644184 .0161148 102.03 0.000 1.612599 1.675768 ------------------------------------------------------------------------------ ------------------------------------------------------------------------------ Random-effects Parameters | Estimate Std. Err. [95% Conf. Interval] -----------------------------+------------------------------------------------ idcode: Identity | var(_cons) | .1037959 .0026579 .0987151 .1091381 -----------------------------+------------------------------------------------ var(Residual) | .089042 .0008183 .0874526 .0906603 ------------------------------------------------------------------------------ LR test vs. linear model: chibar2(01) = 11678.16 Prob &gt;= chibar2 = 0.0000 The fixed part of the equation, ln_w ttl_exp i.race age is the same as with linear regression, ln_w is the outcome and the rest are predictors, with race being categorical. The new part is || idcode:. The || separates the fixed on the left from the random effects on the right. idcode identifies individuals. The : is to enable the more complicated feature of random slopes which we won’t cover here; for our purposes the : is just required. Let’s walk through the output. Note that what we are calling the random effects (e.g. individuals in a repeated measures situation, classrooms in a students nested in classroom situation), Stata refers to as “groups” in much of the output. At the very top, you’ll see that the solution is arrived at iteratively, similar to logistic regression (you probably also noticed how slow it is)! The log likelihood is how the iteration works; essentially the model “guesses” choices for the coefficients, and finds the set of coefficients that minimize the log likelihood. Of course, the “guess” is much smarter than random. The actual value of the log likelihood is meaningless. Since we are dealing with repeated measures of some sort, instead of a single sample size, we record the total number of obs, the number of groups (unique entries in the random effects) and min/mean/max of the groups. As before, just ensure these numbers seem right. As with logistic regression, the \\(\\chi^2\\) test tests the hypothesis that all coefficients are simultaneously 0. We gave a significant p-value, so we continue with the interpretation. The coefficients table is interpreted just as in linear regression, with the addendum that each coefficient is also controlling for the structure introduced by the random effects. Increased values of ttl_exp is associated with higher log incomes. The race baseline is “white”; compared to white, blacks have lower average income and others have higher average income. Higher age is associated with lower income. The second table (“Random-effects parameters”) gives us information about the error structure. The “idcode:” section is examining whether there is variation across individuals above and beyond the differences in characteristics such as age and race. Since the estimate of var(_cons) (the estimated variance of the constant per person - the individual level random effect) is non-zero (and not close to zero), that is evidence that the random effect is beneficial. If the estimate was 0 or close to 0, that would be evidence that the random effect is unnecessary and that any difference between individuals is already accounted for by the covariates. The estimated variance of the residuals is any additional variation between observations. This is akin to the residuals from linear regression. The \\(\\chi^2\\) test at the bottom is a formal test of the inclusion of the random effects versus a linear regression model without the random effects. We reject the null that the models are equivalent, so it is appropriate to include the random effects. 6.4 Assumptions The linear additivity remains necessary. rvfplot will not work following a mixed command, but you can generate the residuals vs fitted plot manually. The homogeneity of residuals assumption is violated by design in a mixed model. However, some forms of heterogeneity, such as increasing variance as fitted values increase, are not supported. Therefore we can still use the residuals vs fitted plot to examine this. Again, the independence assumption is violated by design, but observations between groups (e.g. between individuals) should be independent. 6.5 Miscellaneous As we’ve discussed before, collinearity, overfitting, and model selection remain concerns. Sample size considerations are tricky with mixed models. Typically these are done with simulations. At a rough pass, the rules of thumb from linear regression remain; 10-20 observations per predictor. Adding a new person will improve the power more than adding another observation for an existing group. The margins and predict command work similarly to regress, however note that both (by default) ignore the random effects; that is, the results the produce are averaged across all individuals. As with linear regression and logistic regression, mixed supports vce(robust) to enable robust standard errors. 6.6 Convergence issues As with logistic regression, the solution is arrived at iteratively, which means it can fail to converge for a number of reasons. Separation isn’t an issue here (though it will be in logistic mixed models), but there can be other causes of a failure to converge. Generally, failure to converge will be due to an issue with the data. Things to look for include: Different scales of predictors. For example, salary (in dollars) and number of children. The scales are drastically different which can cause issues. Try re-scaling any variables on extreme scales (you can do this with egen scaledvar = std(origvar)). This will affect interpretation (the estimated coefficient will be the average predicted change with a on standard deviation increase in the predictor) but not the overall model fit. High correlation can cause this. Check correlations (cor) between your predictors (including any categorical variables) and if you find a highly correlated pair, try removing one. If the iteration keeps running (as opposed to ending and complaining about lack of convergence), try passing the option emiterate(#) with a few “large” (“large” is relative to sample size) to tell the algorithm to stop after # iterations, regardless of convergence. You’re looking for two things: First, if there are any estimated standard errors that are extremely close to zero, that predictor may be causing the issue. Try removing it. Second, if you try a few different max iterations (say 50, 100 and 200), and the estimated coefficients and standard errors are relatively constant, you could consider that model as “good enough”. You wouldn’t have much confidence in the point estimates of the coefficients, but you could at least gain insight into the direction and approximate magnitude of the effect. You can try use the “reml” optimizer, by passing the reml option. This optimizer can be a bit easier to converge. 6.7 Logistic Mixed Model Similar to logistic regression being an extension to linear regression, logistic mixed models are an extension to linear mixed models when the outcome variable is binary. The command for logistic mixed models is melogit. The rest of the command works very similarly to mixed, and interpretation is the best of logistic regression (for fixed effects) and linear mixed models (for random effects). Unfortunately, neither estat classification nor estat gof is supported, so goodness of fit must be measured solely on the \\(\\chi^2\\) test and perhaps a manual model fit comparison. By default the log-odds are reported, give the or option to report the odds ratios. 6.7.1 meqrlogit There is a different solver that can be used based upon QR-decomposition. This is run with the command meqrlogit. It functions identically to melogit. If melogit has convergence issues, try using meqrlogit instead. 6.8 Exercise 6 Load up the “chicken” data set from Stata’s website: webuse chicken, clear The data contains order information from a number of restaurants and records whether the order resulted in a complaint. We’d like to see what attributes (if any) of the servers may increase the odds of a complaint. Since we have multiple orders per restaurant, it’s reasonable to assume that certain restaurants just recieve more complaints than others, regardless of the server, so we’ll need to include random effects for those. Fit a mixed effects logistic regression model predicting complain, based upon server characteristics (grade, race, gender, tenure, age, income) and a few restaurant characteristics (genderm for gender of manager and nworkers for number of workers). Include a random effect for restaurant. Does the model fit better than chance? Interpret the model. What predicts a higher odds of recieving a complaint? Does it appear that adding the random effect was needed? Though why called it mixed at that point?↩ Typically, salary information is very right-skewed, and a log transformation produces normality.↩ "],
["survey-data.html", "Chapter 7 Survey Data 7.1 Definitions 7.2 Describing the survey 7.3 Subset analyses for complex sample survey data", " Chapter 7 Survey Data One major strength of Stata is the ease with which it can analyze data sets arising from complex sample surveys. When working with data collected from a sample with a complex design (anything above and beyond a simple random sample of a population, where the sample design involves clustering and stratification of sampled elements, and multiple stages of sampling), standard statistical analysis procedures that assume a simple random sample (such as everything we’ve discussed so far) will result in very biased estimates of statistics that do no take the design of the sample into account. Two major problems arise when survey data is analyzed without taking the design into account: Representation Variance Estimation Incorporation of the weights corrects for biased estimates (representation) and the stratification and clustering produces correct variance estimates. Stata is one of the leaders in terms of statistical software that can perform these types of analyses, and offers a wide variety of commands that will perform design-based analyses of data arising from a sample with a complex design. The basic process consists of two steps (similar to mi), first using svyset to describe the complex survey design, secondly using the svy: prefix to perform analyses. 7.1 Definitions Complex survey design is a massive topic which there are entire departments devoted to (Program at Survey Methodology here at Michigan) and which we offer a separate full day workshop (Survey Design). A simple survey design takes a random sample from the population as a whole. There are various reasons why a simple random sample will not work. It is often infeasible to do either because of time or cost. With smaller sample sizes, it can be difficult to obtain enough individuals in a given subpopulation. For some small subpopulations, it may be very difficult to even obtain any individuals in a simple random sample. A complex survey design allows researchers to consider these limitations and design a sampling pattern to overcome them. Three primary techniques are Stratification. Rather than sample all individuals, instead target specific subpopulations and collect from them explicitly. For example, you may stratify by race and aim to collect 50 white, 50 black, 50 Hispanic, etc. Clustering. Primarily a cost/time saving measure. Similar to stratification, but instead of sampling from all clusters, you take a random sample of clusters and then sample within them. A typical clustering variable is neighborhood or census tract or school. Weighting. If certain sets of characteristics are more or less common, or more or less desired, when randomly sampling individuals, we can down-weight those who we don’t want/are more common, and up-weight those we want/are less common. For example, we might want to collect data on obesity in school children in Ann Arbor. Rather than randomly sampling across all schools, we cluster by schools and randomly select 3. Then at each of those schools, we stratify by race and take a random sample of all students of each race at each school, weighted by their weight to attempt to capture more overweight students. One final term is primary sampling unit which is the first level at which we randomized. In this example, that would be schools. 7.2 Describing the survey The general syntax is svyset &lt;psu&gt; [pweight = &lt;weight&gt;], strata(&lt;strata&gt;) The svyset command defines the variables identifying the complex design of the sample to Stata, and only needs to be submitted once in a given Stata session. The &lt;psu&gt; is a variable identifying the primary sampling unit (PSU) that an observation came from. The is a variable containing sampling weights. Finally, the strata is a variable identifying the sampling stratum that an observation came from. The NHANES data we’ve been using in our examples is actually from a complex sample design, which we’ve been ignoring. Let’s incorporate the sampling into the analysis. . webuse nhanes2, clear The three variables of interest in the data are finalwgt for the sampling weights, strata for the strata, and psu for the clusters. . describe finalwgt strata psu storage display value variable name type format label variable label ------------------------------------------------------------------------------- finalwgt long %9.0g sampling weight (except lead) strata byte %9.0g stratum identifier, 1-32 psu byte %9.0g primary sampling unit, 1 or 2 It’s useful to know that to remove any existing survey design, you can run . svyset, clear Let’s set up the survey design now. . svyset psu [pweight = finalwgt], strata(strata) pweight: finalwgt VCE: linearized Single unit: missing Strata 1: strata SU 1: psu FPC 1: &lt;zero&gt; To get information about the strata and cluster variables use the following command or menu: . svydescribe Survey: Describing stage 1 sampling units pweight: finalwgt VCE: linearized Single unit: missing Strata 1: strata SU 1: psu FPC 1: &lt;zero&gt; #Obs per Unit ---------------------------- Stratum #Units #Obs min mean max -------- -------- -------- -------- -------- -------- 1 2 380 165 190.0 215 2 2 185 67 92.5 118 3 2 348 149 174.0 199 4 2 460 229 230.0 231 5 2 252 105 126.0 147 6 2 298 131 149.0 167 7 2 476 206 238.0 270 8 2 338 158 169.0 180 9 2 244 100 122.0 144 10 2 262 119 131.0 143 11 2 275 120 137.5 155 12 2 314 144 157.0 170 13 2 342 154 171.0 188 14 2 405 200 202.5 205 15 2 380 189 190.0 191 16 2 336 159 168.0 177 17 2 393 180 196.5 213 18 2 359 144 179.5 215 20 2 285 125 142.5 160 21 2 214 102 107.0 112 22 2 301 128 150.5 173 23 2 341 159 170.5 182 24 2 438 205 219.0 233 25 2 256 116 128.0 140 26 2 261 129 130.5 132 27 2 283 139 141.5 144 28 2 299 136 149.5 163 29 2 503 215 251.5 288 30 2 365 166 182.5 199 31 2 308 143 154.0 165 32 2 450 211 225.0 239 -------- -------- -------- -------- -------- -------- 31 62 10,351 67 167.0 288 Once the survey is defined with svyset, most common commands can be prefaced by svy: to analyze the data with the sampling structure. The svy: tab command works exactly like the tabulate command, only taking the design of the sample into account when producing estimates and chi-square statistics. . svy: tab sex (running tabulate on estimation sample) Number of strata = 31 Number of obs = 10,351 Number of PSUs = 62 Population size = 117,157,513 Design df = 31 ---------------------- 1=male, | 2=female | proportion ----------+----------- Male | .4794 Female | .5206 | Total | 1 ---------------------- Key: proportion = cell proportion Next, lets look at the mean weight by gender. . svy: mean weight, over(sex) (running mean on estimation sample) Survey: Mean estimation Number of strata = 31 Number of obs = 10,351 Number of PSUs = 62 Population size = 117,157,513 Design df = 31 Male: sex = Male Female: sex = Female -------------------------------------------------------------- | Linearized Over | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ weight | Male | 78.62789 .2097761 78.20004 79.05573 Female | 65.70701 .266384 65.16372 66.25031 -------------------------------------------------------------- Compare this to the usual mean command, without the design information: . mean weight, over(sex) Mean estimation Number of obs = 10,351 Male: sex = Male Female: sex = Female -------------------------------------------------------------- Over | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ weight | Male | 77.98423 .1945289 77.60292 78.36555 Female | 66.39418 .1998523 66.00243 66.78593 -------------------------------------------------------------- And compare the svy: results to the usual mean command, with only the weights considered: . mean weight [pweight=finalwgt], over(sex) Mean estimation Number of obs = 10,351 Male: sex = Male Female: sex = Female -------------------------------------------------------------- Over | Mean Std. Err. [95% Conf. Interval] -------------+------------------------------------------------ weight | Male | 78.62789 .2272099 78.18251 79.07326 Female | 65.70701 .2265547 65.26292 66.1511 -------------------------------------------------------------- We see that the weights affect on the standard error, whereas the stratification and clustering also affects the estimates. As with mi:, many of the usual commands such as regress or logit can be prefaced by svy:. If a command errors with the svy: prefix, a lot of the time the survey design will not affect it, and the documentation for the command will inform of that. 7.3 Subset analyses for complex sample survey data In general, analysis of a particular subset of observations from a sample with a complex design should be handled very carefully. It is usually not appropriate to delete cases from the data-set that fall outside the sub-population of interest, or to use an if statement to filter them out. In Stata, sub-population analyses for this type of data are analyzed using a subpop indicator. Suppose we want to perform an analysis only for the cases where race is black in the NHANES data set. First, we must create an indicator variable that equals 1 for these cases. . gen race_black = race == 2 . replace race_black = . if race == . (0 real changes made) Now we can run a simple regression model only on . svy, subpop(race_black): regress weight height i.sex (running regress on estimation sample) Survey: Linear regression Number of strata = 30 Number of obs = 10,013 Number of PSUs = 60 Population size = 113,415,086 Subpop. no. obs = 1,086 Subpop. size = 11,189,236 Design df = 30 F( 2, 29) = 50.12 Prob &gt; F = 0.0000 R-squared = 0.1131 ------------------------------------------------------------------------------ | Linearized weight | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- height | .708568 .0728382 9.73 0.000 .5598126 .8573234 | sex | Female | 3.508388 1.348297 2.60 0.014 .7547976 6.261979 _cons | -46.10337 12.56441 -3.67 0.001 -71.76331 -20.44343 ------------------------------------------------------------------------------ Note: 1 stratum omitted because it contains no subpopulation members. Compare the svy, subpop( ): results to the usual svy: regress command using an if statement: . svy: reg weight height i.sex if race_black == 1 (running regress on estimation sample) Survey: Linear regression Number of strata = 30 Number of obs = 1,086 Number of PSUs = 55 Population size = 11,189,236 Design df = 25 F( 0, 25) = . Prob &gt; F = . R-squared = 0.1131 ------------------------------------------------------------------------------ | Linearized weight | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- height | .708568 . . . . . | sex | Female | 3.508388 . . . . . _cons | -46.10337 . . . . . ------------------------------------------------------------------------------ Note: Missing standard errors because of stratum with single sampling unit. Stata refuses to even calculate standard errors. "],
["multiple-imputation.html", "Chapter 8 Multiple Imputation 8.1 Missing at random 8.2 mi 8.3 Setting up data 8.4 Performing the imputation 8.5 Analyzing mi data 8.6 Manual MI 8.7 Removing the MI data 8.8 Survey and multiple imputation 8.9 Citations", " Chapter 8 Multiple Imputation Multiple imputation is a common approach to addressing missing data issues. When there is missing data, the default results are often obtained with complete case analysis (using only observations with complete data) can produce biased results though not always. Additionally, complete case analysis can have a severe negative effect on the power by greatly reducing the sample size. Imputation in general is the idea of filling in missing values to simulate having complete data. Some simpler forms of imputation include: Mean imputation. Replace each missing value with the mean of the variable for all non-missing observations. Cold deck imputation. Replace each missing value with the value from another observation which is similar to the one with the missing value. Regression imputation. Fit a regression model and replace each missing value with its predicted value. There are various pros and cons to each approach, but in general, none are as powerful or as commonly used as multiple imputation. Multiple imputation (or MI) is a three step procedure: For each missing value, obtain a distribution for it. Sample from these distributions to obtain imputed values that have some randomness built in. Do this repeatedly to create \\(M\\) total imputed data sets. Each of these \\(M\\) data sets is identical on non-missing values but will (almost certainly) differ on the imputed values. Perform your statistical analysis on each of the \\(M\\) imputed data sets separately. Pool your results together in a specific fashion to account for the uncertainty in imputations. Thankfully, for simple analyses (e.g. most regression models), Stata will perform all three steps for you automatically. We will briefly discuss later how to perform MI if Stata doesn’t support it. 8.1 Missing at random There can be many causes of missing data. We can classify the reason data is missing into one of three categories: Missing completely at random (MCAR): This is missingness that is truly random - there is no cause of the missingness, it’s just due to chance. For example, you’re entering paper surveys into a spreadsheet and spill coffee on them, obscuring a few answers. Missing at random (MAR): The missingness here is due to observed data but not unobserved data. For example, women may be less likely to report their age, regardless of what their actual age is. Missing not at random (MNAR): Here the missingness is due to the missing value. For example, individuals with higher salary may be less willing to answer survey questions about their salary. There is no statistical test16 to distinguish between these categories; instead you must use your knowledge of the data and its collection to argue which category it falls under. This is important because most imputation methods (including MI) require MCAR or MAR for the data. If the data is MNAR, there is very little you can do. Generally if you believe the data is MNAR, you can assume MAR but discuss that a severe limitation of your analysis is the MAR assumption is likely invalid. 8.2 mi The mi set of commands in Stata perform the steps of multiple imputation. There are three steps, with a preliminary step to examine the missingness. We’ll be using the “mheart5” data from Stata’s website which has some missing data. . webuse mheart5, clear (Fictional heart attack data) . describe, short Contains data from http://www.stata-press.com/data/r15/mheart5.dta obs: 154 Fictional heart attack data vars: 6 19 Jun 2016 10:50 size: 1,848 Sorted by: . summarize Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- attack | 154 .4480519 .4989166 0 1 smokes | 154 .4155844 .4944304 0 1 age | 142 56.43324 11.59131 20.73613 83.78423 bmi | 126 25.23523 4.029325 17.22643 38.24214 female | 154 .2467532 .4325285 0 1 -------------+--------------------------------------------------------- hsgrad | 154 .7532468 .4325285 0 1 We see from the summary that both age and bmi have some missing data. 8.3 Setting up data We need to tell Stata how we’re going to be doing the imputations. First, use the mi set command to determine how the multiple data sets will be stored. Really which option you choose is up to you, I prefer to “flong” option, where each imputed data set is stacked on top of each other. If you have very large data, you might prefer “wide”, “mlong” or “mlongsep”, the last of which stores each imputed data set in a separate file. See help mi styles for more details. (Ultimately the decision is not that important, as you can switch later using mi convert &lt;new style&gt;.) . mi set flong Next, we need to tell Stata what each variable will be used for. The options are imputed: A variable with missing data that needs to be imputed. regular: Any variable that is complete or does not need imputation. Technically we only need specify the imputed variables, as anything unspecified is assumed to be regular. We saw above that age and bmi have missing values: . mi register imputed age bmi (28 m=0 obs. now marked as incomplete) We can examine our setup with mi describe: . mi describe Style: flong last mi update 03oct2017 16:36:54, 0 seconds ago Obs.: complete 126 incomplete 28 (M = 0 imputations) --------------------- total 154 Vars.: imputed: 2; age(12) bmi(28) passive: 0 regular: 0 system: 3; _mi_m _mi_id _mi_miss (there are 4 unregistered variables; attack smokes female hsgrad) We see 126 complete observations with 28 incomplete, the two variables to be imputed, and the 4 unregistered variables which will automatically be registered as regular. 8.3.1 Imputing transformations What happens if you had a transform of a variable? Say you had a variable for salary, and wanted to use a log transformation? You can find literature suggesting either transforming first and then imputing, or imputing first and then transforming. Our suggestion, following current statistical literature is to transform first, impute second.(Hippel 2009) Stata technically supports the other option via mi register passive, but we don’t recommend it’s usage. Instead, transform your original data, then flag both the variable and its transformations as “imputed” 8.4 Performing the imputation Now that we’ve got the MI set up, we can perform the actual procedure. There are a very wide number of variations on how this imputation can be done (including defining your own!). You can see these as the options to mi impute. We’ll just be focusing on the “chained” approach, which is a good approach to start with. The syntax for this is a bit complicated, but straightforward once you understand it. mi impute chained (&lt;method 1&gt;) &lt;variables to impute with method 1&gt; /// (&lt;method 2&gt;) &lt;variables to impute with method 2&gt; /// = &lt;all non-imputed variables&gt;, add(&lt;number of imputations&gt;) The “” are essentially what type of model you would use to predict the outcome. For example, for continuous data, use regress. For binary data use logit. It also supports ologit (ordinal logistic regression, multiple categories with ordering), mlogit (multinomial logistic regression, multiple categories without ordering), poisson or nbreg (poisson regression or negative binomial regression, for count data), as well as some others. See help mi impute chained under “uvmethod” for the full list. The add( ) option specifies how many imputed data sets to generate, we’ll discuss below how to choose this. Continuing with our example might make this more clear. To perform our imputation, we would use . mi impute chained (regress) bmi age = attack smokes female hsgrad, add(5) note: missing-value pattern is monotone; no iteration performed Conditional models (monotone): age: regress age attack smokes female hsgrad bmi: regress bmi age attack smokes female hsgrad Performing chained iterations ... Multivariate imputation Imputations = 5 Chained equations added = 5 Imputed: m=1 through m=5 updated = 0 Initialization: monotone Iterations = 0 burn-in = 0 bmi: linear regression age: linear regression ------------------------------------------------------------------ | Observations per m |---------------------------------------------- Variable | Complete Incomplete Imputed | Total -------------------+-----------------------------------+---------- bmi | 126 28 28 | 154 age | 142 12 12 | 154 ------------------------------------------------------------------ (complete + incomplete = total; imputed is the minimum across m of the number of filled-in observations.) Since both bmi and age are continuous variables, we use method regress. Imagine if we were also imputing smokes, a binary variable. Then the imputation (after running mi register imputed smokes) would be: mi impute chained (regress) bmi age (logit) smokes = attack female hsgrad, add(5) Here, regress was used for bmi and age, and logit was used for smokes. 8.4.1 Choosing the number of imputations Classic literature has suggested you need only 5 imputations to obtain valid results, though some modern literature (Graham 2007) suggest needing many more, 20 or even 100. If your data is not too large, 100 is a great choice. You can always try running the entire procedure with 5 imputations twice. If your results differ, you should try running many more imputations to stabilize the estimates. 8.4.2 mi variables After you’ve performed your imputation17, three new variables are added to your data, and your data gets \\(M\\) additional copies of itself. In the example above, we added 5 imputations, so there are a total of 6 copies of the data - the raw data (with the missing values), and 5 copies with imputed values. The new variables added are: _mi_id is the ID number of each row corresponding to its position in the original data _mi_miss flags whether the row had missing data originally. _mi_m is which data-set we’re looking at. 0 represents the unimputed data, 1 represents the first imputation, 2 the second, etc. 8.5 Analyzing mi data Now that we’ve got the data set up for multiple imputations, and done the imputation, most of the hard part is over. Analyzing MI data is straightforward, usually. (When it isn’t, you can do this manually.) Basically, take any analysis command you would normally run, e.g. regress y x, and preface it by mi estimate:. Let’s try to predict the odds of a heart attack based upon other characteristics in the data. We would run a logistic regression model, logit attack smokes age bmi female hsgrad So to run it with multiple imputations: . mi estimate: logit attack smokes age bmi female hsgrad Multiple-imputation estimates Imputations = 5 Logistic regression Number of obs = 154 Average RVI = 0.0966 Largest FMI = 0.2750 DF adjustment: Large sample DF: min = 62.83 avg = 53,215.09 max = 146,351.98 Model F test: Equal FMI F( 5, 1243.8) = 2.90 Within VCE type: OIM Prob &gt; F = 0.0130 ------------------------------------------------------------------------------ attack | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- smokes | 1.163433 .352684 3.30 0.001 .47217 1.854695 age | .0284627 .0164787 1.73 0.086 -.0040684 .0609938 bmi | .0800942 .0491285 1.63 0.108 -.0180864 .1782749 female | -.0970499 .4091373 -0.24 0.812 -.8989527 .7048528 hsgrad | .10968 .3991282 0.27 0.783 -.6726034 .8919634 _cons | -4.390356 1.598513 -2.75 0.006 -7.531833 -1.248878 ------------------------------------------------------------------------------ We see a single model, even though 5 models (one for each imputation) were run in the background. The results from these models were pooled using something called “Rubin’s rules” to produce a single model output. We see a few additional fit summaries about the multiple imputation that aren’t super relevant; but otherwise all the existing interpretations hold. Note that an F-test instead of \\(\\chi^2\\) test is run, but still tests the same hypothesis that all coefficients are identically zero. Among the coefficients, we see that smokers have significantly higher odds of having a heart attack, and there’s some weak evidence that age plays a role. 8.5.1 MI Postestimation In general, most postestimation commands will not work after MI. The general approach is to do the MI manually and run the postestimation for each imputation. One exception is that mi predict works how predict does. 8.6 Manual MI Since we set the data as flong, each imputed data set lives in the data with a separate _mi_m value. You can conditionally run analyses on each, e.g. logit attack smokes age bmi female hsgrad if _mi_m == 0 to run the model on only the original data. It is tedious to do this over all imputed data, so instead we can run mi xeq: as a prefix to run a command on each separate data set. This is similar to mi estimate: except without the pooling. . mi xeq: summ age m=0 data: -&gt; summ age Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- age | 142 56.43324 11.59131 20.73613 83.78423 m=1 data: -&gt; summ age Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- age | 154 56.20732 11.61166 20.73613 83.78423 m=2 data: -&gt; summ age Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- age | 154 55.79566 11.88629 16.9347 83.78423 m=3 data: -&gt; summ age Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- age | 154 56.35074 11.50551 20.73613 83.78423 m=4 data: -&gt; summ age Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- age | 154 56.35633 11.8424 20.73613 86.11715 m=5 data: -&gt; summ age Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- age | 154 56.40651 11.44234 20.73613 83.78423 This can also be useful if the analysis you want to execute is not supported by mi estimate yet. 8.6.1 Rubin’s rules If you wanted to pool the results yourself, you can obtain an estimate for the pooled parameter by simple average across imputations. The formula for variance is slightly more complicated so we don’t produce it here, however it can be found in the “Methods and formulas” section of the MI manual (run help mi estimate, click on “[MI] mi estimate” at the top of the file to open the manual. 8.7 Removing the MI data Ideally, you should save the data (or preserve it) prior to imputing, so you can easily recover the unimputed data if you wish. If you wanted to return to the original data, the following should work: mi unset drop if mi_m != 0 drop mi_* The first tells Stata not to treat it as imputed anymore; the second drops all imputed data sets; the third removes the MI variables that were generated. This only works for mi set flong; if you use another method, you can tweak the above or use mi convert flong to switch to “flong” first. 8.8 Survey and multiple imputation Just a quick note, if you want to utilize by complex survey design and multiple imputation simultaneously, proper ordering needs to be given. Note that only weights play a role in multiple imputation. mi set ... mi svyset ... mi impute ... [pweight = weight] mi estimate: svy: regress ... There has been some discussion that imputation should not take into account any complex survey design features (because you want the imputation to reflect the sample, not necessarily the population). See for example Little and Vartivarian 2003. If you follow this advice, simply exclude the [pweight = …] part of the mi impute command. In either case, estimation commands still need both the mi estimate: svy: prefixes in that order. 8.9 Citations Little, RJ, and S Vartivarian. 2003. On weighting the rates in non-response weights. Stat Med 22, no. 9: 1589-1599. Von Hippel, Paul T. “How to impute interactions, squares, and other transformed variables.” Sociological Methodology 39.1 (2009): 265-291. There is technically Little’s MCAR test to compare MCAR vs MAR, but the majority of imputation methods require only MAR, not MCAR, so it’s of limited use. Additionally, it is not yet supported in Stata.↩ Technically this happens as soon as you run mi set, but they’re not interesting until after mi impute.↩ "],
["exercise-solutions.html", "Chapter 9 Exercise solutions 9.1 Exercise 1 9.2 Exercise 2 9.3 Exercise 3 9.4 Exercise 4 9.5 Exercise 5 9.6 Exercise 6", " Chapter 9 Exercise solutions 9.1 Exercise 1 . webuse nhanes2, clear 1) . describe, short Contains data from http://www.stata-press.com/data/r15/nhanes2.dta obs: 10,351 vars: 58 20 Dec 2016 10:07 size: 1,107,557 Sorted by: There are 10,351 observations of 59 variables. The full describe output is suppressed for space, but you should run it. 3) . tab race, mi 1=white, | 2=black, | 3=other | Freq. Percent Cum. ------------+----------------------------------- White | 9,065 87.58 87.58 Black | 1,086 10.49 98.07 Other | 200 1.93 100.00 ------------+----------------------------------- Total | 10,351 100.00 No missing data. . tab diabetes, mi diabetes, | 1=yes, 0=no | Freq. Percent Cum. ------------+----------------------------------- 0 | 9,850 95.16 95.16 1 | 499 4.82 99.98 . | 2 0.02 100.00 ------------+----------------------------------- Total | 10,351 100.00 Two missing values. lead is continuous, so a table isn’t the most effective. . codebook lead ------------------------------------------------------------------------------- lead lead (mcg/dL) ------------------------------------------------------------------------------- type: numeric (byte) range: [2,80] units: 1 unique values: 53 missing .: 5,403/10,351 mean: 14.3203 std. dev: 6.16647 percentiles: 10% 25% 50% 75% 90% 8 10 13 17 22 There’s a lot of missingness. 4) . pwcorr height weight bp* | height weight bpsystol bpdiast -------------+------------------------------------ height | 1.0000 weight | 0.4775 1.0000 bpsystol | -0.0364 0.2861 1.0000 bpdiast | 0.0675 0.3799 0.6831 1.0000 Blood pressure is highly correlated, more-so than height and weight. Weight is also correlated with both forms of BP. Height looks to be completely independent of boood pressure. 9.2 Exercise 2 . webuse nhanes2, clear . twoway (scatter bpdiast bpsystol if sex == 1, mcolor(blue)) /// &gt; (scatter bpdiast bpsystol if sex == 2, mcolor(pink)) /// &gt; (lfit bpdiast bpsystol if sex == 1, lcolor(blue)) /// &gt; (lfit bpdiast bpsystol if sex == 2, lcolor(pink)), /// &gt; legend(label(1 &quot;Men&quot;) label(2 &quot;Women&quot;) order(1 2)) We can see the correlation between blood pressure measures, with a bit stronger of a relationship for men. 9.3 Exercise 3 . webuse nhanes2, clear 1) The sample size is massive, so the central limit theorem suffices. 2) . ttest height == 176 if sex == 1 One-sample t test ------------------------------------------------------------------------------ Variable | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- height | 4,915 174.7421 .1026447 7.196115 174.5408 174.9433 ------------------------------------------------------------------------------ mean = mean(height) t = -12.2553 Ho: mean = 176 degrees of freedom = 4914 Ha: mean &lt; 176 Ha: mean != 176 Ha: mean &gt; 176 Pr(T &lt; t) = 0.0000 Pr(|T| &gt; |t|) = 0.0000 Pr(T &gt; t) = 1.0000 The test rejects; the average height of men in the sample is lower than the national average. 3) . ttest age, by(sex) Two-sample t test with equal variances ------------------------------------------------------------------------------ Group | Obs Mean Std. Err. Std. Dev. [95% Conf. Interval] ---------+-------------------------------------------------------------------- Male | 4,915 47.4238 .2448869 17.1683 46.94372 47.90389 Female | 5,436 47.72057 .2340613 17.25716 47.26171 48.17942 ---------+-------------------------------------------------------------------- combined | 10,351 47.57965 .1692044 17.21483 47.24798 47.91133 ---------+-------------------------------------------------------------------- diff | -.2967619 .338842 -.9609578 .3674339 ------------------------------------------------------------------------------ diff = mean(Male) - mean(Female) t = -0.8758 Ho: diff = 0 degrees of freedom = 10349 Ha: diff &lt; 0 Ha: diff != 0 Ha: diff &gt; 0 Pr(T &lt; t) = 0.1906 Pr(|T| &gt; |t|) = 0.3812 Pr(T &gt; t) = 0.8094 We fail to reject; there is no difference that the average height differs by gender. 4) . tab race diabetes 1=white, | 2=black, | diabetes, 1=yes, 0=no 3=other | 0 1 | Total -----------+----------------------+---------- White | 8,659 404 | 9,063 Black | 1,000 86 | 1,086 Other | 191 9 | 200 -----------+----------------------+---------- Total | 9,850 499 | 10,349 Given the different scales per race, it’s hard to draw a comparison. We can look at the rowwise percents. (If you ran tab diabetes race, you’d need the columnwise percents.) . tab race diabetes, row chi2 +----------------+ | Key | |----------------| | frequency | | row percentage | +----------------+ 1=white, | 2=black, | diabetes, 1=yes, 0=no 3=other | 0 1 | Total -----------+----------------------+---------- White | 8,659 404 | 9,063 | 95.54 4.46 | 100.00 -----------+----------------------+---------- Black | 1,000 86 | 1,086 | 92.08 7.92 | 100.00 -----------+----------------------+---------- Other | 191 9 | 200 | 95.50 4.50 | 100.00 -----------+----------------------+---------- Total | 9,850 499 | 10,349 | 95.18 4.82 | 100.00 Pearson chi2(2) = 25.3630 Pr = 0.000 Nearly double the percent of blacks have diabetes and the \\(\\chi^2\\) test confirms the difference is statistically significant. 9.4 Exercise 4 . webuse nhanes2, clear . regress lead i.sex i.race c.age c.weight c.height i.region Source | SS df MS Number of obs = 4,948 -------------+---------------------------------- F(9, 4938) = 129.98 Model | 36027.945 9 4003.105 Prob &gt; F = 0.0000 Residual | 152083.33 4,938 30.7985682 R-squared = 0.1915 -------------+---------------------------------- Adj R-squared = 0.1901 Total | 188111.275 4,947 38.0253234 Root MSE = 5.5496 ------------------------------------------------------------------------------ lead | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- sex | Female | -5.26415 .2259894 -23.29 0.000 -5.70719 -4.82111 | race | Black | 2.937124 .2671182 11.00 0.000 2.413454 3.460794 Other | -.5521668 .5617011 -0.98 0.326 -1.653351 .549017 | age | .0196081 .0048918 4.01 0.000 .0100181 .0291981 weight | -.0012435 .0059001 -0.21 0.833 -.0128103 .0103233 height | -.0275532 .0127149 -2.17 0.030 -.05248 -.0026264 | region | MW | -.1099025 .233614 -0.47 0.638 -.5678897 .3480848 S | -1.858491 .2346487 -7.92 0.000 -2.318507 -1.398476 W | -.2854048 .2380554 -1.20 0.231 -.7520992 .1812897 | _cons | 21.16891 2.199034 9.63 0.000 16.85783 25.48 ------------------------------------------------------------------------------ 1) The F-test rejects and the R-squared is low but good, so this model fits decently. 2) The coffecient on “Female” is -5 and is statistically significant, so there is evidence that males have higher average lead levels. 3) The p-value is very small, so it is statistically significant. However, if we look at lead levels: . summ lead Variable | Obs Mean Std. Dev. Min Max -------------+--------------------------------------------------------- lead | 4,948 14.32033 6.166468 2 80 We see that lead levels range from 2 to 80. The coefficient on age is about .02, so a person 50 years old would only expect .02*50 = 1 higher value for the lead score. Unlikely to be clinically interesting! This is a side effect of the massive sample size. 4) . margins region Predictive margins Number of obs = 4,948 Model VCE : OLS Expression : Linear prediction, predict() ------------------------------------------------------------------------------ | Delta-method | Margin Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- region | NE | 14.93498 .176734 84.51 0.000 14.5885 15.28145 MW | 14.82507 .1532375 96.75 0.000 14.52466 15.12549 S | 13.07649 .1525888 85.70 0.000 12.77734 13.37563 W | 14.64957 .1588465 92.22 0.000 14.33816 14.96098 ------------------------------------------------------------------------------ . margins region, pwcompare(pv) Pairwise comparisons of predictive margins Model VCE : OLS Expression : Linear prediction, predict() ----------------------------------------------------- | Delta-method Unadjusted | Contrast Std. Err. t P&gt;|t| -------------+--------------------------------------- region | MW vs NE | -.1099025 .233614 -0.47 0.638 S vs NE | -1.858491 .2346487 -7.92 0.000 W vs NE | -.2854048 .2380554 -1.20 0.231 S vs MW | -1.748589 .2161764 -8.09 0.000 W vs MW | -.1755023 .2216647 -0.79 0.429 W vs S | 1.573087 .2227974 7.06 0.000 ----------------------------------------------------- It looks like South is significantly lower levels of lead than the other regions, which show no difference between them. 5) . regress lead i.sex##c.age i.race c.weight c.height i.region Source | SS df MS Number of obs = 4,948 -------------+---------------------------------- F(10, 4937) = 123.65 Model | 37676.3642 10 3767.63642 Prob &gt; F = 0.0000 Residual | 150434.91 4,937 30.4709156 R-squared = 0.2003 -------------+---------------------------------- Adj R-squared = 0.1987 Total | 188111.275 4,947 38.0253234 Root MSE = 5.52 ------------------------------------------------------------------------------ lead | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- sex | Female | -8.485856 .4923314 -17.24 0.000 -9.451044 -7.520667 age | -.0150619 .0067745 -2.22 0.026 -.0283429 -.0017809 | sex#c.age | Female | .0676205 .0091936 7.36 0.000 .0495969 .0856441 | race | Black | 2.985719 .2657756 11.23 0.000 2.464681 3.506758 Other | -.5307013 .5587129 -0.95 0.342 -1.626027 .5646243 | weight | -.0044452 .0058847 -0.76 0.450 -.0159819 .0070915 height | -.0266069 .0126477 -2.10 0.035 -.0514021 -.0018118 | region | MW | -.1417546 .2324084 -0.61 0.542 -.5973783 .3138691 S | -1.897967 .2334589 -8.13 0.000 -2.35565 -1.440284 W | -.2945939 .2367891 -1.24 0.214 -.7588057 .169618 | _cons | 22.89998 2.199931 10.41 0.000 18.58714 27.21282 ------------------------------------------------------------------------------ . margins sex, at(age = (20(10)70)) Predictive margins Number of obs = 4,948 Model VCE : OLS Expression : Linear prediction, predict() 1._at : age = 20 2._at : age = 30 3._at : age = 40 4._at : age = 50 5._at : age = 60 6._at : age = 70 ------------------------------------------------------------------------------ | Delta-method | Margin Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- _at#sex | 1#Male | 17.46983 .2448614 71.35 0.000 16.98979 17.94987 1#Female | 10.33639 .2137915 48.35 0.000 9.91726 10.75551 2#Male | 17.31921 .1931539 89.67 0.000 16.94055 17.69788 2#Female | 10.86197 .1671112 65.00 0.000 10.53436 11.18958 3#Male | 17.16859 .1543971 111.20 0.000 16.86591 17.47128 3#Female | 11.38756 .1374569 82.84 0.000 11.11808 11.65704 4#Male | 17.01798 .1398113 121.72 0.000 16.74388 17.29207 4#Female | 11.91315 .1364241 87.32 0.000 11.64569 12.1806 5#Male | 16.86736 .156316 107.91 0.000 16.56091 17.17381 5#Female | 12.43873 .1645527 75.59 0.000 12.11613 12.76133 6#Male | 16.71674 .1962165 85.20 0.000 16.33207 17.10141 6#Female | 12.96432 .2104578 61.60 0.000 12.55173 13.37691 ------------------------------------------------------------------------------ . marginsplot Variables that uniquely identify margins: age sex We see significance in the interaction, so we looked at an interaction plot. Looks like men’s lead levels don’t change with age, but women’s increases with age. 6) . rvfplot This doesn’t look great. We don’t see any signs of nonnormality, but we do see a lot of very large positive residuals. If you look at a histogram for lead, . hist lead (bin=36, start=2, width=2.1666667) We see right skew. The maintainers of this data noticed the same concern, as they include a loglead variable in the data to attempt to address this. . desc loglead storage display value variable name type format label variable label ------------------------------------------------------------------------------- loglead float %9.0g log(lead) Perhaps we should have run the model with loglead as the output instead. 7) . estat vif Variable | VIF 1/VIF -------------+---------------------- 2.sex | 9.82 0.101781 age | 2.19 0.456706 sex#c.age | 2 | 9.83 0.101694 race | 2 | 1.05 0.949633 3 | 1.06 0.941038 weight | 1.34 0.745052 height | 2.44 0.410048 region | 2 | 1.71 0.583150 3 | 1.77 0.565864 4 | 1.73 0.576865 -------------+---------------------- Mean VIF | 3.30 Nothing to concern here. Sex and sex/age interaction are close, but we expect interactions to be correlated to main effects. You can center age if you are concerned. 9.5 Exercise 5 Exercise 5 . webuse nhanes2, clear . logit diabetes i.sex i.race c.age weight height i.region Iteration 0: log likelihood = -1999.7591 Iteration 1: log likelihood = -1819.9899 Iteration 2: log likelihood = -1777.7462 Iteration 3: log likelihood = -1776.9939 Iteration 4: log likelihood = -1776.9935 Iteration 5: log likelihood = -1776.9935 Logistic regression Number of obs = 10,349 LR chi2(9) = 445.53 Prob &gt; chi2 = 0.0000 Log likelihood = -1776.9935 Pseudo R2 = 0.1114 ------------------------------------------------------------------------------ diabetes | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- sex | Female | .0934803 .137068 0.68 0.495 -.175168 .3621286 | race | Black | .5781259 .1326244 4.36 0.000 .3181869 .8380648 Other | .408093 .3623376 1.13 0.260 -.3020757 1.118262 | age | .058799 .0039646 14.83 0.000 .0510286 .0665695 weight | .0268805 .0031111 8.64 0.000 .0207829 .0329782 height | -.0220928 .0076659 -2.88 0.004 -.0371175 -.007068 | region | MW | -.0265833 .1418506 -0.19 0.851 -.3046054 .2514388 S | .0998071 .1375642 0.73 0.468 -.1698138 .3694281 W | -.0984779 .1457444 -0.68 0.499 -.3841317 .1871758 | _cons | -4.645124 1.3496 -3.44 0.001 -7.290291 -1.999956 ------------------------------------------------------------------------------ 1) . estat gof Logistic model for diabetes, goodness-of-fit test ------------------------------------------------- number of observations = 10349 number of covariate patterns = 10349 Pearson chi2(10339) = 10025.54 Prob &gt; chi2 = 0.9860 . estat classification Logistic model for diabetes -------- True -------- Classified | D ~D | Total -----------+--------------------------+----------- + | 0 1 | 1 - | 499 9849 | 10348 -----------+--------------------------+----------- Total | 499 9850 | 10349 Classified + if predicted Pr(D) &gt;= .5 True D defined as diabetes != 0 -------------------------------------------------- Sensitivity Pr( +| D) 0.00% Specificity Pr( -|~D) 99.99% Positive predictive value Pr( D| +) 0.00% Negative predictive value Pr(~D| -) 95.18% -------------------------------------------------- False + rate for true ~D Pr( +|~D) 0.01% False - rate for true D Pr( -| D) 100.00% False + rate for classified + Pr(~D| +) 100.00% False - rate for classified - Pr( D| -) 4.82% -------------------------------------------------- Correctly classified 95.17% -------------------------------------------------- Both suggest the model does not fit well. Note that the original \\(\\chi^2\\) test does show the model is better than a model with no predictors, but the classification shows that we predict a negative response for all but a single observation. 2) . margins race, pwcompare(pv) Pairwise comparisons of predictive margins Model VCE : OIM Expression : Pr(diabetes), predict() -------------------------------------------------------- | Delta-method Unadjusted | Contrast Std. Err. z P&gt;|z| ----------------+--------------------------------------- race | Black vs White | .0301076 .0081584 3.69 0.000 Other vs White | .0197925 .0204751 0.97 0.334 Other vs Black | -.0103152 .0220354 -0.47 0.640 -------------------------------------------------------- . margins region, pwcompare(pv) Pairwise comparisons of predictive margins Model VCE : OIM Expression : Pr(diabetes), predict() ----------------------------------------------------- | Delta-method Unadjusted | Contrast Std. Err. z P&gt;|z| -------------+--------------------------------------- region | MW vs NE | -.0011484 .0061371 -0.19 0.852 S vs NE | .0045414 .0062131 0.73 0.465 W vs NE | -.0041309 .0061392 -0.67 0.501 S vs MW | .0056899 .0056911 1.00 0.317 W vs MW | -.0029825 .0056876 -0.52 0.600 W vs S | -.0086724 .0057535 -1.51 0.132 ----------------------------------------------------- Blacks are more likely to have diabetes than whites or others. Age and weight are positive predictors whereas height is a negative predictor for some reason. There is no effect of gender or region. 9.6 Exercise 6 . webuse chicken, clear . melogit complain grade i.race i.gender tenure age income nworkers i.genderm | &gt; | restaurant: Fitting fixed-effects model: Iteration 0: log likelihood = -1341.7541 Iteration 1: log likelihood = -1337.7735 Iteration 2: log likelihood = -1337.7659 Iteration 3: log likelihood = -1337.7659 Refining starting values: Grid node 0: log likelihood = -1331.542 Fitting full model: Iteration 0: log likelihood = -1331.542 Iteration 1: log likelihood = -1323.2469 Iteration 2: log likelihood = -1321.7555 Iteration 3: log likelihood = -1321.7325 Iteration 4: log likelihood = -1321.7325 Mixed-effects logistic regression Number of obs = 2,763 Group variable: restaurant Number of groups = 500 Obs per group: min = 3 avg = 5.5 max = 8 Integration method: mvaghermite Integration pts. = 7 Wald chi2(9) = 117.56 Log likelihood = -1321.7325 Prob &gt; chi2 = 0.0000 ------------------------------------------------------------------------------ complain | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- grade | .0616416 .0463355 1.33 0.183 -.0291742 .1524574 | race | 2 | .5512152 .140432 3.93 0.000 .2759736 .8264568 3 | 1.180759 .137298 8.60 0.000 .9116599 1.449858 | 1.gender | .5866638 .1059402 5.54 0.000 .3790247 .7943029 tenure | -.0699963 .1727861 -0.41 0.685 -.4086509 .2686582 age | -.0748883 .0230589 -3.25 0.001 -.1200828 -.0296938 income | -.0034531 .0016933 -2.04 0.041 -.006772 -.0001343 nworkers | -.0303442 .0391781 -0.77 0.439 -.1071318 .0464435 1.genderm | .0795912 .1245047 0.64 0.523 -.1644336 .3236159 _cons | .3910687 1.103955 0.35 0.723 -1.772644 2.554781 -------------+---------------------------------------------------------------- restaurant | var(_cons)| .5755458 .1417722 .3551455 .9327247 ------------------------------------------------------------------------------ LR test vs. logistic model: chibar2(01) = 32.07 Prob &gt;= chibar2 = 0.0000 1) We can’t look at fit statistics, but the \\(\\chi^2\\) is significant, so we’re doing better than chance. 2) . margins race, pwcompare(pv) Pairwise comparisons of predictive margins Model VCE : OIM Expression : Marginal predicted mean, predict() ----------------------------------------------------- | Delta-method Unadjusted | Contrast Std. Err. z P&gt;|z| -------------+--------------------------------------- race | 2 vs 1 | .0665377 .0166607 3.99 0.000 3 vs 1 | .1685295 .0185599 9.08 0.000 3 vs 2 | .1019918 .019303 5.28 0.000 ----------------------------------------------------- Unfortunately, this data is poorly labeled so we can’t talk in specifics about things, but generally Race 1, 2 and 3 have increasing odds of a complaint. Gender 1 has significantly higher odds than gender 2. Age and income are negatively related to the odds of a complaint (older, more well paid employees are less likely to have complaints). Neither restaurant level characteristic is significant once server characteristics are accounted for. 3) The estimated random variance is non-zero, so yes, the random effects for restaurants are warranted. "]
]
