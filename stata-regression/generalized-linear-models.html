<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Regression Modeling in Stata</title>
  <meta name="description" content="Regression Modeling in Stata">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Regression Modeling in Stata" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Regression Modeling in Stata" />
  
  
  

<meta name="author" content="Josh Errickson">


<meta name="date" content="2018-10-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ordinary-least-squares.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="css/standard_html.css" type="text/css" />
<link rel="stylesheet" href="css/book.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Regression Modeling in Stata</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#how-to-use-this-document"><i class="fa fa-check"></i><b>1.1</b> How to use this document</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#contact-information"><i class="fa fa-check"></i><b>1.2</b> Contact information</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#cscar"><i class="fa fa-check"></i><b>1.2.1</b> CSCAR</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#author"><i class="fa fa-check"></i><b>1.2.2</b> Author</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#usage"><i class="fa fa-check"></i><b>1.3</b> Usage</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>2</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="2.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#terminology"><i class="fa fa-check"></i><b>2.1</b> Terminology</a></li>
<li class="chapter" data-level="2.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#theory"><i class="fa fa-check"></i><b>2.2</b> Theory</a></li>
<li class="chapter" data-level="2.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#fitting-the-model"><i class="fa fa-check"></i><b>2.3</b> Fitting the model</a></li>
<li class="chapter" data-level="2.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#including-categorical-predictors"><i class="fa fa-check"></i><b>2.4</b> Including categorical predictors</a></li>
<li class="chapter" data-level="2.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#interactions"><i class="fa fa-check"></i><b>2.5</b> Interactions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#centering"><i class="fa fa-check"></i><b>2.5.1</b> Centering</a></li>
<li class="chapter" data-level="2.5.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#fitting-separate-models"><i class="fa fa-check"></i><b>2.5.2</b> Fitting Separate Models</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#robust-standard-errors"><i class="fa fa-check"></i><b>2.6</b> Robust standard errors</a></li>
<li class="chapter" data-level="2.7" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#assumptions"><i class="fa fa-check"></i><b>2.7</b> Assumptions</a><ul>
<li class="chapter" data-level="2.7.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#relationship-is-linear-and-additive"><i class="fa fa-check"></i><b>2.7.1</b> Relationship is linear and additive</a></li>
<li class="chapter" data-level="2.7.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#errors-are-homogeneous"><i class="fa fa-check"></i><b>2.7.2</b> Errors are homogeneous</a></li>
<li class="chapter" data-level="2.7.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#independence"><i class="fa fa-check"></i><b>2.7.3</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#variable-transformations"><i class="fa fa-check"></i><b>2.8</b> Variable Transformations</a></li>
<li class="chapter" data-level="2.9" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#miscellaneous-concerns"><i class="fa fa-check"></i><b>2.9</b> Miscellaneous concerns</a><ul>
<li class="chapter" data-level="2.9.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multicollinearity"><i class="fa fa-check"></i><b>2.9.1</b> Multicollinearity</a></li>
<li class="chapter" data-level="2.9.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#overfitting"><i class="fa fa-check"></i><b>2.9.2</b> Overfitting</a></li>
<li class="chapter" data-level="2.9.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-selection-is-bad"><i class="fa fa-check"></i><b>2.9.3</b> Model Selection is bad</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>3</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="3.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#logistic-regression"><i class="fa fa-check"></i><b>3.1</b> Logistic Regression</a><ul>
<li class="chapter" data-level="3.1.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#fitting-the-logistic-model"><i class="fa fa-check"></i><b>3.1.1</b> Fitting the logistic model</a></li>
<li class="chapter" data-level="3.1.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#categorical-variables-and-interactions"><i class="fa fa-check"></i><b>3.1.2</b> Categorical Variables and Interactions</a></li>
<li class="chapter" data-level="3.1.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#margins-and-predict"><i class="fa fa-check"></i><b>3.1.3</b> <code>margins</code> and <code>predict</code></a></li>
<li class="chapter" data-level="3.1.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#logistic-regression-assumptions"><i class="fa fa-check"></i><b>3.1.4</b> Logistic regression assumptions</a></li>
<li class="chapter" data-level="3.1.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#logistic-goodness-of-fit"><i class="fa fa-check"></i><b>3.1.5</b> Logistic goodness of fit</a></li>
<li class="chapter" data-level="3.1.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#separation"><i class="fa fa-check"></i><b>3.1.6</b> Separation</a></li>
<li class="chapter" data-level="3.1.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#logit-miscellaneous."><i class="fa fa-check"></i><b>3.1.7</b> <code>logit</code> Miscellaneous.</a></li>
<li class="chapter" data-level="3.1.8" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#logit-vs-logistic"><i class="fa fa-check"></i><b>3.1.8</b> <code>logit</code> vs <code>logistic</code></a></li>
<li class="chapter" data-level="3.1.9" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#sample-size-concerns"><i class="fa fa-check"></i><b>3.1.9</b> Sample size concerns</a></li>
<li class="chapter" data-level="3.1.10" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#rare-outcomes"><i class="fa fa-check"></i><b>3.1.10</b> Rare outcomes</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>3.2</b> Poisson regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#fitting-the-model-1"><i class="fa fa-check"></i><b>3.2.1</b> Fitting the model</a></li>
<li class="chapter" data-level="3.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#interactions-categorical-variables-margins-predict"><i class="fa fa-check"></i><b>3.2.2</b> Interactions, categorical variables, <code>margins</code>, <code>predict</code></a></li>
<li class="chapter" data-level="3.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#assumptions-1"><i class="fa fa-check"></i><b>3.2.3</b> Assumptions</a></li>
<li class="chapter" data-level="3.2.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#exposure"><i class="fa fa-check"></i><b>3.2.4</b> Exposure</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#other-regression-models"><i class="fa fa-check"></i><b>3.3</b> Other regression models</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Regression Modeling in Stata</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="generalized-linear-models" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Generalized Linear Models</h1>
<p>
If the outcome variable is not continuous, while OLS will usually be able to be fit, the results may be unexpected or undesired. For example, if the response is a binary indicator, an OLS model fit may predict an individual has a negative response.
</p>
<p>
We can generalize the <a target="_blank" href="ordinary-least-squares.html#theory">model</a> from ordinary least squares to allow a non-linear relationship between the predictors and the outcome, which may fit different outcomes better.
</p>
<p>
Recall that the equation for OLS is
</p>
<p>
<span class="math display">\[ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p + \epsilon \]</span>
</p>
<p>
We can modify this by allowing the left hand side to be a function of <span class="math inline">\(Y\)</span>,
</p>
<p>
<span class="math display">\[ f(Y) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p + \epsilon \]</span>
</p>
<p>
Note that this is still linear in <span class="math inline">\(X\)</span> (the right-hand side). Non-linear regession refers to something such as
</p>
<p>
<span class="math display">\[ Y = \beta_1X_1^{\beta_2X_2} + \epsilon \]</span>
</p>
<p>
Therefore, even though the function <span class="math inline">\(f()\)</span> may not be linear, the model is still linear - hence “generalized linear model”.
</p>
<p>
The function, <span class="math inline">\(f()\)</span>, is called the “link” function. If the link function is the identify function, <span class="math inline">\(f(x) = x\)</span>, the GLM simplifies to ordinary least squares.
</p>
<p>
We’ll talk about a few link functions and the regression models they define.
</p>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">3.1</span> Logistic Regression</h2>
<p>
Logistic regression is used when the outcome is dichotomous - either a positive outcome (1) or a negative outcome (0). For example, presence or absence of some disease. The link function for logistic regression is logit,
</p>
<p>
<span class="math display">\[ \textrm{logit}(x) = \textrm{log}\Big(\frac{x}{1-x}\Big) \]</span>
</p>
<p>
<span class="math display">\[ \textrm{logit}\left(P(Y = 1 | X)\right) = \beta_0 + \beta_1X_1 + \cdots + \beta_pX_p + \epsilon. \]</span>
</p>
<p>
Note also that unlike in OLS, the left-hand side is not the observed outcome, but rather the probability of a positive outcome. So the goal of a logistic model is not to predict whether an individual will have a positive outcome, but rather to predict their probability of a positive outcome. This is a subtle difference, but worth pointing out since predicted values will be probabilities, not a binary response.
</p>
<div id="fitting-the-logistic-model" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Fitting the logistic model</h3>
<p>
We can fit a logistic regression using the <code>logit</code> command in State. It works very similarly to <code>regress</code>. Let’s run a model predicting the presence of a cellar based on square footage, region and electricity expenditure.
</p>
<pre><code></code></pre>
<pre><code>. tab cellar

     CELLAR |      Freq.     Percent        Cum.
------------+-----------------------------------
         -2 |      1,455       25.59       25.59
          0 |      2,490       43.79       69.38
          1 |      1,741       30.62      100.00
------------+-----------------------------------
      Total |      5,686      100.00

. replace cellar = . if cellar == -2
(1,455 real changes made, 1,455 to missing)

. logit cellar dollarel totsqft_en i.regionc female

Iteration 0:   log likelihood = -2866.0585  
Iteration 1:   log likelihood =      -1748  
Iteration 2:   log likelihood = -1730.0798  
Iteration 3:   log likelihood = -1729.9962  
Iteration 4:   log likelihood = -1729.9962  

Logistic regression                             Number of obs     =      4,231
                                                LR chi2(6)        =    2272.12
                                                Prob &gt; chi2       =     0.0000
Log likelihood = -1729.9962                     Pseudo R2         =     0.3964

------------------------------------------------------------------------------
      cellar |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    dollarel |  -.0004174   .0000575    -7.26   0.000    -.0005301   -.0003048
  totsqft_en |   .0009164   .0000417    21.98   0.000     .0008347    .0009981
             |
     regionc |
    Midwest  |  -.3488001   .1461488    -2.39   0.017    -.6352464   -.0623538
      South  |  -3.188248   .1435758   -22.21   0.000    -3.469651   -2.906844
       West  |   -3.11719   .1482945   -21.02   0.000    -3.407842   -2.826538
             |
      female |  -.1458111   .0864176    -1.69   0.092    -.3151864    .0235642
       _cons |   .0504946   .1659137     0.30   0.761    -.2746904    .3756795
------------------------------------------------------------------------------

</code></pre>
<p>
When you try this yourself, you may notice that its not quite as fast as <code>regress</code>. That is because for OLS we have a “closed form solution” - we just do some quick math and reach an answer. However, almost every other type of regression lacks a closed form solution, so instead we solve it iteratively - Stata guesses at the best coefficients that minimize error<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>, and uses an algorithm to repeatedly improve those coefficients until the reduction in error is below some threshold.
</p>
<p>
From this output, we get the “Number of obs” again. Instead of an ANOVA table with a F-statistic to test model significance, there is instead a “chi2” (<span class="math inline">\(\chi^2\)</span>, pronounced “ky-squared” as in “Kyle”). In this model, we reject the null that all coefficients are identically 0.
</p>
<p>
When we move away from linear regression, we no longer get an <span class="math inline">\(R^2\)</span> measure. There have been various pseudo-<span class="math inline">\(R^2\)</span>’s suggested, and Stata reports one here, but be careful assigning too much meaning to it. It is not uncommon to get pseudo-<span class="math inline">\(R^2\)</span> values that are negative or above 1. We’ll discuss measuring <a target="_blank" href="#logistic-goodness-of-fit">goodness of fit</a> below.
</p>
<p>
The coefficients table is interpreted in almost the same way as with regression. We see that square footage and energy expenditure have significant coefficient (positive and negative respectively), and there appears to be no gender effect. There are differences between regions.
</p>
<p>
However, we <em>cannot</em> nicely interpret these coefficients, which are known as the “log odds”. All we can say is that “As square footage increases, the probability of a house having a cellar increases.”
</p>
<p>
To add any interpretability to these coefficients, we should instead look at the odds ratios. These are the exponentiated log odds. We can ask Stata to produce these with the <code>or</code> option.
</p>
<pre><code>. logit, or

Logistic regression                             Number of obs     =      4,231
                                                LR chi2(6)        =    2272.12
                                                Prob &gt; chi2       =     0.0000
Log likelihood = -1729.9962                     Pseudo R2         =     0.3964

------------------------------------------------------------------------------
      cellar | Odds Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
    dollarel |   .9995826   .0000575    -7.26   0.000       .99947    .9996953
  totsqft_en |   1.000917   .0000417    21.98   0.000     1.000835    1.000999
             |
     regionc |
    Midwest  |   .7055342   .1031129    -2.39   0.017     .5298049    .9395504
      South  |   .0412441   .0059217   -22.21   0.000     .0311279    .0546479
       West  |   .0442814   .0065667   -21.02   0.000     .0331126    .0592175
             |
      female |    .864321   .0746925    -1.69   0.092     .7296529    1.023844
       _cons |   1.051791   .1745066     0.30   0.761     .7598073     1.45598
------------------------------------------------------------------------------
Note: _cons estimates baseline odds.

</code></pre>
<p>
Notice that the “chi2”, “PseudoR2”, “z” and “P&gt;|z|” do not change - we’re fitting the same model! We’re just changing how the coefficients are represented.
</p>
<p>
Odds ratios null hypothesis is at 1, not at 0. A value of 1 represents equal odds (or no change in odds). Odds ratios are always positive. So a significant odds ratio will be away from 1, rather than away from 0 as in linear regression or the log odds. The interpretation of odds ratios can be tricky, so let’s be precise here.
</p>
<p>
For categorical predictors, the interpretation is fairly straightforward. The coefficient on females is 0.8643. This means that for every 1 female respondent who has a basement in their house, you would expect 0.8643 male respondents to have a basement.
</p>
<p>
For continuous predictors, its the odds as the value of the predictor changes. Consider the coefficient on energy expenditure, 0.999583. For every 1 house of expenditure <span class="math inline">\(e\)</span> which has a cellar, you’d expect 0.999583 houses at expenditure <span class="math inline">\(e+1\)</span> to have a cellar.
</p>
<p>
Those coefficients are really close to 1 due to scaling: a $1 increase or 1-sqft increase is irrelevant. Due to the non-linear relationship between the predictors and the outcome, we cannot simply multiply the odds ratios. Instead, let’s scale the variables and re-fit the model.
</p>
<pre><code>. generate dollarel1000 = dollarel/1000

. generate totsqft1000 = totsqft_en/1000

. logit cellar dollarel1000 totsqft1000 i.regionc female, or

Iteration 0:   log likelihood = -2866.0585  
Iteration 1:   log likelihood =      -1748  
Iteration 2:   log likelihood = -1730.0798  
Iteration 3:   log likelihood = -1729.9962  
Iteration 4:   log likelihood = -1729.9962  

Logistic regression                             Number of obs     =      4,231
                                                LR chi2(6)        =    2272.12
                                                Prob &gt; chi2       =     0.0000
Log likelihood = -1729.9962                     Pseudo R2         =     0.3964

------------------------------------------------------------------------------
      cellar | Odds Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
dollarel1000 |   .6587254   .0378633    -7.26   0.000     .5885423    .7372778
 totsqft1000 |   2.500286   .1042513    21.98   0.000     2.304083    2.713196
             |
     regionc |
    Midwest  |   .7055342   .1031129    -2.39   0.017     .5298049    .9395504
      South  |   .0412441   .0059217   -22.21   0.000     .0311279    .0546479
       West  |   .0442814   .0065667   -21.02   0.000     .0331126    .0592175
             |
      female |    .864321   .0746925    -1.69   0.092     .7296529    1.023844
       _cons |   1.051791   .1745066     0.30   0.761     .7598073     1.45598
------------------------------------------------------------------------------
Note: _cons estimates baseline odds.

</code></pre>
<p>
Note once again that the model fit characteristics haven’t changed; we’ve fit the same model, just with different units. Now the interpretation are more reasonable. As the expenditure increases by $1000, the odds of having a cellar decrease by 34%.
</p>
<p>
For every additional 1000-square feet, the odds of having a cellar increases by 150%. In other words, for every two 2000-square foot house, you’d expect five 3000-square foot houses.
</p>
</div>
<div id="categorical-variables-and-interactions" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Categorical Variables and Interactions</h3>
<p>
Both <a href="#including-categorical%20predictors">categorical variables</a> and <a target="_blank" href="#interactions">interactions</a> can be included as they were in linear regression, with the appropriate interpretation of coefficients/odds ratios.
</p>
</div>
<div id="margins-and-predict" class="section level3">
<h3><span class="header-section-number">3.1.3</span> <code>margins</code> and <code>predict</code></h3>
<p>
The <code>margins</code> command works mostly the same, though it produces results on the probability scale, not the odds scale.
</p>
<pre><code>. margins regionc

Predictive margins                              Number of obs     =      4,231
Model VCE    : OIM

Expression   : Pr(cellar), predict()

------------------------------------------------------------------------------
             |            Delta-method
             |     Margin   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     regionc |
  Northeast  |   .7852513   .0181353    43.30   0.000     .7497068    .8207958
    Midwest  |   .7288181   .0144762    50.35   0.000     .7004453    .7571909
      South  |   .2087012   .0102263    20.41   0.000     .1886581    .2287443
       West  |    .218189   .0113706    19.19   0.000      .195903     .240475
------------------------------------------------------------------------------

</code></pre>
<p>
The <code>predict</code> command adds a new <a target="_blank" href="#obtaining-predicted-values-and-residuals">statistic</a>. <code>xb</code> now is the linear predictor, which is often not useful. Instead, the <code>pr</code> statistic returns the estimated probability of a positive outcome.
</p>
</div>
<div id="logistic-regression-assumptions" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Logistic regression assumptions</h3>
<p>
The assumptions for logistic regression are simpler than linear. The outcome measure must be binary with a 0 for a negative response and 1 for a positive. (Technically they don’t have to be positive/negative. We could think of predicting male/female and code male = 0 and female = 1. However, the convention would be to consider the outcome as “The person is female” so a 1 represents a “success” and a 0 a “failure” of that statement.) The errors in a logistic regression model are fairly contained (you can’t be wrong than more than 1!) so there are no real assumptions about them. The <a target="_blank" href="#independence">independence</a> assumption is still here and still important, again, a mixed logistic model may be appropriate if the data is not independent.
</p>
</div>
<div id="logistic-goodness-of-fit" class="section level3">
<h3><span class="header-section-number">3.1.5</span> Logistic goodness of fit</h3>
<p>
As we mentioned earlier, there are various issues with the Pseudo <span class="math inline">\(R^2\)</span> reported by <code>logit</code>, so use it carefully. In fact, all measures of goodness of fit in non-OLS models are problematic. However, here are two approaches commonly used in logistic regression.
</p>
<p>
The first is to look at a classification results: If we were to choose a threshold (say .2) and classify positive/negative outcomes against it (If a predicted probability is below .2, classify as 0. If a predicted probability is above .2, classify as 1.). We don’t know what the “correct” threshold is (if you even believe there could be one), but we can test over a range of thresholds and see how well we classify.
</p>
<pre><code>. lroc

Logistic model for cellar

number of observations =     4231
area under ROC curve   =   0.8928

</code></pre>
<img src="Graph.svg" >
<p>
This is called a ROC curve (Receiver Operating Characteristic). Starting with a threshold of 1 (so no houses are predicted to have a cellar) at (0,0) and continuing to a threshold of 0 (all houses are predicted to have a cellar), each point is plotted as sensitivity (percent of correctly predicted positive responses) versus specificity (incorrecly predicted negative responses). With a threshold of 1, every house with a cellar is predicted to not a cellar (so 0% correct) and every house without a cellar is predicted to have a cellar (0% correct). As the threshold increases, these values are computed and plotted. The diagonal <span class="math inline">\(y=x\)</span> line represents a completely uninformative model, and the ROC curve cannot pass below it. The closer it is to the top left corner, the better predictive the model is. The AUC (area under curve) is another measure of model-fit: .5 would indicate no information (ROC on the diagonal), 1 would indicate perfect fit (ROC to the top left corner). The AUC here is 0.89 indicating some predictive power.
</p>
<p>
The second is a more formal test. There are two variants, a Pearson <span class="math inline">\(\chi^2\)</span> test and the Hosmer-Lemeshow test. Both are fit with the <code>estat gof</code> command. Both are testing the hypothesis that the observed positive responses match predicted positive responses in subgroups of the population. Therefore we do <em>not</em> want to reject these tests, and a large p-value is desired.
</p>
<pre><code>. estat gof

Logistic model for cellar, goodness-of-fit test
-----------------------------------------------

       number of observations =      4231
 number of covariate patterns =      4231
           Pearson chi2(4224) =      4070.48
                  Prob &gt; chi2 =         0.9539

</code></pre>
<p>
We see here a p-value of 0.954, failing to reject the null, so there is no evidence that the model fits poorly.
</p>
<p>
There is some concern that when the “number of covariate patterns” is close to the number of observations , the Pearson test is invalid. In this data, we see that 4231 is indeed equal to 4231. Instead, we can use the Hosmer-Lemeshow by passing the <code>group(#)</code> option:
</p>
<pre><code>. estat gof, group(10)

Logistic model for cellar, goodness-of-fit test
-----------------------------------------------

  (Table collapsed on quantiles of estimated probabilities)

       number of observations =      4231
             number of groups =        10
      Hosmer-Lemeshow chi2(8) =        58.12
                  Prob &gt; chi2 =         0.0000

</code></pre>
<p>
The p-value gets very significant.
</p>
<p>
Why did we choose 10 groups? It’s just the standard. The only thing to keep in mind is that the Hosmer-Lemeshow test is only appropriate if the number of groups is greater than the number of predictors (including the intercept). In this model, we had two predictors, so that’s 3 total (including the intercept), so 10 is OK. There is some discussion that this choice can bias results - there are examples out there where passing 9 to the option rejects the test, whereas passing 11 passes.
</p>
<p>
Overall, as mentioned, take these goodness-of-fit measures with a grain of salt. Focus on interpreting the coefficients.
</p>
</div>
<div id="separation" class="section level3">
<h3><span class="header-section-number">3.1.6</span> Separation</h3>
<p>
Since the logistic regression model is solved <a target="_blank" href="regression.html#fitting-the-logistic-model">iteratively</a>, this can fail for a number of reasons. Before you begin interpreting the model, you’ll want to glance at the iteration steps and make sure that no errors were printed. The most common failure is due to separation.
</p>
<p>
With a binary outcome instead of a continuous outcome, it is much easier to have a predictor (or set of predictors) that perfectly predict the outcome. Consider trying to predict gender based on height. With a smaller sample, it’s not hard to imagine a scenario where every male is taller than every female. This is called “perfect separation”; using this sample, knowing height gives perfect information about gender
</p>
<p>
“Partial separation” can also occur; this is when prediction is perfect only for one limit. Take the height scenario again; say everyone above 5’8&quot; is male, and there are two men but the rest women below 5’8“. Here, we will always predict Male for heights above 5’8”.
</p>
<p>
Separation (of either type) often produces coefficients to be extreme with large standard errors. Stata will sometimes warn about this, but not always. If you notice these exceptional coefficients or if Stata does warn about separation, you’ll need to investigate and consider excluding certain predictors. It may seem counter-intuitive to exclude extremely highly predictive variables, but if a variable produces perfect separation, you don’t need a model to inform you of that.
</p>
</div>
<div id="logit-miscellaneous." class="section level3">
<h3><span class="header-section-number">3.1.7</span> <code>logit</code> Miscellaneous.</h3>
<p>
The <code>logit</code> model supports the margins command just like <code>regress</code> does. It does not support <code>estat vif</code> because variance inflation factors are not defined for logistic models.
</p>
<p>
Collinearity, overfitting, and model selection remain concerns in the logistic model.
</p>
<p>
Robust standard errors via <code>vce(robust)</code> are supported.
</p>
<p>
One other common cause of failed convergence is scaling - try scaling all your variables and see if that improves convergence.
</p>
</div>
<div id="logit-vs-logistic" class="section level3">
<h3><span class="header-section-number">3.1.8</span> <code>logit</code> vs <code>logistic</code></h3>
<p>
Instead of <code>logit</code>, we could run the <code>logistic</code> command. The only difference is that <code>logistic</code> reports the odds ratio by default whereas <code>logit</code> reports the log odds. My personal preference is <code>logit</code>, but there’s no need to use one over the other.
</p>
</div>
<div id="sample-size-concerns" class="section level3">
<h3><span class="header-section-number">3.1.9</span> Sample size concerns</h3>
<p>
When the percent of positive outcomes is close to 50%, the rules we <a target="_blank" href="ordinary-least-squares.html#overfitting">discussed for OLS</a> hold, 10-20 observations per predictor. As the percent of positive outcomes deviates from 50%, you may need a much larger sample size - mostly to ensure a reasonable number of both positive and negative outcomes. For example, if you expect 5% of individuals to have a positive outcome, and have a sample size of 40, that’s only 2 individuals with a positive outcome! Instead you should strive to have at least 10 or ideally over 100 individuals per outcome.
</p>
</div>
<div id="rare-outcomes" class="section level3">
<h3><span class="header-section-number">3.1.10</span> Rare outcomes</h3>
<p>
If the percent of positive outcomes is extreme (e.g. 99% or 1%), logistic regression may fail to converge. Sometimes <a target="_blank" href="#poisson-regresion">Poisson regression</a> which we’ll talk about next can be used in this situation. The estimated coefficients will be slightly biased, but convergence may easier to achieve.
</p>
</div>
</div>
<div id="poisson-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Poisson regression</h2>
<p>
When the response variable is a count (number of occurences), then using a log link function produces what’s known as Poisson regression.
</p>
<div id="fitting-the-model-1" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Fitting the model</h3>
<p>
As with logistic regression, the Poisson regression command is simple and similar to <code>regress</code>. Let’s predict the number of rooms in a house based upon the variables we’ve been dealing with so far.
</p>
<pre><code>. histogram totrooms
(bin=37, start=1, width=.48648649)

</code></pre>
<img src="Graph1.svg" >
<pre><code>. poisson totrooms dollarel1000 totsqft1000 i.cellar i.regionc female

Iteration 0:   log likelihood = -8755.2204  
Iteration 1:   log likelihood = -8755.2185  
Iteration 2:   log likelihood = -8755.2185  

Poisson regression                              Number of obs     =      4,231
                                                LR chi2(7)        =     986.09
                                                Prob &gt; chi2       =     0.0000
Log likelihood = -8755.2185                     Pseudo R2         =     0.0533

------------------------------------------------------------------------------
    totrooms |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
dollarel1000 |   .0576783    .007261     7.94   0.000      .043447    .0719096
 totsqft1000 |   .1124977   .0049483    22.73   0.000     .1027992    .1221962
    1.cellar |   .0609588   .0159101     3.83   0.000     .0297756    .0921421
             |
     regionc |
    Midwest  |   .0242295    .019944     1.21   0.224      -.01486    .0633189
      South  |   .0369115   .0211105     1.75   0.080    -.0044642    .0782873
       West  |   .0705931   .0218298     3.23   0.001     .0278075    .1133788
             |
      female |  -.0206701   .0117413    -1.76   0.078    -.0436825    .0023424
       _cons |    1.50274   .0244648    61.42   0.000      1.45479     1.55069
------------------------------------------------------------------------------

</code></pre>
<p>
We see the same sort of header information, including a Chi2 test for model significance and another pseudo <span class="math inline">\(R^2\)</span>.
</p>
<p>
The coefficients are again not interpretable other than sign and magnitude, but we can report incidence-rate ratios (IRR) instead. The <code>irr</code> option produces these, which are just the exponetiated coefficients.
</p>
<pre><code>. poisson, irr

Poisson regression                              Number of obs     =      4,231
                                                LR chi2(7)        =     986.09
                                                Prob &gt; chi2       =     0.0000
Log likelihood = -8755.2185                     Pseudo R2         =     0.0533

------------------------------------------------------------------------------
    totrooms |        IRR   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
dollarel1000 |   1.059374   .0076921     7.94   0.000     1.044405    1.074558
 totsqft1000 |    1.11907   .0055375    22.73   0.000     1.108269    1.129976
    1.cellar |   1.062855   .0169102     3.83   0.000     1.030223    1.096521
             |
     regionc |
    Midwest  |   1.024525   .0204331     1.21   0.224     .9852499    1.065367
      South  |   1.037601   .0219042     1.75   0.080     .9955457    1.081433
       West  |   1.073145   .0234265     3.23   0.001     1.028198    1.120056
             |
      female |   .9795421   .0115011    -1.76   0.078     .9572578    1.002345
       _cons |   4.493985   .1099446    61.42   0.000     4.283582    4.714722
------------------------------------------------------------------------------
Note: _cons estimates baseline incidence rate.

</code></pre>
<p>
IRR’s are slightly easier to interpret than OR’s. Each represents a average percent change in the count of the outcome predicted when there is a 1 increase in the predictor variable.
</p>
<p>
For example, the IRR for square footage is 1.119 which translates to a 11.9% predicted average increase in the number of rooms in a house which increases in sample size by 1,000 square feet.
</p>
</div>
<div id="interactions-categorical-variables-margins-predict" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Interactions, categorical variables, <code>margins</code>, <code>predict</code></h3>
<p>
Interactions and categorical variables work the same. <code>margins</code> estimates the marginal means, which are the expected number of counts. <code>predict</code> by default predicts the number of events.
</p>
</div>
<div id="assumptions-1" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Assumptions</h3>
<p>
Poisson regression has the same independence assumption. It also has a very strong assumption which is specific to the Poisson distribution - namely that the mean and variance of a Poisson random variable are equal. In other words, as the mean of a Poisson variable increases, it becomes more spread out in a linear fashion.
</p>
<p>
We can examine whether this may be true for a given variable.
</p>
<pre><code>. summarize totrooms

    Variable |        Obs        Mean    Std. Dev.       Min        Max
-------------+---------------------------------------------------------
    totrooms |      5,686    6.191347    2.360918          1         19

</code></pre>
<p>
Here the mean, 6.19 is very close to the variance, 2.36<span class="math inline">\(^2\)</span> = 5.57. This is not always the case for count data. If this is not true, a Negative Binomial model may be more appropriate. It’s extremely similar to Poisson, except it allows the mean and variance to be decoupled by means of <span class="math inline">\(\alpha\)</span>, called the overdispersion factor.
</p>
<p>
We can fit it with the <code>nbreg</code> command.
</p>
<pre><code>. nbreg totrooms dollarel1000 totsqft1000 i.cellar i.regionc female

Fitting Poisson model:

Iteration 0:   log likelihood = -8755.2204  
Iteration 1:   log likelihood = -8755.2185  
Iteration 2:   log likelihood = -8755.2185  

Fitting constant-only model:

Iteration 0:   log likelihood = -12716.439  
Iteration 1:   log likelihood = -9248.2645  
Iteration 2:   log likelihood = -9248.2645  

Fitting full model:

Iteration 0:   log likelihood = -8758.8756  
Iteration 1:   log likelihood = -8755.2187  
Iteration 2:   log likelihood = -8755.2185  

Negative binomial regression                    Number of obs     =      4,231
                                                LR chi2(7)        =     986.09
Dispersion     = mean                           Prob &gt; chi2       =     0.0000
Log likelihood = -8755.2185                     Pseudo R2         =     0.0533

------------------------------------------------------------------------------
    totrooms |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
dollarel1000 |   .0576783    .007261     7.94   0.000      .043447    .0719096
 totsqft1000 |   .1124977   .0049483    22.73   0.000     .1027992    .1221962
    1.cellar |   .0609588   .0159101     3.83   0.000     .0297756    .0921421
             |
     regionc |
    Midwest  |   .0242295    .019944     1.21   0.224      -.01486    .0633189
      South  |   .0369115   .0211105     1.75   0.080    -.0044642    .0782873
       West  |   .0705931   .0218298     3.23   0.001     .0278075    .1133788
             |
      female |  -.0206701   .0117413    -1.76   0.078    -.0436825    .0023424
       _cons |    1.50274   .0244648    61.42   0.000      1.45479     1.55069
-------------+----------------------------------------------------------------
    /lnalpha |  -27.77865          .                             .           .
-------------+----------------------------------------------------------------
       alpha |   8.63e-13          .                             .           .
------------------------------------------------------------------------------
LR test of alpha=0: chibar2(01) = 0.00                 Prob &gt;= chibar2 = 1.000

</code></pre>
<p>
The row for <code>alpha</code> is the estimate of the overdispersion factor. If this value is close to 0, the Poisson model is appropriate. That’s what is occcurring here - in fact it’s so close to 0 that Stata refuses to even compute a standard error for it. The likelihood ratio test below it is formally testing whether the Poisson model is appropriate; here the p-value of 1.0 let’s us stick with Poisson. If this rejected, the negative binomial model is more appropriate. The negative binomial model is interpreted in the same fashion as Poisson.
</p>
</div>
<div id="exposure" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Exposure</h3>
<p>
Sometimes the count is limited by the exposure - for example, if you are counting the number of students in a class who fail an exam, this number will likely be higher for classes with more students. We can adjust for this in the Poisson model by specifying the <code>exposure(___)</code> option.
</p>
<p>
In the energy data, let’s say instead of the total number of rooms, we want to predict the number of bedrooms. Obviously the total number of rooms in the house will greatly affect the number of bedrooms.
</p>
<pre><code>. histogram bedrooms
(bin=37, start=0, width=.27027027)

</code></pre>
<img src="Graph2.svg" >
<pre><code>. poisson bedrooms dollarel1000 totsqft1000 i.cellar i.regionc female, exposure
&gt; (totrooms) irr

Iteration 0:   log likelihood = -6697.7375  
Iteration 1:   log likelihood = -6697.7375  

Poisson regression                              Number of obs     =      4,231
                                                LR chi2(7)        =      29.36
                                                Prob &gt; chi2       =     0.0001
Log likelihood = -6697.7375                     Pseudo R2         =     0.0022

------------------------------------------------------------------------------
    bedrooms |        IRR   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
dollarel1000 |    1.00444   .0110315     0.40   0.687       .98305    1.026296
 totsqft1000 |   .9736346   .0074929    -3.47   0.001      .959059    .9884317
    1.cellar |   .9749456   .0229176    -1.08   0.280      .931047    1.020914
             |
     regionc |
    Midwest  |   .9819498   .0292231    -0.61   0.540      .926312    1.040929
      South  |   .9932802   .0310743    -0.22   0.829     .9342054    1.056091
       West  |   1.023995   .0329379     0.74   0.461     .9614307     1.09063
             |
      female |   1.000022    .017315     0.00   0.999     .9666552    1.034542
       _cons |   .4965649   .0181979   -19.10   0.000     .4621485    .5335443
ln(totrooms) |          1  (exposure)
------------------------------------------------------------------------------
Note: _cons estimates baseline incidence rate.

</code></pre>
<p>
We interpret this exactly as before. Negative binomial models can likewise have <code>exposure(___)</code> set.
</p>
</div>
</div>
<div id="other-regression-models" class="section level2">
<h2><span class="header-section-number">3.3</span> Other regression models</h2>
<p>
There are two extensions to logistic regression, ordinal logistic and multinomial logistic.
</p>
<p>
Ordinal logistic is used when there are more than 2 outcome categories, and they are ordered (e.g. not sick (0), mildly sick (1), very sick (2)). Using <code>ologit</code>, Stata estimates an underlying continuous distribution and returns the “cut points”, allowing categorization.
</p>
<p>
If there are multiple groups but not ordered, e.g. race, use <code>mlogit</code> for multinomial logistic regression. It essentially fits a model predicting membership in each group versus all other, with some restrictions across the models.
</p>

</div>
</div>















<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>Technically that maximizes likelihood, but that distinction is not important for understanding.<a href="generalized-linear-models.html#fnref10" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ordinary-least-squares.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
