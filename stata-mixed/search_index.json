[
["index.html", "Mixed Effects Modeling in Stata Chapter 1 Preface 1.1 How to use this document 1.2 Contact information 1.3 Usage", " Mixed Effects Modeling in Stata Josh Errickson 2018-10-23 Chapter 1 Preface 1.1 How to use this document These notes are published in bookdown format which enables easy creation of longform documents (using a mixture of markdown, R, and for these notes specifically, Stata’s dyndoc, which was added in Stata 15). The table of contents is found on the left hand side, with subsections expanding below the current section. At the top of the page are three icons, from left to right they 1) Hide/show the table of contents, 2) Search this document, and 3) Change the font the book is displayed in. All images should link to full-size versions to see detail if needed. 1.2 Contact information 1.2.1 CSCAR Office Hours: Monday-Friday 9am to 5pm (Closed Tuesday 12-1pm) Phone: 734.764.7828 Statistical Assistance: stats-consulting@umich.edu Data Science Assistance: ds-consulting@umich.edu High Performance Computing: hpc-consulting@umich.edu http://cscar.research.umich.edu/ 1.2.2 Author Josh Errickson 3550 Rackham Building 915 E. Washington Street Ann Arbor, MI 48109-1070 Email: jerrick@umich.edu 1.3 Usage This material was created for use in workshops and short courses presented by faculty and staff from the Consulting for Statistics, Computing &amp; Analytics Research (CSCAR) at the University of Michigan. No part of this material may be used for other purposes, copied, changed, or sold. "],
["mixed-model-theory.html", "Chapter 2 Mixed Model Theory 2.1 Terminology 2.2 Wide vs Long data 2.3 Level-1 versus Level-2 variables 2.4 Theory", " Chapter 2 Mixed Model Theory When fitting a regression model, the most important assumption the models make (whether it’s linear regression or generalized linear regression) is that of independence - each row of your data set is independent on all other rows. Now in general, this is almost never entirely true. If this violation is mild, it can be ignored. For example, if you give an exam to a class full of students, it’s reasonable to assume some students study together and therefore their answers on some questions (right or wrong) will tend to be similar. Here we are more concerned with a structured violation of independence. The most straightforward situation in which this arises is repeated measures. Say you’re administering an experiment where you are testing stress to different stimuli and are measuring quantities like blood pressure or heart rate. If I were to take the test multiple times, my measurements are correlated with each other - if I tend to have higher blood pressure, my blood pressure will be higher in each round regardless of stimuli. Another common repeated measures situation is follow-up over time. Say you track individuals who undergo some surgery, and follow-up with them every 2 months, asking them to take a survey on their quality of life. While there are many research questions which could be asked about such data, one might be to examine the patient’s quality of life depending on the various health markers they are experiencing. The surgery is targeting symptom A, is improvement in symptom A associated with better QoL? Is it symptom B which was unaffected by the surgery that has the largest effect on QoL? Obviously a given patient’s QoL and health markers at one time-point are highly correlated with each other. Another situation where this arises which isn’t explicitly repeated measures is when your data is collected in some sort of clustered fashion. Note that if your data is collected via a complex survey design, this is an entirely different beast that needs to be addressed appropriately (with the svyset and svy set of commands). Instead I’m assuming a simpler set-up here. Say you are conducting a survey that asks participants to come into a lab with their family. If the unit of analysis is a person (instead of a family), it’s very likely that two individuals in the same household will have similar food habits. Therefore we gain less information by getting information on a new individual in an existing household, then adding a new individual from a new household. The canonical example of this is students in classrooms in schools in districts. All the situations above are 2-level (defined precisely below, but here we have four levels. Again, adding a new student from an existing class/school/district will likely not add as much information as a new student from a new class in a new school in a new district. To address the lack of independence, we will move from normal regression (linear or otherwise) into a mixed models framework, which models for this dependence structure. It does this (at the most basic level) by allowing each higher level unit to have it’s own intercept which we do not estimate. 2.1 Terminology There are several different names for mixed models which you might encounter, that all fit essentially the same model: Mixed model Mixed Effects regression/model Multilevel regression/model Hierarchical regression/model (specifically HLM, hierarchical linear model) The hierarchical/multilevel variations require thinking about the levels of the data and involves “nesting”, where one variable only occurs within another, e.g. family members nested in a household. The most canonical example of this is students in classrooms, we could have Level 1: The lowest level, the students. Level 2: Classroom or teacher (this could also be two separate levels of classrooms inside teacher) Level 3: District Level 4: State Level 5: Country This is taking it a bit far; it’s rare to see more than 3 levels, but in theory, any number can exist. For this workshop, we will only briefly discuss this from hierarchical point of view, preferring the mixed models view (with the reminder again that they are the same!). 2.1.1 Econometric terminology To make the terminology a bit more complicated, in econometrics, some of the terms we will use here are overloaded. When you are discussing mixed models with someone with econometric or economics training, it’s important to differentiate between the statistical terms of “fixed effects” and “random effects” which are the two components of a mixed model that we discuss below, and what econometricians called “fixed effects regression” and “random effects regression”. Without going into the full details of the econometric world, what econometricians called “random effects regression” is essentially what statisticians called “mixed models”, what we’re talking about here. The Stata command xtreg handles those econometric models. I have a document which demostrates the equality of these, as well as explaing the ecoonometric “fixed effects regression” in terms of the statistical view of regression. 2.2 Wide vs Long data Before you begin your analysis, you need to ensure that the data is in the proper format. The data can be either in wide-form or long-form. Long-format is sometimes called tall-form. If the data is longitudinal (follows the same person over time, collecting data at intervals), the wide form would entail each row representing a single person, with the variables representing the questions at each wave. For example, if you were asking about income every 2 years, you’d have variables income14, income16, income18 representing the individuals income in 2014, 2016 and 2018. The tall form would have each row of data represent a person and a year. So you’d have a column for ID, a column for year, and then, continuing the example above, a single variable income. If the data is clustered in some sense, e.g. students in classroom, the wide form would have each row be a single classroom, and have a separate set of variables for the first student, second student, etc. In this format, unbalanced data (classrooms having different number of students) would result in a lot of missing values. The tall form would have a single row per student, and a variable representing their class. In a lot of situations, it is easier to collect and manage data in wide form. However, to fit a mixed model, we need the data in long format. We can use the reshape command to transform wide data to long. This is covered in my Introduction to Stata set of notes. 2.3 Level-1 versus Level-2 variables Often you will have variables that measure something about the unit of analysis, and other variables which measure something about the grouping variable. For example, you may have a variable indicating the GPA of a student, another variable indicating the size of their classroom, and yet another variable indicating the percent minority in their school. When thinking about this from the hierarchical view, it’s important to differentiate between these types of variables. If you take my advice and think of this from the mixed model point of view, the difference is irrelevant - each is just a variable associated with the student. The only thing that matters is ensuring the data is correct - if you had 10 students in a class, and the variable sizeofclass was 10 for half them and 8 for the other half, Stata wouldn’t know this is an issue and would fit the model without complaining - but now you’ve got a data issue that could be affecting your analysis. When the data is repeated measures over time, these are sometimes called time-variant (e.g. patient follow-ups, measured at each follow-up) or time-invariant (baseline characteristics or immutable demographics). 2.4 Theory The equation for ordinal least squares (linear regression) is \\[ Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi} + \\epsilon_i \\] where \\(Y_i\\) represents the response of individual \\(i\\), the various \\(X_{ki}\\) represent the predictor variables for the same respondent, the \\(beta\\) are the coefficients to be estimated (which are constant across individuals). \\(\\epsilon_i\\) is the additional error for this individual. As mentioned above, there are two ways to think about mixed models - as a mixed model, or as a hierarchical model. Let’s talk about the mixed model first. When we fit a mixed model, instead of a single set of \\(X\\)’s and \\(\\beta\\)’s on the right-hand side, there are now two sets, one corresponding to the fixed effects and one corresponding to the random effects (a mixture of the two, hence the name). For example, the most basic form of a mixed model, which has some number of fixed effects and a single random intercept, we have: \\[ Y_{ij} = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_pX_{pi} + \\kappa_j + \\epsilon_{ij} \\] The subscript notation helps us keep track of things. Here, I’ve set it up such that each observation \\(i\\) belongs to a group \\(j\\). The response belonging to individual \\(i\\) in group \\(j\\) is predicted based upon some \\(X\\) variables, plus some additive effect which is unique to group \\(j\\) (\\(\\kappa_j\\)), and additional error unique to that individual ($_{ij}).1 Both error terms, \\(\\epsilon\\) and \\(\\kappa\\) are assumed to have a mean of 0. This is important here because it means that on average the random intercept has no affect, but varies from individual to individual. The catch is that we do not estimate \\(\\kappa_j\\). If we include a categorical variable for the grouping variable as a fixed effect, we could estimate all those intercepts, however, doing so would in most situations overfit the model (if each individual had two measurements, we’d be including \\(n/2\\) predictors in a model with \\(n\\) observations). Instead, we estimate only the variance of \\(\\kappa_j\\) which allows us to determine whether the intercepts differ between groups. Let’s use a concrete example to make this more precise. Let your data set consist of \\(n\\) students, labeled \\(s = 1, 2, \\cdots, n\\), each belonging to one of \\(m\\) classrooms, labeled \\(c = 1, 2, \\cdots, m\\). For further simplicity, let’s assume there is only a single fixed predictor \\(X\\). \\[ Y_{sc} = \\beta_0 + \\beta_1X_s + \\kappa_c + \\epsilon_{sc} \\] It helps to think of \\(\\kappa\\) as part of the error. You predict \\(Y\\) based upon the \\(X\\)’s, then there is some common error amongst all students in classroom \\(c\\) which captured by \\(\\kappa_c\\), then there is individual error captured by \\(\\epsilon_{sc}\\). (You sometimes see the \\(\\kappa_j\\) term written as \\(\\kappa_j Z_j\\) where \\(Z_j\\) would be the variable indicating group membership. I find the above notation clearer, though they are mathematically equivalent.) In the above example, I’ve assumed that the sole \\(X\\) in the model is measured as the student level (hence \\(X_s\\)). There is no need for that. I could instead fit the above model with one variable measured per student (say GPA) and one variable measured per classroom (say average teacher evaluation). \\[ Y_{sc} = \\beta_0 + \\beta_1X_{1s} + \\beta_2X_{2c} + \\kappa_c + \\epsilon_{sc} \\] The nice part is that when fitting the model, this distinction doesn’t matter! Both \\(X_{1s}\\) and \\(X_{2c}\\) are treated the same way. The error terms can be expanded if desired. For students (\\(s\\)) nested inside classrooms (\\(c\\)) nested inside districts (\\(d\\)): \\[ Y_{scd} = \\beta_0 + \\beta_1X_{1s} + \\beta_2X_{2c} + \\gamma_d + \\kappa_{cd} + \\epsilon_{scd} \\] Here \\(\\gamma_d\\) is the error common to all students in a given district, \\(\\kappa_{cd}\\) is the additional error common to all students in a given classroom, and \\(\\epsilon_{scd}\\) is any left over student error. 2.4.1 The Hierarchical framework The hierarchical way to write these models is, in my opinion, unnecessarily complicated and does not improve on the understanding. It is more complicated for two reasons: 1) it is simply more complicated to write and understand, and 2) it requires conceptualizing the levels of the model more than needed. For completeness, I will briefly re-write the school and classroom above. In mixed form, the model would be: \\[ Y_{sc} = \\beta_0 + \\beta_1X_{1s} + \\kappa_c + \\epsilon_{sc} \\] In the hierarchical form, the model is: \\[ Y_{sc} = \\beta_{s0} + \\beta_1X_{1s} + \\epsilon_{sc} \\] \\[ \\beta_{s0} = \\gamma_{0} + \\gamma_{1}Z_c + \\sigma_c \\] If you were to plug \\(\\beta_{s0}\\) back into the first equation, you can see the equivalence of the two forms. The choice of \\(\\epsilon\\) for the individual error in a regression is fairly standardized in the literature. My choice of \\(\\kappa\\) is not, as the literature have any standard choice for the random effect.↩ "],
["fitting-linear-mixed-models.html", "Chapter 3 Fitting Linear Mixed Models 3.1 Fitting the model 3.2 Assumptions 3.3 Predicting the random effects 3.4 Nested and Crossed random effects 3.5 Choosing Random or Fixed effects 3.6 Miscellaneous 3.7 Convergence issues", " Chapter 3 Fitting Linear Mixed Models The data we’ll be using is The Irish Longitudinal Study on Ageing, specifically the 2012-2013 data. It is available via ICPSR, https://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/37105/datadocumentation (you may need to sign in to access the data). The data represents surveys of the elderly (50+) in Ireland. (The reason we’re using such an esoteric data set is that a lot of good publicly available longitudinal data comes in separate files per wave. This is very common, and if you need to use these, you’ll want to get familiar with the append and merge commands. This dataset requires no merging and is easier for demonstration purposes.) Once you’ve downloaded and extracted the files, and set your proper working directory, you can load the data and run the provided cleaning script which identifies missing values for Stata. . use ICPSR_37105/DS0001/37105-0001-Data (The Irish Longitudinal Study on Ageing (TILDA), 2012-2013) . quietly do ICPSR_37105/DS0001/37105-0001-Supplemental_syntax . rename _all, lower Each row of this data is from a single individual, but multiple individuals from the same household may be included. The primary variable of interest we’ll be focusing on is a Quality of Life scale, mhcasp19_total, which is a score built from several sub-surveys. Let’s see if we can predict it based upon age, social class, and gender. First let’s explore each variable. . rename mhcasp19_total qol . histogram qol (bin=37, start=4, width=1.4324324) Looks fine. . histogram age (bin=38, start=51, width=.81578947) There’s actually a bit of censoring in the data, anyone below 52 or above 82. One way around this is to add dummy variables flagging those individuals, so let’s do that. . label list AGE AGE: 51 Less than 52 82 82+ . generate agebelow52 = age == 51 . replace agebelow52 = . if missing(age) (0 real changes made) . generate ageabove82 = age == 81 . replace ageabove82 = . if missing(age) (0 real changes made) Next social class and gender: . rename w2socialclass socialclass . tab socialclass w2socialclass | Freq. Percent Cum. -----------------------------+----------------------------------- Professional | 338 5.06 5.06 Managerial &amp; Technical | 1,753 26.23 31.29 Non-Manual | 2,023 30.27 61.56 Skilled | 1,159 17.34 78.90 Semi-skilled | 1,115 16.68 95.59 Unskilled | 295 4.41 100.00 -----------------------------+----------------------------------- Total | 6,683 100.00 . tab gd002 gd002 - | Gender of | respondent | Freq. Percent Cum. ------------+----------------------------------- Male | 3,203 44.44 44.44 Female | 4,004 55.56 100.00 ------------+----------------------------------- Total | 7,207 100.00 . generate female = gd002 == 2 . replace female = . if missing(gd002) (0 real changes made) Both look fine. Finally, the household variable identifies individuals belonging to the same household. . display _N 7207 . quietly levelsof household . display r(r) 5376 So we have 7,207 total individuals across 5,376 households. 3.1 Fitting the model First, let’s fit a linear regression model ignoring the dependence within households. . regress qol age agebelow52 ageabove82 i.socialclass female Source | SS df MS Number of obs = 5,179 -------------+---------------------------------- F(9, 5169) = 16.52 Model | 9173.37321 9 1019.26369 Prob &gt; F = 0.0000 Residual | 318919.729 5,169 61.6985354 R-squared = 0.0280 -------------+---------------------------------- Adj R-squared = 0.0263 Total | 328093.103 5,178 63.3629012 Root MSE = 7.8548 ------------------------------------------------------------------------------ qol | Coef. Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- age | -.010319 .0135138 -0.76 0.445 -.0368117 .0161737 agebelow52 | -.5032791 .559864 -0.90 0.369 -1.600849 .5942913 ageabove82 | -1.479405 1.006563 -1.47 0.142 -3.452694 .4938839 | socialclass | Manageria.. | .2622181 .5266783 0.50 0.619 -.7702942 1.29473 Non-Manual | -1.227245 .525863 -2.33 0.020 -2.258159 -.1963312 Skilled | -2.1252 .5508147 -3.86 0.000 -3.20503 -1.04537 Semi-skil~d | -3.091173 .5596054 -5.52 0.000 -4.188236 -1.994109 Unskilled | -4.056536 .7457017 -5.44 0.000 -5.518427 -2.594645 | female | .4453072 .2310058 1.93 0.054 -.007562 .8981763 _cons | 45.07279 1.002216 44.97 0.000 43.10802 47.03756 ------------------------------------------------------------------------------ This model doesn’t do so hot, but it’s sufficient for our purposes - the F-test rejects. The interpretation of the age variables is that the coefficient on age represents the relationship between age and QoL for individuals between ages 52 and 81. The two coefficients on agebelow52 and ageabove82 are allowing those individuals which censored ages to have a unique intercept, which means they don’t affect the slope on age. If you really wanted to drill down into what this all means, you could do some fancy margins calls to predict the average response using at() to force the two dummies to the appropriate levels (not run): margins, at(age = 51 agebelow52 = 1 ageabove82 = 0) at(age = (52 81) agebelow52 = 0 ageabove82 = 0) at(age=81 agebelow52 = 0 ageabove82 = 1) marginsplot In this case, there doesn’t seem to be much effect of age (though it is good we controlled for it!). We see a marginal effect for female, and we see some differences amongst social classes. Let’s explore them more with margins: . margins socialclass Predictive margins Number of obs = 5,179 Model VCE : OLS Expression : Linear prediction, predict() ------------------------------------------------------------------------------ | Delta-method | Margin Std. Err. t P&gt;|t| [95% Conf. Interval] -------------+---------------------------------------------------------------- socialclass | Professio~l | 44.61599 .4839074 92.20 0.000 43.66733 45.56465 Manageria.. | 44.87821 .2053926 218.50 0.000 44.47555 45.28086 Non-Manual | 43.38874 .1976598 219.51 0.000 43.00125 43.77624 Skilled | 42.49079 .2765152 153.67 0.000 41.9487 43.03288 Semi-skil~d | 41.52482 .2788682 148.90 0.000 40.97812 42.07152 Unskilled | 40.55945 .5653632 71.74 0.000 39.4511 41.6678 ------------------------------------------------------------------------------ . margins socialclass, pwcompare(pv) Pairwise comparisons of predictive margins Model VCE : OLS Expression : Linear prediction, predict() ------------------------------------------------------------------------------ | Delta-method Unadjusted | Contrast Std. Err. t P&gt;|t| --------------------------------------+--------------------------------------- socialclass | Managerial &amp; Technical | vs | Professional | .2622181 .5266783 0.50 0.619 Non-Manual vs Professional | -1.227245 .525863 -2.33 0.020 Skilled vs Professional | -2.1252 .5508147 -3.86 0.000 Semi-skilled vs Professional | -3.091173 .5596054 -5.52 0.000 Unskilled vs Professional | -4.056536 .7457017 -5.44 0.000 Non-Manual vs Managerial &amp; Technical | -1.489463 .2842616 -5.24 0.000 Skilled vs Managerial &amp; Technical | -2.387418 .3458945 -6.90 0.000 Semi-skilled | vs | Managerial &amp; Technical | -3.353391 .3461459 -9.69 0.000 Unskilled vs Managerial &amp; Technical | -4.318754 .6011768 -7.18 0.000 Skilled vs Non-Manual | -.8979548 .3446961 -2.61 0.009 Semi-skilled vs Non-Manual | -1.863928 .3411728 -5.46 0.000 Unskilled vs Non-Manual | -2.829291 .5978386 -4.73 0.000 Semi-skilled vs Skilled | -.9659729 .3939009 -2.45 0.014 Unskilled vs Skilled | -1.931336 .6317203 -3.06 0.002 Unskilled vs Semi-skilled | -.965363 .6305738 -1.53 0.126 ------------------------------------------------------------------------------ So Professional and Managerial are indistinguishable, and Semi-skilled and Unskilled are likewise indistinguishable. To fit the mixed model, the command is mixed. Let’s first fit it again ignoring the household random effects. . mixed qol age agebelow52 ageabove82 i.socialclass female Mixed-effects ML regression Number of obs = 5,179 Wald chi2(9) = 148.97 Log likelihood = -18018.271 Prob &gt; chi2 = 0.0000 ------------------------------------------------------------------------------ qol | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- age | -.010319 .0135007 -0.76 0.445 -.0367799 .016142 agebelow52 | -.5032791 .5593233 -0.90 0.368 -1.599533 .5929744 ageabove82 | -1.479405 1.00559 -1.47 0.141 -3.450326 .4915162 | socialclass | Manageria.. | .2622181 .5261696 0.50 0.618 -.7690553 1.293492 Non-Manual | -1.227245 .525355 -2.34 0.019 -2.256922 -.1975681 Skilled | -2.1252 .5502827 -3.86 0.000 -3.203734 -1.046666 Semi-skil~d | -3.091173 .5590649 -5.53 0.000 -4.18692 -1.995426 Unskilled | -4.056536 .7449814 -5.45 0.000 -5.516673 -2.596399 | female | .4453072 .2307827 1.93 0.054 -.0070187 .897633 _cons | 45.07279 1.001248 45.02 0.000 43.11038 47.0352 ------------------------------------------------------------------------------ ------------------------------------------------------------------------------ Random-effects Parameters | Estimate Std. Err. [95% Conf. Interval] -----------------------------+------------------------------------------------ var(Residual) | 61.5794 1.210117 59.25271 63.99746 ------------------------------------------------------------------------------ We get identical results. If you look at the equations again, when there are no random effects, the model simplifies to ordinal least squares. To add our random effect, we’ll use the following notation: mixed y &lt;fixed effects&gt; || &lt;group variable&gt;: The || splits a formula into two sides, the left of it is the fixed effects, the right is the random effects. The : is for including random slopes which we’ll discuss below. . mixed qol age agebelow52 ageabove82 i.socialclass female || household: Performing EM optimization: Performing gradient-based optimization: Iteration 0: log likelihood = -17951.595 Iteration 1: log likelihood = -17945.184 Iteration 2: log likelihood = -17945.183 Computing standard errors: Mixed-effects ML regression Number of obs = 5,179 Group variable: household Number of groups = 3,995 Obs per group: min = 1 avg = 1.3 max = 3 Wald chi2(9) = 129.84 Log likelihood = -17945.183 Prob &gt; chi2 = 0.0000 ------------------------------------------------------------------------------ qol | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- age | -.0102437 .0139236 -0.74 0.462 -.0375335 .0170461 agebelow52 | -.379831 .5276012 -0.72 0.472 -1.41391 .6542483 ageabove82 | -1.210758 .986185 -1.23 0.220 -3.143645 .7221288 | socialclass | Manageria.. | .2347399 .5116288 0.46 0.646 -.7680341 1.237514 Non-Manual | -1.09934 .5130227 -2.14 0.032 -2.104846 -.0938337 Skilled | -1.802139 .5375521 -3.35 0.001 -2.855722 -.7485562 Semi-skil~d | -2.97498 .5493453 -5.42 0.000 -4.051677 -1.898283 Unskilled | -3.555771 .7322568 -4.86 0.000 -4.990968 -2.120574 | female | .570825 .2086576 2.74 0.006 .1618637 .9797863 _cons | 44.78498 1.022019 43.82 0.000 42.78186 46.7881 ------------------------------------------------------------------------------ ------------------------------------------------------------------------------ Random-effects Parameters | Estimate Std. Err. [95% Conf. Interval] -----------------------------+------------------------------------------------ household: Identity | var(_cons) | 22.9339 1.795418 19.67162 26.73718 -----------------------------+------------------------------------------------ var(Residual) | 38.98732 1.613295 35.95015 42.28109 ------------------------------------------------------------------------------ LR test vs. linear model: chibar2(01) = 146.18 Prob &gt;= chibar2 = 0.0000 Let’s walk through the output. Note that what we are calling the random effects (e.g. individuals in a repeated measures situation, classrooms in a students nested in classroom situation), Stata refers to as “groups” in the top of the output. You probably noticed how slow it is - at the very top, you’ll see that the solution is arrived at iteratively. Recall that any regression model aside from OLS requires an iterative solution. The log likelihood is how the iteration works; essentially the model “guesses” choices for the coefficients, and finds the set of coefficients that minimize the log likelihood. Of course, the “guess” is much smarter than random. The actual value of the log likelihood is meaningless. Since we are dealing with repeated measures of some sort, instead of a single sample size, we record the total number of obs, the number of groups (unique entries in the random effects) and min/mean/max of the groups. Just ensure there are no surprises in these numbers. In this model, the quality of life has a good chunk of missingness, so we’re losing about 2000 individuals. The \\(\\chi^2\\) test tests the hypothesis that all coefficients are simultaneously 0. We have a significant p-value, so we continue with the interpretation. The coefficients table is interpreted just as in linear regression, with the addendum that each coefficient is also controlling for the structure introduced by the random effects. There is still nothing informative in age, however, compared to OLS, the two dummy variables have notably different coefficients. We’ll have to check whether there is any difference in our conclusion regarding social class. The coefficient on gender is larger and much more significant. The second table (“Random-effects parameters”) gives us information about the error structure. The “household:” section is examining whether there is variation across households above and beyond the differences in the controlled variables. Since the estimate of var(_cons) (the estimated variance of the constant per person - the individual level random effect) is non-zero (and not close to zero), that is evidence that the random effect is necessary. If the estimate was 0 or close to 0, that would be evidence that the random effect is unnecessary and that any difference between individuals is already accounted for by the covariates. The estimated variance of the residuals is any additional variation between observations. This is akin to the residuals from linear regression. The \\(\\chi^2\\) test at the bottom is a formal test of the inclusion of the random effects versus a linear regression model without the random effects. We reject the null that the models are equivalent, so it is appropriate to include the random effects. Let’s quickly examine the social class categories. The calls to margins are identical, but they operate slightly differently with the random effects. Recall that the margins command works by assuming every row data is a given social class, then uses the observed values of the other fixed effects and the regression equation to predict the outcome, and averaging to obtain the marginal means. This is not the case with the random effects; the random effects (\\(\\kappa_j\\)) are assumed to be 0. So for any given household, the predicted response could be higher or lower, but on aggregate across all households, we are estimating the marginal means. . margins socialclass Predictive margins Number of obs = 5,179 Expression : Linear prediction, fixed portion, predict() ------------------------------------------------------------------------------ | Delta-method | Margin Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- socialclass | Professio~l | 44.41056 .4747326 93.55 0.000 43.4801 45.34101 Manageria.. | 44.64529 .207459 215.20 0.000 44.23868 45.05191 Non-Manual | 43.31122 .198432 218.27 0.000 42.9223 43.70014 Skilled | 42.60842 .2721936 156.54 0.000 42.07493 43.14191 Semi-skil~d | 41.43558 .2762047 150.02 0.000 40.89422 41.97693 Unskilled | 40.85478 .555399 73.56 0.000 39.76622 41.94335 ------------------------------------------------------------------------------ . margins socialclass, pwcompare(pv) Pairwise comparisons of predictive margins Expression : Linear prediction, fixed portion, predict() ------------------------------------------------------------------------------ | Delta-method Unadjusted | Contrast Std. Err. z P&gt;|z| --------------------------------------+--------------------------------------- socialclass | Managerial &amp; Technical | vs | Professional | .2347399 .5116288 0.46 0.646 Non-Manual vs Professional | -1.09934 .5130227 -2.14 0.032 Skilled vs Professional | -1.802139 .5375521 -3.35 0.001 Semi-skilled vs Professional | -2.97498 .5493453 -5.42 0.000 Unskilled vs Professional | -3.555771 .7322568 -4.86 0.000 Non-Manual vs Managerial &amp; Technical | -1.334079 .2766414 -4.82 0.000 Skilled vs Managerial &amp; Technical | -2.036879 .33834 -6.02 0.000 Semi-skilled | vs | Managerial &amp; Technical | -3.20972 .3412208 -9.41 0.000 Unskilled vs Managerial &amp; Technical | -3.790511 .5910519 -6.41 0.000 Skilled vs Non-Manual | -.7027993 .33355 -2.11 0.035 Semi-skilled vs Non-Manual | -1.87564 .3334359 -5.63 0.000 Unskilled vs Non-Manual | -2.456432 .5864838 -4.19 0.000 Semi-skilled vs Skilled | -1.172841 .3799212 -3.09 0.002 Unskilled vs Skilled | -1.753633 .6150853 -2.85 0.004 Unskilled vs Semi-skilled | -.5807918 .6134086 -0.95 0.344 ------------------------------------------------------------------------------ Here our conclusions don’t change - Unskilled &amp; Semi-skilled have the same marginal means, and Professional and Managerial have the same marginal means, all other comparisons are significant. 3.2 Assumptions The linear additivity remains necessary - we need to assume that the true relationship between the predictors and the outcome is linear (as opposed to something more complicated like exponential) and additive (as opposed to multiplicative, unless we are including interactions). With regress, we could use the rvf post-estimation command to generate a plot of residuals versus predicted values. The rvfplot command does not work after mixed, but we can generate it manually. . predict fitted, xb (524 missing values generated) . predict residuals, res (2,028 missing values generated) . twoway scatter residuals fitted The odd grouping pattern shown is due to the two categorical variables (gender and social class in the model). Each “blob” represents one permutation. You can see this by overlaying many plots: . twoway (scatter residuals fitted if female == 1 &amp; socialclass == 1) /// &gt; (scatter residuals fitted if female == 0 &amp; socialclass == 1) /// &gt; (scatter residuals fitted if female == 1 &amp; socialclass == 2) /// &gt; (scatter residuals fitted if female == 0 &amp; socialclass == 2) /// &gt; (scatter residuals fitted if female == 1 &amp; socialclass == 3) /// &gt; (scatter residuals fitted if female == 0 &amp; socialclass == 3) /// &gt; (scatter residuals fitted if female == 1 &amp; socialclass == 4) /// &gt; (scatter residuals fitted if female == 0 &amp; socialclass == 4) /// &gt; (scatter residuals fitted if female == 1 &amp; socialclass == 5) /// &gt; (scatter residuals fitted if female == 0 &amp; socialclass == 5) /// &gt; (scatter residuals fitted if female == 1 &amp; socialclass == 6) /// &gt; (scatter residuals fitted if female == 0 &amp; socialclass == 6), /// &gt; legend(off) Overall though we see no pattern in the residuals. The other two assumptions which are relevant in linear regression, homogeneity of residuals and independence, are both violated by design in a mixed model. However, you need to assume that no other violations occur - if there is additional variance heterogeneity, such as that brought above by very skewed response variables, you may need to make adjustments. Similarly, if there is some other form of dependence you are not yet modeling, you need to adjust your model to account for it. 3.3 Predicting the random effects While keeping in mind that we do not estimate the random intercepts when fitting this model, we can predict their BLUPS - best linear unbiased predictors. What’s the difference? It’s subtle, but basically we have far less confidence in predicted values than in estimated values. So any confidence intervals will be substantially larger. We know from the model output above that there is variance among the household random intercepts but don’t have a sense of the pattern - is there a single household that’s much different than all the rest? Are they all rather noisy? Or something in between. We can test this. First, we’ll predict the random intercepts and the standard error of those intercepts. . predict rintercept, reffects (1,592 missing values generated) . predict rintse, reses (1,592 missing values generated) As noted above, the quality of life variable is missing for around 2000 individuals, so we’ll remove them from the data for ease (calling preserve first to recover the full data later). . preserve . drop if missing(qol) (1,670 observations deleted) Now within each household, the random intercepts and standard errors are identical, so we can collapse down to the household level. . collapse (first) rintercept rintse (count) age, by(household) Now we will sort by those random intercepts to make the output look nicer, generate a variable indicating row number for plotting on the x-axis, and compute upper and lower bounds of confidence intervals. Finally we generate a dummy variable to identify which households have confidence intervals not crossing zero. . sort rintercept . gen n = _n . gen lb = rintercept - 1.96*rintse (209 missing values generated) . gen ub = rintercept + 1.96*rintse (209 missing values generated) . gen significant = (lb &gt; 0 &amp; ub &gt; 0) | (lb &lt; 0 &amp; ub &lt; 0) Now we can plot. . twoway (rcap ub lb n if significant) /// &gt; (scatter rintercept n if significant), /// &gt; legend(off) yline(0) The range in the middle is all households which we predict to have intercepts not distinguishable from zero. We can see that most of the random intercept’s confidence intervals cross with the exception of very few large values and a larger chunk of small values. So there’s a negative skew to the distribution of random intercepts; most households had no additional error, but a decent chunk had significantly lower outcome than we would expect given their personel attributes. We can count exactly how many fall below: . count if ub &lt; 0 &amp; lb &lt; 0 100 3.4 Nested and Crossed random effects The model we’ve fit so far has a single random intercept, corresponding to household. However, we could have more than one. 3.4.1 Nested random effects Nested random effects exist when we have a nested level structure. In this data, we actually have this as each household belongs to a single sampling cluster, identified by the cluster variable. We can re-fit the model including a random intercept for cluster, then within each cluster, a random intercept for household. We do so by adding another equation via ||: . restore . mixed qol age agebelow52 ageabove82 i.socialclass female || cluster: || house &gt; hold: Performing EM optimization: Performing gradient-based optimization: Iteration 0: log likelihood = -17961.965 Iteration 1: log likelihood = -17945.566 Iteration 2: log likelihood = -17945.22 Iteration 3: log likelihood = -17945.184 Iteration 4: log likelihood = -17945.183 Computing standard errors: Mixed-effects ML regression Number of obs = 5,179 ------------------------------------------------------------- | No. of Observations per Group Group Variable | Groups Minimum Average Maximum ----------------+-------------------------------------------- cluster | 619 1 8.4 28 household | 3,995 1 1.3 3 ------------------------------------------------------------- Wald chi2(9) = 129.84 Log likelihood = -17945.183 Prob &gt; chi2 = 0.0000 ------------------------------------------------------------------------------ qol | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- age | -.0102438 .0139236 -0.74 0.462 -.0375336 .017046 agebelow52 | -.3798295 .5275999 -0.72 0.472 -1.413906 .6542472 ageabove82 | -1.210748 .9861847 -1.23 0.220 -3.143634 .7221387 | socialclass | Manageria.. | .2347385 .5116282 0.46 0.646 -.7680344 1.237511 Non-Manual | -1.099335 .5130222 -2.14 0.032 -2.10484 -.0938297 Skilled | -1.802128 .5375516 -3.35 0.001 -2.85571 -.7485466 Semi-skil~d | -2.974975 .5493449 -5.42 0.000 -4.051671 -1.898278 Unskilled | -3.555756 .7322564 -4.86 0.000 -4.990952 -2.12056 | female | .5708288 .2086569 2.74 0.006 .1618689 .9797888 _cons | 44.78498 1.02202 43.82 0.000 42.78186 46.7881 ------------------------------------------------------------------------------ ------------------------------------------------------------------------------ Random-effects Parameters | Estimate Std. Err. [95% Conf. Interval] -----------------------------+------------------------------------------------ cluster: Identity | var(_cons) | 2.43e-09 1.02e-06 0 . -----------------------------+------------------------------------------------ household: Identity | var(_cons) | 22.93459 1.795629 19.67196 26.73835 -----------------------------+------------------------------------------------ var(Residual) | 38.98676 1.613252 35.94966 42.28043 ------------------------------------------------------------------------------ LR test vs. linear model: chi2(2) = 146.18 Prob &gt; chi2 = 0.0000 Note: LR test is conservative and provided only for reference. Overall the output looks very similar to the model before, but now in the last table, the Random-effects Parameters, we have estimates of three variances, with the addition of cluster random effects. In this case, the estimated variance is almost 0 (so close to 0 that Stata refuses to even compute a confidence interval), so we do not need this random effect - once we control for the fixed effects, there is no additional error common to each cluster. This shouldn’t be too surprising, as the clusters in this data are part of the survey design (which, again, we are ignoring) and somewhat meaningless. If you compare the results of this model to the first model, you’ll see that the results are basically identical - as we expect when adding a predictor that does not improve model fit. It’s up to you whether you want to remove it. On the one hand, you can now appropriately claim you’re controlling for cluster-effects, but on the other hand, an additional random intercept makes the model converge much more slowly. 3.4.2 Crossed random effects In the above design, we had nesting - within each cluster there are households, within each household there are individuals. However, this need not always be the case. For example, imagine you lead a large research team where 100 research assistants are conducting repeated surveys of individuals. Here we have repeated measures per person, but we also potentially have interviewer effects that we want to include as random intercepts. However, there is no nesting structure here. Instead, we call this a crossed random effects structure. Here’s the rub though: There is no such thing as “nested random effects”. Everything is crossed. So why do nested random effects exist? Consider the following small data set. classroom studentid a 1 a 2 b 1 b 2 Here the first row of data represents the first student in classroom a, and the third row represents the first student in classroom b - both students are not the same student. If we attempted to tell Stata that these random effects are crossed, Stata will incorrectly think rows 1 and 3 are the same student. By telling Stata that studentid is nested inside classroom, it knows that student 1 from classroom a is distinct from student 1 from classroom b. However, if we were more clever with our data, we might instead store our data as: classroom studentid a 1 a 2 b 3 b 4 Now if I tell Stata these are crossed random effects, it won’t get confused! So all nested random effects are are a way to make up for the fact that you may have been foolish in identifying individuals earlier. Unfortunately fitting crossed random effects in Stata is a bit unwieldy. Here’s the model we’ve been working with with crossed random effects. . mixed qol age agebelow52 ageabove82 i.socialclass female || /// &gt; _all:R.household || _all:R.cluster Performing EM optimization: likelihood evaluates to missing r(430); This exposes one difference we haven’t addressed - just because functionally crossed and nested effects are the same, does not mean the algorithm which the software uses functions the same. Stata simply fails more often with crossed random effects. On the other hand, the lmer command in R has an easier time of specifying crossed effects and can converge this model just fine. 3.5 Choosing Random or Fixed effects When these models were first being developed, the recommended guidelines were: Fixed effects are used when the categories in the data represent all possible categories. For example, the collection of all schools in a given state. Random effects are used when the categories in the data represent a sample of all possible categories. For example, a sample of students in a school. There are plenty of grey-areas in between so this advice isn’t always useful. Instead think of it from a practical point of view: Do you have a large enough sample size to include fixed effects (recall the rule of 10-20 observations per predictor. If each person in your data has no more than 2 observations, that’s far too many fixed effects)? Do you need to actually estimate the inercept within each group, as opposed to just controlling for group differences? (We can always predict random effects but that’s not as powerful as estimating.) If the answer to both these are No, include as random effects. (If you don’t need to estimate the group intercepts but the number of groups is low (say less than 5-10), it’s more common to just include it as a fixed effect anyways. But that’s just convention, not due to any strong argument which I’m aware of.) 3.6 Miscellaneous There are various concerns we had with non-mixed models; most still hold. See the previous set of notes (linked to each topic) for more details. Multicollinearity, the issue that predictor variables can be correlated amongst each other to provide false positive results, remains an issue in linear regression. The VIF cannot be estimated after a mixed model. Overfitting is still possible. Sample size considerations are tricky, but the general rule of 10-20 observations per predictor is a good starting point. There’s the additional complexity of the group structure, but generally there are no restrictions on it. However, in practice, in you have a large number of groups with only a single measurement, convergence issues tend to arise. Model selection is still a very bad idea to do. Robust standard errors are still obtainable using vce(robust) option. 3.7 Convergence issues With mixed models, the solution is arrived at iteratively, which means it can fail to converge for a number of reasons. Generally, failure to converge will be due to an issue with the data. The first thing to try is scaling all your parameters: egen scaled_var = std(var) Dummy variables typically don’t need to be scaled, but can be. If scaling all your variables allows convergence, try different combinations of scaled and unscaled to figure out what variables are causing the problem. It’s typically (but not always) the variables which have the largest scale to begin with. Another potential convergence issue is extremely high correlation between predictors (including dummy variables). You should have already addressed this when considering multicollinearity, but if not, it can make convergence challenging. If the iteration keeps running (as opposed to ending and complaining about lack of convergence), try passing the option emiterate(#) with a few “large” (“large” is relative to running time) numbers to tell the algorithm to stop after # iterations, regardless of convergence. (Recall that an iterative solution produces an answer at each iteration, it’s just not a consistent answer until you reach convergence.) You’re looking for two things: First, if there are any estimated standard errors that are extremely close to zero or exploding towards infinity, that predictor may be causing the issue. Try removing it. Second, if you try a few different max iterations (say 50, 100 and 200), and the estimated coefficients and standard errors are relatively constant, you may be running into a case where the model is converging to just beyond the tolerance which Stata uses, but for all intents and purposes is converged. Set emiterate(#) to a high number and use that result. You can try use the “reml” optimizer, by passing the reml option. This optimizer can be a bit easier to converge, though may be slower. Finally, although it pains me to admit it, you should try running the model in different software such as R or SAS. Each software has slightly different algorithms it uses, and there are situations where one software will converge and another won’t "],
["random-slopes.html", "Chapter 4 Random slopes 4.1 Fitting a random slope 4.2 Do you need to include the fixed slope if you have the random slopes", " Chapter 4 Random slopes So far all we’ve talked about are random intercepts. This is by far the most common form of mixed effects regression models. Recall that we set up the theory by allowing each group to have its own intercept which we don’t estimate. We can also allow each group to have it’s own slope which we don’t estimate. Just as random intercepts are akin to including a fixed effect allowing each group to have it’s own fixed effect, random slopes are akin to interacting a variable with the grouping variable, allowing each group to have it’s own relationship. We would include a random slope in the model if, instead of the relationship between a predictor and the outcome when controlling for group membership, we were interested in the average relationship between the predictor and the outcome across groups. For example, if we had our basic class example, mixed gpa familyincome || class: In this model, the coefficient on familyincome would estimate the relationship between family income and GPA, removing any additional class-level differences. (Class-level differences here might be that a class that randomly has lower average income has a better teacher.) If we include a random slope, we add the variable after the : in the second equation. mixed gpa familyincome || class: familyincome Now the model would allow each classroom to have it’s own relationship between family income and GPA (which, just like random intercepts, is not actually estimated) and the coefficient on familyincome would represent the average of those relationships. Do you need a random slope? It depends on your theory. If your grouping variable is a nuisance and you’re simply controlling for it (as it is in most cases), you probably don’t need a random slope. If, on the other hand, you suspect there are substantial differences between groups and you’re really interested in the average of those differences, then you should. For reference, I’d say conservatively 95% of the mixed models I fit are in situations where a random slope is not needed, and 75% of the time when people ask me if they need a random slope, the answer is no. However, my general bias is towards simpler models.2 4.1 Fitting a random slope Let’s add a random slope for gender. . mixed qol age agebelow52 ageabove82 i.socialclass female || household: female Performing EM optimization: Performing gradient-based optimization: Iteration 0: log likelihood = -18119.073 (not concave) Iteration 1: log likelihood = -17948.133 Iteration 2: log likelihood = -17943.723 Iteration 3: log likelihood = -17943.715 Iteration 4: log likelihood = -17943.715 Computing standard errors: Mixed-effects ML regression Number of obs = 5,179 Group variable: household Number of groups = 3,995 Obs per group: min = 1 avg = 1.3 max = 3 Wald chi2(9) = 129.85 Log likelihood = -17943.715 Prob &gt; chi2 = 0.0000 ------------------------------------------------------------------------------ qol | Coef. Std. Err. z P&gt;|z| [95% Conf. Interval] -------------+---------------------------------------------------------------- age | -.0091431 .0139002 -0.66 0.511 -.0363871 .0181008 agebelow52 | -.3693919 .5323223 -0.69 0.488 -1.412724 .6739406 ageabove82 | -1.205025 .9809621 -1.23 0.219 -3.127675 .7176254 | socialclass | Manageria.. | .2311014 .5075454 0.46 0.649 -.7636692 1.225872 Non-Manual | -1.107476 .5090861 -2.18 0.030 -2.105267 -.1096861 Skilled | -1.807633 .5325278 -3.39 0.001 -2.851369 -.7638979 Semi-skil~d | -2.980059 .5455297 -5.46 0.000 -4.049278 -1.910841 Unskilled | -3.576478 .7302271 -4.90 0.000 -5.007697 -2.14526 | female | .5655487 .2081086 2.72 0.007 .1576634 .973434 _cons | 44.72468 1.018608 43.91 0.000 42.72825 46.72112 ------------------------------------------------------------------------------ ------------------------------------------------------------------------------ Random-effects Parameters | Estimate Std. Err. [95% Conf. Interval] -----------------------------+------------------------------------------------ household: Independent | var(female) | 4.050557 2.356374 1.295197 12.66757 var(_cons) | 22.71352 1.789211 19.46403 26.50551 -----------------------------+------------------------------------------------ var(Residual) | 36.96963 1.960132 33.32072 41.01813 ------------------------------------------------------------------------------ LR test vs. linear model: chi2(2) = 149.11 Prob &gt; chi2 = 0.0000 Note: LR test is conservative and provided only for reference. Most of the output seems very familiar. The only addition is the “var(female)” in the Random-effects Parameters table which, just like in random intercepts, estimates the variance across all the random slopes. Here it is very non-zero, so improves model fit. However, none of the fixed effects really change. The only difference is in the interpretation of the coefficient on age: In the random intercepts model, the coefficient on age represented that females were on average that much higher than males, regardless of age, social class, or inter-household variance. In this model with the random slope as well, the coefficient on age represents the average across all households of the amount that females are above males, regardless of age or social class. 4.2 Do you need to include the fixed slope if you have the random slopes Yes. In almost every model. This is very similar to excluding the intercept (\\(\\beta_0\\)) in a model - this forces the slope to pass through (0,0). In some very rare situations that might be appropriate, but extremely rarely. Excluding the fixed slope when including random slopes forces the average of all random slopes to be 0. If the true random slope is far from zero, this will have catastrophic effects, including reversing the signs on a good number of the random slopes. If you’re curious, may rational is I’d rather fit a simpler model that misses a nuanced complexity, then fit a more complicated model that has takes a substantial power hit and potentially is drastically further from the “truth”.↩ "],
["generalized-linear-mixed-models.html", "Chapter 5 Generalized Linear Mixed Models 5.1 Logistic Mixed Model 5.2 Poisson Mixed Model 5.3 Ordinal Logistic Regression 5.4 Multinomial Logistic Regression", " Chapter 5 Generalized Linear Mixed Models Just as how generalized linear models are an extension of linear regression, generalized linear mixed models are an extension of linear mixed models. In general, analysis and interpretation proceeds in a logical fashion from GLM’s and mixed models. We’ll briefly list some of the mixed models, and any quirks to be discussed about them. This section may be expanded in the future. 5.1 Logistic Mixed Model There are actually two commands for logistic mixed models: melogit and meqrlogit. The former is faster, but the latter is more likely to converge. Both commands function generally identically. Note that meqrlogit is a somewhat outdated command, so it’s possible that newer features to melogit may no longer work with meqrlogit. Separation remains a major concern amongst fixed effects, but of lesser concern amongst random intercepts (e.g. a household where everyone had a positive response would break if included as a fixed effect, but generally would run as a random intercept). The only concern is that separation in random effects can make convergence harder to achieve. 5.2 Poisson Mixed Model Poisson mixed models can be run with the mepoisson command. A meqrpoisson command exists and has benefits just like meqrlogit, but again, is an outdated command. If over-dispersion is an issue, menbreg exists for negative binomial regression. 5.3 Ordinal Logistic Regression These models can be run with meologit. 5.4 Multinomial Logistic Regression To my knowledge, Stata does not implement this model. It can be fit in several packages in R if you need it. "]
]
